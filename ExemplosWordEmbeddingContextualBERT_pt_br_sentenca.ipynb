{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExemplosWordEmbeddingContextualBERT_pt_br_sentenca.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osmarbraz/exemplos_BERT/blob/main/ExemplosWordEmbeddingContextualBERT_pt_br_sentenca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78HE8FLsKN9Q"
      },
      "source": [
        "#Exemplo de Word Embeddings(pt-br) Contextual usando BERT Transformers by HuggingFace\n",
        "\n",
        "## **A execução pode ser feita através do menu Ambiente de Execução opção Executar tudo.**\n",
        "\n",
        "Exemplos de uso de **Word Embeddings Contextuais** para **desambiguação** de documentos originais e permutados utilizando suas sentenças. No final do notebook estão os exemplos com os documentos:\n",
        "\n",
        "*   documento original e permutado\n",
        "\n",
        "**Link biblioteca Huggingface:**\n",
        "https://github.com/huggingface/transformers\n",
        "\n",
        "\n",
        "**Artigo original BERT Jacob Devlin:**\n",
        "https://arxiv.org/pdf/1506.06724.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyxb5Px3p1-e"
      },
      "source": [
        "## 0 - Preparação do ambiente\n",
        "Preparação do ambiente para execução do exemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAPVtRXQqDim"
      },
      "source": [
        "###Tratamento de logs\n",
        "\n",
        "Método para tratamento dos logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcopxbGZqDip"
      },
      "source": [
        "# Biblioteca de logging\n",
        "import logging\n",
        "\n",
        "# Formatando a mensagem de logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GjYtXcMnSAe"
      },
      "source": [
        "### Identificando o ambiente Colab\n",
        "\n",
        "Cria uma variável para identificar que o notebook está sendo executado no Google Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMiH0E3OnRa1"
      },
      "source": [
        "# Se estiver executando no Google Colaboratory\n",
        "import sys\n",
        "\n",
        "# Retorna true ou false se estiver no Google Colaboratory\n",
        "IN_COLAB = 'google.colab' in sys.modules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqa-7WXBAw8q"
      },
      "source": [
        "## 1 - Instalação BERT da Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCdqJCtQN52l"
      },
      "source": [
        "Instala a interface pytorch para o BERT by Hugging Face. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RfUN_KolV-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef8498a-30d5-4571-8819-9141b10b91f4"
      },
      "source": [
        "# Instala a última versão da biblioteca\n",
        "#!pip install transformers\n",
        "\n",
        "# Instala uma versão específica da biblioteca\n",
        "!pip install -U transformers==4.5.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 39.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 39.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.1) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.1) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.1) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQj2wmKDpkrH"
      },
      "source": [
        "## 2 - Download do arquivo do PyTorch Checkpoint\n",
        "\n",
        "Lista de modelos da comunidade:\n",
        "* https://huggingface.co/models\n",
        "\n",
        "Português(https://github.com/neuralmind-ai/portuguese-bert):  \n",
        "* **'neuralmind/bert-base-portuguese-cased'**\n",
        "* **'neuralmind/bert-large-portuguese-cased'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajrTjZzapkrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f62964c3-88d8-48d8-bba4-505f40d3fd1d"
      },
      "source": [
        "# Importando as bibliotecas\n",
        "import os\n",
        "\n",
        "# Variável para setar o arquivo\n",
        "URL_MODELO = None\n",
        "\n",
        "# Comente uma das urls para carregar modelos de tamanhos diferentes(base/large)\n",
        "# URL_MODELO do arquivo do modelo tensorflow\n",
        "# arquivo menor(base) 1.1 Gbytes\n",
        "URL_MODELO = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip\"\n",
        "\n",
        "# arquivo grande(large) 3.5 Gbytes\n",
        "#URL_MODELO = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-large-portuguese-cased/bert-large-portuguese-cased_pytorch_checkpoint.zip\"\n",
        "\n",
        "# Se a variável foi setada\n",
        "if URL_MODELO:\n",
        "\n",
        "    # Diretório descompactação\n",
        "    DIRETORIO_MODELO = '/content/modelo'\n",
        "\n",
        "    # Recupera o nome do arquivo do modelo da URL_MODELO\n",
        "    arquivo = URL_MODELO.split(\"/\")[-1]\n",
        "\n",
        "    # Nome do arquivo do vocabulário\n",
        "    arquivo_vocab = \"vocab.txt\"\n",
        "\n",
        "    # Caminho do arquivo na URL_MODELO\n",
        "    caminho = URL_MODELO[0:len(URL_MODELO)-len(arquivo)]\n",
        "\n",
        "    # Verifica se a pasta de descompactação existe na pasta corrente\n",
        "    if os.path.exists(DIRETORIO_MODELO):\n",
        "      print(\"Apagando diretório existente do modelo!\")\n",
        "      # Apaga a pasta e os arquivos existentes\n",
        "      !rm -rf $DIRETORIO_MODELO      \n",
        "    \n",
        "    # Baixa o arquivo do modelo\n",
        "    !wget $URL_MODELO\n",
        "    # Descompacta o arquivo na pasta de descompactação\n",
        "    !unzip -o $arquivo -d $DIRETORIO_MODELO\n",
        "\n",
        "    # Baixa o arquivo do vocabulário\n",
        "    # O vocabulário não está no arquivo compactado acima, mesma url mas arquivo diferente\n",
        "    URL_MODELO_VOCAB = caminho + arquivo_vocab\n",
        "    !wget $URL_MODELO_VOCAB\n",
        "    \n",
        "    # Coloca o arquivo do vocabulário no diretório de descompactação\n",
        "    !mv $arquivo_vocab $DIRETORIO_MODELO\n",
        "            \n",
        "    # Move o arquivo para pasta de descompactação\n",
        "    !mv $arquivo $DIRETORIO_MODELO\n",
        "       \n",
        "    print('Pasta do ' + DIRETORIO_MODELO + ' pronta!')\n",
        "    \n",
        "    # Lista a pasta corrente\n",
        "    !ls -la $DIRETORIO_MODELO\n",
        "else:\n",
        "    print('Variável URL_MODELO não setada!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-28 09:15:49--  https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip\n",
            "Resolving neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)... 52.219.96.184\n",
            "Connecting to neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)|52.219.96.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 406220891 (387M) [application/zip]\n",
            "Saving to: ‘bert-base-portuguese-cased_pytorch_checkpoint.zip’\n",
            "\n",
            "bert-base-portugues 100%[===================>] 387.40M  87.7MB/s    in 4.6s    \n",
            "\n",
            "2021-05-28 09:15:54 (83.5 MB/s) - ‘bert-base-portuguese-cased_pytorch_checkpoint.zip’ saved [406220891/406220891]\n",
            "\n",
            "Archive:  bert-base-portuguese-cased_pytorch_checkpoint.zip\n",
            "  inflating: /content/modelo/config.json  \n",
            "  inflating: /content/modelo/pytorch_model.bin  \n",
            "--2021-05-28 09:15:59--  https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/vocab.txt\n",
            "Resolving neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)... 52.219.103.66\n",
            "Connecting to neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)|52.219.103.66|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 209528 (205K) [text/plain]\n",
            "Saving to: ‘vocab.txt’\n",
            "\n",
            "vocab.txt           100%[===================>] 204.62K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2021-05-28 09:16:00 (2.22 MB/s) - ‘vocab.txt’ saved [209528/209528]\n",
            "\n",
            "Pasta do /content/modelo pronta!\n",
            "total 824888\n",
            "drwxr-xr-x 2 root root      4096 May 28 09:16 .\n",
            "drwxr-xr-x 1 root root      4096 May 28 09:16 ..\n",
            "-rw-r--r-- 1 root root 406220891 Jan 22  2020 bert-base-portuguese-cased_pytorch_checkpoint.zip\n",
            "-rw-rw-r-- 1 root root       873 Jan 12  2020 config.json\n",
            "-rw-rw-r-- 1 root root 438235074 Jan 12  2020 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root    209528 Jan 21  2020 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcpd9t9PpkrX"
      },
      "source": [
        "## 3 - Carregando o Tokenizador BERT\n",
        "\n",
        "O tokenizador utiliza WordPiece, veja em [artigo original](https://arxiv.org/pdf/1609.08144.pdf).\n",
        "\n",
        "Carregando o tokenizador da pasta '/content/modelo/' do diretório padrão se variável `URL_MODELO` setada.\n",
        "\n",
        "**Caso contrário carrega da comunidade**\n",
        "\n",
        "Por default(`do_lower_case=True`) todas as letras são colocadas para minúsculas. Para ignorar a conversão para minúsculo use o parâmetro `do_lower_case=False`. Esta opção também considera as letras acentuadas(ãçéí...), que são necessárias a língua portuguesa.\n",
        "\n",
        "O parâmetro `do_lower_case` interfere na quantidade tokens a ser gerado apartir de um texto. Quando igual a `False` reduz a quantidade de tokens gerados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8cKVs4fpkrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c11b7bee-5a93-4552-a836-079fd0d3932b"
      },
      "source": [
        "# Importando as bibliotecas do tokenizador\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Se a variável URL_MODELO foi setada\n",
        "if DIRETORIO_MODELO:\n",
        "    # Carregando o Tokenizador\n",
        "    print('Carrgando o tokenizador BERT do diretório ' + DIRETORIO_MODELO + '...')\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(DIRETORIO_MODELO, \n",
        "                                              do_lower_case=False)    \n",
        "else:\n",
        "    # Carregando o Tokenizador da comunidade\n",
        "    print('Carregando o tokenizador da comunidade...')\n",
        "    \n",
        "    #tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)\n",
        "    tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased', \n",
        "                                              do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Carrgando o tokenizador BERT do diretório /content/modelo...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m__On2g1a--K"
      },
      "source": [
        "## 4 - Carregando o Modelo BERT(BertModel)\n",
        "\n",
        "Se a variável `URL_MODELO` estiver setada carrega o modelo do diretório `content/modelo`.\n",
        "\n",
        "Caso contrário carrega da comunidade.\n",
        "\n",
        "Carregando o modelo da pasta '/content/modelo/' do diretório padrão.\n",
        "\n",
        "A implementação do huggingface pytorch inclui um conjunto de interfaces projetadas para uma variedade de tarefas de PNL. Embora essas interfaces sejam todas construídas sobre um modelo treinado de BERT, cada uma possui diferentes camadas superiores e tipos de saída projetados para acomodar suas tarefas específicas de PNL.\n",
        "\n",
        "A documentação para estas pode ser encontrada em [aqui](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html).\n",
        "\n",
        "Por default o modelo está em modo avaliação ou seja `model.eval()`.\n",
        "\n",
        "-----------------------\n",
        "\n",
        "Durante a avaliação do modelo, este retorna um número de diferentes objetos com base em como é configurado na chamada do método `from_pretrained`. \n",
        "\n",
        "Quando definimos `output_hidden_states = True` na chamada do método `from_pretrained`, retorno do modelo possui no terceiro item os estados ocultos(**hidden_states**) de todas as camadas.  Veja a documentação para mais detalhes: https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "\n",
        "Quando **`output_hidden_states = True`** model retorna:\n",
        "- outputs[0] = last_hidden_state;\n",
        "- outputs[1] = pooler_output; \n",
        "- outputs[2] = hidden_states.\n",
        "\n",
        "Quando **`output_hidden_states = False`** ou não especificado model retorna:\n",
        "- outputs[0] = last_hidden_state;\n",
        "- outputs[1] = pooler_output.\n",
        "\n",
        "\n",
        "**ATENÇÃO**: O parâmetro ´**output_hidden_states = True**´ habilita gerar as camadas ocultas do modelo. Caso contrário somente a última camada é mantida. Este parâmetro otimiza a memória mas não os resultados.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRV6l_I-qg9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c0bb82d-17e1-42e4-d8c9-65e7f5b2b6e6"
      },
      "source": [
        "# Importando as bibliotecas do Modelo\n",
        "from transformers import BertModel\n",
        "\n",
        "# Se a variável URL_MODELO1 foi setada\n",
        "if URL_MODELO:\n",
        "    # Carregando o Tokenizador\n",
        "    print('Carregando o modelo BERT do diretório ' + DIRETORIO_MODELO + '...')\n",
        "\n",
        "    model = BertModel.from_pretrained(DIRETORIO_MODELO, \n",
        "                                      output_attentions = True,\n",
        "                                      output_hidden_states = True)    \n",
        "else:\n",
        "    # Carregando o Tokenizador da comunidade\n",
        "    print('Carregando o modelo BERT da comunidade ...')\n",
        "\n",
        "    model = BertModel.from_pretrained('neuralmind/bert-large-portuguese-cased', \n",
        "                                      output_attentions = True,\n",
        "                                      output_hidden_states = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Carregando o modelo BERT do diretório /content/modelo...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU3wHzNUmmBP"
      },
      "source": [
        "## 5 - Funções auxiliares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWqMsrb-ew5T"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm98RoojJcqP"
      },
      "source": [
        "# Import das bibliotecas\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s42mgtmSZ8MR"
      },
      "source": [
        "### getEmbeddingsCamadas\n",
        "\n",
        "Funções que recuperam os embeddings das camadas:\n",
        "- Primeira camada;\n",
        "- Penúltima camada;\n",
        "- Ùltima camada;\n",
        "- Soma das 4 últimas camadas;\n",
        "- Concatenação das 4 últimas camadas;\n",
        "- Soma de todas as camadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgo3EBTRZ9-3"
      },
      "source": [
        "def getEmbeddingPrimeiraCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][0]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingPenultimaCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][-2]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingUltimaCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "     \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][-1]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado    \n",
        "\n",
        "def getEmbeddingSoma4UltimasCamadas(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  embeddingCamadas = output[2][-4:]\n",
        "  # Saída: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "\n",
        "  # Usa o método `stack` para criar uma nova dimensão no tensor \n",
        "  # com a concateção dos tensores dos embeddings.        \n",
        "  #Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultadoStack = torch.stack(embeddingCamadas, dim=0)\n",
        "  # Saída: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  \n",
        "  # Realiza a soma dos embeddings de todos os tokens para as camadas\n",
        "  # Entrada: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  resultado = torch.sum(resultadoStack, dim=0)\n",
        "  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingConcat4UltimasCamadas(output):  \n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Cria uma lista com os tensores a serem concatenados\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  # Lista com os tensores a serem concatenados\n",
        "  listaConcat = []\n",
        "  # Percorre os 4 últimos\n",
        "  for i in [-1,-2,-3,-4]:\n",
        "      # Concatena da lista\n",
        "      listaConcat.append(output[2][i])\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  # Realiza a concatenação dos embeddings de todos as camadas\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = torch.cat(listaConcat, dim=-1)\n",
        "  # Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "    \n",
        "  return resultado   \n",
        "\n",
        "def getEmbeddingSomaTodasAsCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "   \n",
        "  # Retorna todas as camadas descontando a primeira(0)\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  embeddingCamadas = output[2][1:]\n",
        "  # Saída: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  # Usa o método `stack` para criar uma nova dimensão no tensor \n",
        "  # com a concateção dos tensores dos embeddings.        \n",
        "  #Entrada: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultadoStack = torch.stack(embeddingCamadas, dim=0)\n",
        "  # Saída: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "    \n",
        "  # Realiza a soma dos embeddings de todos os tokens para as camadas\n",
        "  # Entrada: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  resultado = torch.sum(resultadoStack, dim=0)\n",
        "  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "    \n",
        "  return resultado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7nx_eZ8hSlr"
      },
      "source": [
        "### getEmbeddingsVisual\n",
        "\n",
        "Função para gerar as coordenadas de plotagem a partir das sentenças de embeddings.\n",
        "\n",
        "Existe uma função para os tipos de camadas utilizadas:\n",
        "- Ùltima camada;\n",
        "- Soma das 4 últimas camadas;\n",
        "- Concatenação das 4 últimas camadas;\n",
        "- Soma de todas as camadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLdbOT8-g43V"
      },
      "source": [
        "def getEmbeddingsVisualUltimaCamada(texto, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    texto_marcado = \"[CLS] \" + texto + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    texto_tokenizado = tokenizador.tokenize(texto_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(texto_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(texto_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingUltimaCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, texto_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAf9lJJ2hZbt"
      },
      "source": [
        "def getEmbeddingsVisualSoma4UltimasCamadas(texto, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    texto_marcado = \"[CLS] \" + texto + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    texto_tokenizado = tokenizador.tokenize(texto_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(texto_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(texto_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSoma4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, texto_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XpwSN1ghpnz"
      },
      "source": [
        "def getEmbeddingsVisualConcat4UltimasCamadas(texto, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    texto_marcado = \"[CLS] \" + texto + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    texto_tokenizado = tokenizador.tokenize(texto_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(texto_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(texto_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingConcat4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, texto_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3KU1EFrnSPK"
      },
      "source": [
        "def getEmbeddingsVisualSomaTodasAsCamadas(texto, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    texto_marcado = \"[CLS] \" + texto + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    texto_tokenizado = tokenizador.tokenize(texto_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(texto_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(texto_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSomaTodasAsCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, texto_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFd1rse11DpZ"
      },
      "source": [
        "### getDocumentoTokenizado \n",
        "Retorna o documento tokenizado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvWIBFTLJ7z9"
      },
      "source": [
        "def getDocumentoTokenizado(documento, tokenizador):\n",
        "\n",
        "    # Adiciona os tokens especiais.\n",
        "    documentoMarcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Documento tokenizado\n",
        "    documentoTokenizado = tokenizador.tokenize(documentoMarcado)\n",
        "\n",
        "    return documentoTokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wvgXwN81RCz"
      },
      "source": [
        "### encontrarIndiceSubLista \n",
        "\n",
        "Retorna os índices de início e fim da sublista na lista"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abS44M4yvFxf"
      },
      "source": [
        "# Localiza os índices de início e fim de uma sublista em uma lista\n",
        "def encontrarIndiceSubLista(lista, sublista):\n",
        "    # Recupera o tamanho da lista \n",
        "    h = len(lista)\n",
        "    # Recupera o tamanho da sublista\n",
        "    n = len(sublista)    \n",
        "    skip = {sublista[i]: n - i - 1 for i in range(n - 1)}\n",
        "    i = n - 1\n",
        "    while i < h:\n",
        "        for j in range(n):\n",
        "            if lista[i - j] != sublista[-j - 1]:\n",
        "                i += skip.get(lista[i], n)\n",
        "                break\n",
        "        else:\n",
        "            indiceInicio = i - n + 1\n",
        "            indiceFim = indiceInicio + len(sublista)-1\n",
        "            return indiceInicio, indiceFim\n",
        "    return -1, -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_pnIh1h1Z_J"
      },
      "source": [
        "### getEmbeddingSentencaDocumentoEmbedding\n",
        "\n",
        "Retorna os embeddings de uma sentença apartir dos embeddings do documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSQs3O5QpJSj"
      },
      "source": [
        "def getEmbeddingSentencaDocumentoEmbedding(embeddingDocumento, documento, sentenca, tokenizador):\n",
        "  # Tokeniza o documento\n",
        "  documentoTokenizado = getDocumentoTokenizado(documento, tokenizador)  \n",
        "  #print(documentoTokenizado)\n",
        "\n",
        "  # Tokeniza a sentença\n",
        "  sentençaTokenizada = getDocumentoTokenizado(sentenca, tokenizador)\n",
        "  \n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentençaTokenizada.remove('[CLS]')\n",
        "  sentençaTokenizada.remove('[SEP]')  \n",
        "  #print(sentençaTokenizada)\n",
        "  #print(len(sentençaTokenizada))\n",
        "  \n",
        "  # Localiza os índices dos tokens da sentença no documento\n",
        "  inicio, fim = encontrarIndiceSubLista(documentoTokenizado,sentençaTokenizada)\n",
        "  #print(\"Sentença inicia em:\", inicio, \"até\", fim) \n",
        " \n",
        "  # Recupera os embeddings dos tokens da sentença a partir dos embeddings do documento\n",
        "  embeddingSentenca = embeddingDocumento[inicio:fim+1]\n",
        "  #print(\"embeddingSentenca=\", embeddingSentenca.shape)\n",
        "  \n",
        "  # Retorna o embedding da sentença no documento\n",
        "  return embeddingSentenca"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-3QzDMwmfiJ"
      },
      "source": [
        "## 6 - Exemplo sentenças de documento original e permutado utilizando embedding da última camada do BERT usando a estratégia MEAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTmrN_IRmfiO"
      },
      "source": [
        "### Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYKIVpzTmfiP",
        "outputId": "8cf52c1b-1924-437a-a24b-c98b25cc4cde"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_original = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "stringDocumentoPermutado = ' '.join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + stringDocumentoPermutado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print('{:>3} {:<12} {:>6,}'.format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLsHMNz9mfiQ"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pXg2A6zmfiR",
        "outputId": "385ade50-ee5a-4e4a-c26f-684000cee05a"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM5GUtVNmfiR"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ePiuflemfiS"
      },
      "source": [
        "# Importa a bibliteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qhHUUoAmfiS"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o970gzwhmfiS"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnXcn_dvmfiT"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSiGmlpjmfiT",
        "outputId": "b4cd7a38-f96f-4d47-c365-9e3e130251e0"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFDtOmN_mfiV"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM4Aosw4mfiV",
        "outputId": "dc2c5ba0-583e-4c68-87e2-5cc10eb2b7cf"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoOriginal = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", embeddingDocumentoOriginal.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([26, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T68Aje2tmfiW"
      },
      "source": [
        "Confirmando vetores dependentes do contexto\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeNaW--hmfiW",
        "outputId": "7bf7bc9d-8d01-4c0a-c44f-23babc9f4f48"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVDBdrHWmfiX"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkZrVaVFmfiX",
        "outputId": "d1baa53e-ab8c-4640-e807-c5fe23192490"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "embeddingSentenca1Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca1Original, tokenizer)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca2Original, tokenizer)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca3Original, tokenizer)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca4Original, tokenizer)\n",
        "\n",
        "print('Os primeiros 4 valores de cada sentença do documento original.')\n",
        "\n",
        "print('\\nSentença 1:', sentenca1Original,'-', str(embeddingSentenca1Original[:4]))\n",
        "print('Soma embedding Sentença1:', sentenca1Original,'-', str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print('\\nSentença 2:', sentenca2Original,'-', str(embeddingSentenca2Original[:4]))\n",
        "print('Soma embedding Sentença2:', sentenca2Original,'-', str(torch.sum(embeddingSentenca2Original[:4])))\n",
        "\n",
        "print('\\nSentença 3:', sentenca3Original,'-', str(embeddingSentenca3Original[:4]))\n",
        "print('Soma embedding Sentença3:', sentenca3Original,'-', str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print('\\nSentença 4:', sentenca4Original,'-', str(embeddingSentenca4Original[:4]))\n",
        "print('Soma embedding Sentença4:', sentenca4Original,'-', str(torch.sum(embeddingSentenca4Original[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: Bom Dia, professor. - tensor([[-0.0680, -0.4615,  0.3552,  ..., -0.3943, -0.1818, -0.4821],\n",
            "        [-0.1000, -0.0630,  0.0840,  ..., -0.6630,  0.1641, -0.8297],\n",
            "        [-0.3165,  0.4208,  0.2178,  ..., -0.4981,  0.1935, -0.3366],\n",
            "        [ 0.1248,  0.2383,  0.8987,  ..., -0.4940, -0.4578, -0.0353]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-13.8934)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.5894, -0.4310,  0.1449,  ...,  0.1601, -0.2918, -0.5303],\n",
            "        [ 0.1349, -0.2476,  0.4605,  ..., -0.3036, -0.6972,  0.2135],\n",
            "        [ 0.4359, -0.6972,  0.4066,  ...,  0.0177, -0.5852, -0.0615],\n",
            "        [ 0.0544,  0.1606,  0.4150,  ..., -0.3822, -0.1052, -0.0296]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-13.9466)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[-0.3652, -0.5015, -0.1626,  ...,  0.0161, -0.5967,  0.1675],\n",
            "        [ 0.1421,  0.1797, -0.0014,  ..., -0.5730, -0.5169,  0.3205],\n",
            "        [-0.1274, -0.0926, -0.1861,  ...,  0.4762, -0.4671,  0.2165],\n",
            "        [-0.2886,  0.4056,  0.6759,  ...,  0.5541, -0.3019, -0.1783]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-7.3502)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[-0.0835, -0.4042,  0.4330,  ..., -0.3271,  0.2262, -0.4599],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.4231, -0.3089, -0.5329],\n",
            "        [-0.5474, -0.0118,  0.3950,  ..., -0.0949, -0.3534, -0.3717],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ...,  0.1161,  0.0407, -0.6904]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-9.1314)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exqsxerrmfiY"
      },
      "source": [
        "Examinando os embeddings do documento original\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5py_A7lVmfiY",
        "outputId": "330c6eed-715e-4dae-9f34-a4c1800b523e"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaOriginal = tokenizer.tokenize(sentenca1Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1TokenizadaOriginal)\n",
        "embeddingSentenca1Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca1Original, tokenizer)\n",
        "print('\\nSentença 1 Original=\\'', sentenca1Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca1Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaOriginal = tokenizer.tokenize(sentenca2Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2TokenizadaOriginal)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca2Original, tokenizer)\n",
        "print('\\nSentença 2 Original=\\'', sentenca2Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca2TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca2Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca2Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaOriginal = tokenizer.tokenize(sentenca3Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3TokenizadaOriginal)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca3Original, tokenizer)\n",
        "print('\\nSentença 3 Original=\\'', sentenca3Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca3TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca3Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca3Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaOriginal = tokenizer.tokenize(sentenca4Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4TokenizadaOriginal)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca4Original, tokenizer)\n",
        "print('\\nSentença 4 Original=\\'', sentenca4Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca4Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Original))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=' Bom Dia, professor. '\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 768])\n",
            "    Soma embeddings:  -15.02\n",
            "\n",
            "Sentença 2 Original=' Qual o conteúdo da prova? '\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -17.08\n",
            "\n",
            "Sentença 3 Original=' Vai cair tudo na prova? '\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -10.65\n",
            "\n",
            "Sentença 4 Original=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -12.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hg9eKyjEfE5"
      },
      "source": [
        "### Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqHzON3PCa49",
        "outputId": "81e8517a-7c2b-4c7e-ef71-2c110e8ecbb7"
      },
      "source": [
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_permutado = [documento_original[3],   # \"Aguardo uma resposta, João.\",\n",
        "             documento_original[1],             # \"Qual o conteúdo da prova?\",              \n",
        "             documento_original[0],             # \"Vai cair tudo na prova?\",\n",
        "             documento_original[2]]             # \"Bom Dia, professor.\"]     \n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "stringDocumentoPermutado = ' '.join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + stringDocumentoPermutado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print('{:>3} {:<12} {:>6,}'.format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 Bom           8,399\n",
            " 15 Dia           3,616\n",
            " 16 ,               117\n",
            " 17 professor     2,917\n",
            " 18 .               119\n",
            " 19 Vai          20,805\n",
            " 20 cair          9,322\n",
            " 21 tudo          2,745\n",
            " 22 na              229\n",
            " 23 prova         2,310\n",
            " 24 ?               136\n",
            " 25 [SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5Snv8-ACy47",
        "outputId": "a1bf0a25-bf6a-4fd0-b867-011170cc5210"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLv52fBItM3I"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMFR5tSiCy48"
      },
      "source": [
        "# Importa a bibliteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDFnt2yntIgn"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je2zyykXCy49"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZSIxolutQSp"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z09FmGtnCy49",
        "outputId": "b032d86c-cc26-4ba3-ee85-cf8b66da39e9"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aetb3LVYtXnI"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkk3Ix9kC93C",
        "outputId": "c6e57c2e-46ef-44c6-f3bb-cbba044772ed"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoPermutado = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", embeddingDocumentoPermutado.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([26, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIsMSKxNIUg9"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkHr7wEFIUhA",
        "outputId": "a97b5b65-392a-42df-e512-3c769b9016fa"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca1Permutado, tokenizer)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca2Permutado, tokenizer)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca3Permutado, tokenizer)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca4Permutado, tokenizer)\n",
        "\n",
        "print('Os primeiros 4 valores de cada sentença do documento permutado.')\n",
        "\n",
        "print('\\nSentença 1:', sentenca1Permutado,'-', str(embeddingSentenca1Permutado[:4]))\n",
        "print('Soma embedding Sentença1:', sentenca1Original,'-', str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print('\\nSentença 2:', sentenca2Permutado,'-', str(embeddingSentenca2Permutado[:4]))\n",
        "print('Soma embedding Sentença2:', sentenca2Permutado,'-', str(torch.sum(embeddingSentenca2Permutado[:4])))\n",
        "\n",
        "print('\\nSentença 3:', sentenca3Permutado,'-', str(embeddingSentenca3Permutado[:4]))\n",
        "print('Soma embedding Sentença3:', sentenca3Permutado,'-', str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print('\\nSentença 4:', sentenca4Permutado,'-', str(embeddingSentenca4Permutado[:4]))\n",
        "print('Soma embedding Sentença4:', sentenca4Permutado,'-', str(torch.sum(embeddingSentenca4Permutado[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.4811,  0.2881, -0.4057],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.5534, -0.3164, -0.3582],\n",
            "        [-0.7358, -0.0988,  0.5145,  ..., -0.1814, -0.3193, -0.3159],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.0720, -0.0478, -0.4931]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-13.8934)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.4295, -0.3426,  0.1005,  ...,  0.1568, -0.3225, -0.3857],\n",
            "        [ 0.1047, -0.1946,  0.4177,  ..., -0.3388, -0.7529,  0.1693],\n",
            "        [ 0.4377, -0.5952,  0.5448,  ...,  0.0463, -0.5607, -0.1783],\n",
            "        [ 0.0898,  0.1063,  0.4610,  ..., -0.3313, -0.1764,  0.0920]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-13.8054)\n",
            "\n",
            "Sentença 3: Bom Dia, professor. - tensor([[-0.0458, -0.4268,  0.2399,  ..., -0.4504, -0.2225, -0.6343],\n",
            "        [ 0.0650, -0.1385,  0.0230,  ..., -0.7129,  0.1112, -0.8076],\n",
            "        [-0.1147,  0.4120,  0.0659,  ..., -0.3666,  0.0509, -0.2134],\n",
            "        [ 0.1290,  0.2181,  0.7986,  ..., -0.4771, -0.4328,  0.0041]])\n",
            "Soma embedding Sentença3: Bom Dia, professor. - tensor(-7.3502)\n",
            "\n",
            "Sentença 4: Vai cair tudo na prova? - tensor([[-0.0414, -0.5222, -0.1612,  ..., -0.0763, -0.6968,  0.0127],\n",
            "        [ 0.4540,  0.0075, -0.0953,  ..., -0.4919, -0.4762,  0.2358],\n",
            "        [ 0.0887, -0.1633, -0.3073,  ...,  0.4414, -0.5028,  0.0877],\n",
            "        [-0.0159,  0.2363,  0.7514,  ...,  0.3057, -0.4097, -0.2738]])\n",
            "Soma embedding Sentença4: Vai cair tudo na prova? - tensor(-4.3409)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MINDqF2LDA9z",
        "outputId": "db353c53-bacb-4a18-b43a-d40af1d92e4b"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaPermutado = tokenizer.tokenize(sentenca1Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1TokenizadaPermutado)\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca1Permutado, tokenizer)\n",
        "print('\\nSentença 1 Permutada=\\'', sentenca1Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca1Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaPermutado = tokenizer.tokenize(sentenca2Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2TokenizadaPermutado)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca2Permutado, tokenizer)\n",
        "print('\\nSentença 2 Permutada=\\'', sentenca2Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca2TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca2Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca2Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaPermutado = tokenizer.tokenize(sentenca3Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3TokenizadaPermutado)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca3Permutado, tokenizer)\n",
        "print('\\nSentença 3 Permutada=\\'', sentenca3Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca3TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca3Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca3Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaPermutado = tokenizer.tokenize(sentenca4Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4TokenizadaPermutado)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca4Permutado, tokenizer)\n",
        "print('\\nSentença 4 Permutada=\\'', sentenca4Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca4Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Permutado))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "\n",
            "Sentença 1 Permutada=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -14.89\n",
            "\n",
            "Sentença 2 Permutada=' Qual o conteúdo da prova? '\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -17.25\n",
            "\n",
            "Sentença 3 Permutada=' Bom Dia, professor. '\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 14 e término em 18\n",
            "    Formato modelo : torch.Size([5, 768])\n",
            "    Soma embeddings:  -15.11\n",
            "\n",
            "Sentença 4 Permutada=' Vai cair tudo na prova? '\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 19 e término em 24\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -6.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UILLnj7KvHi"
      },
      "source": [
        "### Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eyEbV-7Kz6r",
        "outputId": "c998a9c3-4103-4a23-d35a-99c9963c0a21"
      },
      "source": [
        "print('\\nSentença 4 Original=\\'', sentenca4Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaOriginal)\n",
        "print('    Formato modelo :', embeddingSentenca4Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Original))\n",
        "print('    Os 4 primeiros embeddings:', str(embeddingSentenca4Original[:4]))\n",
        "\n",
        "print('\\nSentença 1 Permutada=\\'', sentenca1Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaPermutado)\n",
        "print('    Formato modelo :', embeddingSentenca1Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Permutado))\n",
        "print('    Os 4 primeiros embeddings:', str(embeddingSentenca1Permutado[:4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sentença 4 Original=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -12.13\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0835, -0.4042,  0.4330,  ..., -0.3271,  0.2262, -0.4599],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.4231, -0.3089, -0.5329],\n",
            "        [-0.5474, -0.0118,  0.3950,  ..., -0.0949, -0.3534, -0.3717],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ...,  0.1161,  0.0407, -0.6904]])\n",
            "\n",
            "Sentença 1 Permutada=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -14.89\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.4811,  0.2881, -0.4057],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.5534, -0.3164, -0.3582],\n",
            "        [-0.7358, -0.0988,  0.5145,  ..., -0.1814, -0.3193, -0.3159],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.0720, -0.0478, -0.4931]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKbJXYgzDLiL"
      },
      "source": [
        "### Subtração entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExMeyUkADLiN"
      },
      "source": [
        "#### Calcula a média aritmética da subtração entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xisG3xtyDLiN",
        "outputId": "5213f6ad-5d39-42a6-925e-cfd74f536929"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSsub = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)  \n",
        "    Ssub = torch.sub(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: <768 ou 1024>  \n",
        "    #print(\"Ssub=\", Ssub.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings do subtração para realizar a soma\n",
        "    somaSsub = somaSsub + torch.mean(Ssub)\n",
        "\n",
        "DsubOriginal = float(somaSsub)/float(len(documento_original)-1)\n",
        "print(\"Ssub Original:\", DsubOriginal)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ssub Original: 0.0007491791620850563\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X59wswnRDLiO"
      },
      "source": [
        "#### Calcula a média aritmética da subtração entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5tlZm81DLiO",
        "outputId": "688087f3-764c-48a5-fc9a-475b3a98e4f0"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSsub = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Ssub = torch.sub(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: <768 ou 1024>  \n",
        "    #print(\"Ssub=\", Ssub.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings do subtração para realizar a soma\n",
        "    somaSsub = somaSsub + torch.mean(Ssub)\n",
        "\n",
        "DsubPermutado = float(somaSsub)/float(np-1)\n",
        "print(\"Ssub permutado:\", DsubPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ssub permutado: -0.0006407623489697775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LICi8HTlDLiP"
      },
      "source": [
        "#### Compara as médias da subtração dos embeddings das sentenças do documento original e permutado.\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa7CKjMsDLiP",
        "outputId": "52c8b293-e772-4dbd-996c-a9391ff438ee"
      },
      "source": [
        "print(\"Dsub Original :\", DsubOriginal)\n",
        "print(\"Dsub Permutado:\", DsubPermutado)\n",
        "\n",
        "if (DsubOriginal < DsubPermutado):\n",
        "    print(\"Documento original tem menor subtração entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor subtração entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dsub Original : 0.0007491791620850563\n",
            "Dsub Permutado: -0.0006407623489697775\n",
            "Documento Permutado tem menor subtração entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqkuDlFPDLiP"
      },
      "source": [
        "### Subtração absoluta entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYG_XabmDLiP"
      },
      "source": [
        "#### Calcula a média aritmética da subtração absoluta entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0hoMmMVDLiP",
        "outputId": "1a756fa0-69ec-4631-b352-e8f4903cce8b"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSsubabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Ssubabs = abs(torch.sub(mediaEmbeddingSi, mediaEmbeddingSj))\n",
        "    # Saída: <768 ou 1024>  \n",
        "    #print(\"Ssubabs=\", Ssubabs.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings do subtração para realizar a soma\n",
        "    somaSsubabs = somaSsubabs + torch.mean(Ssubabs)\n",
        "\n",
        "DsubabsOriginal = float(somaSsubabs)/float(n-1)\n",
        "print(\"Dsubabs Original:\", DsubabsOriginal)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dsubabs Original: 0.17778096596399942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2td0000SDLiQ"
      },
      "source": [
        "#### Calcula a média aritmética da subtração absoluta entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59h5bwSzDLiQ",
        "outputId": "fe7b8862-caff-4f49-cd83-2e7bf51e504e"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSsubabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Ssubabs = abs(torch.sub(mediaEmbeddingSi, mediaEmbeddingSj))\n",
        "    # Saída: <768 ou 1024>  \n",
        "    #print(\"Ssubabs=\", Ssubabs.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings do subtração para realizar a soma\n",
        "    somaSsubabs = somaSsubabs + torch.mean(Ssubabs)\n",
        "\n",
        "DsubabsPermutado = float(somaSsubabs)/float(np-1)\n",
        "print(\"Dsubabs permutado:\", DsubabsPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dsubabs permutado: 0.17206366856892905\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NV04x6oDLiQ"
      },
      "source": [
        "#### Compara as médias da subtração absoluta dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "craekVcHDLiQ",
        "outputId": "fa72f844-201a-4476-c38b-e573ef68a034"
      },
      "source": [
        "print(\"Dsubabs Original :\", DsubabsOriginal)\n",
        "print(\"Dsubabs Permutado:\", DsubabsPermutado)\n",
        "\n",
        "if (DsubabsOriginal < DsubabsPermutado):\n",
        "    print(\"Documento original tem menor subtração absoluta entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor subtração absoluta entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dsubabs Original : 0.17778096596399942\n",
            "Dsubabs Permutado: 0.17206366856892905\n",
            "Documento Permutado tem menor subtração absoluta entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msxo3687DLiQ"
      },
      "source": [
        "### Produto entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d34R3QJDLiR"
      },
      "source": [
        "#### Calcula a média aritmética do produto das matrizes dos tokens das sentenças do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfMrftuXDLiR",
        "outputId": "afed726f-239e-448f-9c31-98dda94e61ad"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSprod = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Para a multiplicação pode ocorrer número de colunas(qtde_tokens) de Si tem que ser igual ao número de linhas(embeddings) de Sj.\n",
        "    # Permute realiza a troca da dimensão 1(qtde_embeddings) pela 0(qtde_tokesn) na sentença Sj para que seja possível realizar o produto. \n",
        "    # Ocorre a troca da qtde_tokens pela quantidade de embeddings.\n",
        "    # Entrada: (<qtde_tokensSi> x <768 ou 1024>) x (<qtde_tokensSj> x <768 ou 1024>)\n",
        "    # Permute: <qtde_tokensSi> x <768 ou 1024>) x (<768 ou 1024> x <qtde_tokensSj>)\n",
        "    Sprod = torch.matmul(embeddingSi, embeddingSj.permute(1,0))    \n",
        "    # Saída: <qtde_tokensSi> x  <qtde_tokensSj    #print(\"Sprod=\", Sprod.shape)\n",
        "    \n",
        "    # Calcula a média dos embeddings do produto para realizar a soma\n",
        "    somaSprod = somaSprod + torch.mean(Sprod)\n",
        "\n",
        "DprodOriginal = float(somaSprod)/float(n-1)\n",
        "print(\"Dprod Original:\", DprodOriginal)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dprod Original: 43.7073974609375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpwrTwhbDLiR"
      },
      "source": [
        "#### Calcula a média aritmética do produto das matrizes dos tokens das sentenças do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IxUeXUqDLiR",
        "outputId": "636d16da-f52a-4cb8-a586-41a13dffaf5f"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSprod = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        " \n",
        "    # Produto entre os embeddings de Si e Sj    \n",
        "    # Para a multiplicação pode ocorrer número de colunas(qtde_tokens) de Si tem que ser igual ao número de linhas(embeddings) de Sj.\n",
        "    # Permute realiza a troca da dimensão 1(qtde_embeddings) pela 0(qtde_tokesn) na sentença Sj para que seja possível realizar o produto. \n",
        "    # Ocorre a troca da qtde_tokens pela quantidade de embeddings.\n",
        "    # Entrada: (<qtde_tokensSi> x <768 ou 1024>) x (<qtde_tokensSj> x <768 ou 1024>)\n",
        "    # Permute: <qtde_tokensSi> x <768 ou 1024>) x (<768 ou 1024> x <qtde_tokensSj>)\n",
        "    Sprod = torch.matmul(embeddingSi, embeddingSj.permute(1,0))    \n",
        "    # Saída: <qtde_tokensSi> x  <qtde_tokensSj>\n",
        "    #print(\"Sprod=\", Sprod.shape)\n",
        "        \n",
        "    # Calcula a média dos embeddings do produto para realizar a soma\n",
        "    somaSprod = somaSprod + torch.mean(Sprod)\n",
        "\n",
        "DprodPermutado = float(somaSprod)/float(np-1)\n",
        "print(\"Dprod permutado:\", DprodPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dprod permutado: 43.55047607421875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9Q-FPL7DLiR"
      },
      "source": [
        "#### Compara as médias do produto dos embeddings das sentenças do documento original e permutado\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3hCGiRkDLiS",
        "outputId": "c4fd109b-dcbb-422d-f57d-5479df23ab9f"
      },
      "source": [
        "print(\"Dprod Original :\", DprodOriginal)\n",
        "print(\"Dprod Permutado:\", DprodPermutado)\n",
        "\n",
        "if (DprodOriginal < DprodPermutado):\n",
        "    print(\"Documento original tem maior similaridade com o produto entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade com o produto entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dprod Original : 43.7073974609375\n",
            "Dprod Permutado: 43.55047607421875\n",
            "Documento Permutado tem menor similaridade com o produto entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z89J3nEDLiS"
      },
      "source": [
        "### Produto Absoluto entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pp7lOeNDLiS"
      },
      "source": [
        "#### Calcula a média aritmética do produto das matrizes entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6zI663LDLiS",
        "outputId": "908c2256-17b5-4bf1-bc49-fd59eecba147"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSprodabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "    \n",
        "    # Produto entre os embeddings de Si e Sj\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>\n",
        "    # Para a multiplicação pode ocorrer número de colunas(qtde_tokens) de Si tem que ser igual ao número de linhas(embeddings) de Sj.\n",
        "    # Permute realiza a troca da dimensão 1(qtde_embeddings) pela 0(qtde_tokesn) na sentença Sj para que seja possível realizar o produto. \n",
        "    # Ocorre a troca da qtde_tokens pela quantidade de embeddings.\n",
        "    # Entrada: (<qtde_tokensSi> x <768 ou 1024>) x (<qtde_tokensSj> x <768 ou 1024>)\n",
        "    # Permute: <qtde_tokensSi> x <768 ou 1024>) x (<768 ou 1024> x <qtde_tokensSj>)\n",
        "    Sprodabs = abs(torch.matmul(embeddingSi, embeddingSj.permute(1,0)))   \n",
        "    # Saída: <qtde_tokensSi> x  <qtde_tokensSj>\n",
        "    #print(\"Sprodabs=\", Sprodabs.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings do produto para realizar a soma\n",
        "    somaSprodabs = somaSprodabs + torch.mean(Sprodabs)\n",
        "\n",
        "DprodabsOriginal = float(somaSprodabs)/float(n-1)\n",
        "print(\"Dprodabs Original:\", DprodabsOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dprodabs Original: 43.7073974609375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9sQZBG0DLiS"
      },
      "source": [
        "#### Calcula a média aritmética do produto das matrizes entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6YSTMbCDLiS",
        "outputId": "301423ec-1a9d-4c16-edd4-c3855254ae7a"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSprodabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        " \n",
        "    # Produto entre os embeddings de Si e Sj\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>\n",
        "    # Para a multiplicação pode ocorrer número de colunas(qtde_tokens) de Si tem que ser igual ao número de linhas(embeddings) de Sj.\n",
        "    # Permute realiza a troca da dimensão 1(qtde_embeddings) pela 0(qtde_tokesn) na sentença Sj para que seja possível realizar o produto. \n",
        "    # Ocorre a troca da qtde_tokens pela quantidade de embeddings.\n",
        "    # Entrada: (<qtde_tokensSi> x <768 ou 1024>) x (<qtde_tokensSj> x <768 ou 1024>)\n",
        "    # Permute: <qtde_tokensSi> x <768 ou 1024>) x (<768 ou 1024> x <qtde_tokensSj>)\n",
        "    Sprodabs = abs(torch.matmul(embeddingSi, embeddingSj.permute(1,0)))    \n",
        "    # Saída: <qtde_tokensSi> x  <qtde_tokensSj>\n",
        "    #print(\"Sprodabs=\", Sprodabs.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings do produto para realizar a soma\n",
        "    somaSprodabs = somaSprodabs + torch.mean(Sprodabs)\n",
        "\n",
        "DprodabsPermutado = float(somaSprodabs)/float(np-1)\n",
        "print(\"Dprodabs permutado:\", DprodabsPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dprodabs permutado: 43.55047607421875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odvuPQdQDLiT"
      },
      "source": [
        "#### Compara as médias do produto absoluto dos embeddings das sentenças do documento original e permutado\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6rXOTBJDLiU",
        "outputId": "70ef9395-df89-489a-edf7-9a563ccfc841"
      },
      "source": [
        "print(\"Dprodabs Original :\", DprodabsOriginal)\n",
        "print(\"Dprodabs Permutado:\", DprodabsPermutado)\n",
        "\n",
        "if (DprodabsOriginal < DprodabsPermutado):\n",
        "    print(\"Documento original tem maior similaridade com o produto absoluto entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade com o produto absoluto entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dprodabs Original : 43.7073974609375\n",
            "Dprodabs Permutado: 43.55047607421875\n",
            "Documento Permutado tem menor similaridade com o produto absoluto entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of8s6tqVDLiU"
      },
      "source": [
        "### Produto Escalar entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzIkTe6tDLiU"
      },
      "source": [
        "#### Calcula a média aritmética do produto escalar entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento orginal. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpRxhTjEDLiU",
        "outputId": "0de31ebb-695a-4157-8a5d-1795d4fbd18b"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSprode = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "\n",
        "    # Produto escalar entre os embeddings de Si e Sj, pois Si e Sj possui uma única dimensão\n",
        "    # https://pytorch.org/docs/master/generated/torch.matmul.html#torch.matmul\n",
        "    # Entrada: <768 ou 1024> * <768 ou 1024>\n",
        "    Sprode = torch.matmul(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real \n",
        "    #print(\"Sprode=\", Sprode.shape)\n",
        "        \n",
        "    somaSprode = somaSprode + Sprode\n",
        "\n",
        "DprodeOriginal = float(somaSprode)/float(n-1)\n",
        "print(\"Dprode Original:\", DprodeOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dprode Original: 43.7073974609375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8RyYYzwDLiV"
      },
      "source": [
        "#### Calcula a média aritmética do produto escalar entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfu-CfTIDLiV",
        "outputId": "4ab14793-ed05-46c8-ff66-0b75b1896eea"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSprode = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Produto escalar entre os embeddings de Si e Sj, pois Si e Sj possui uma única dimensão\n",
        "    # https://pytorch.org/docs/master/generated/torch.matmul.html#torch.matmul\n",
        "    # Entrada: <768 ou 1024> * <768 ou 1024>\n",
        "    Sprode = torch.matmul(mediaEmbeddingSi, mediaEmbeddingSj)    \n",
        "    # Saída: Um número real \n",
        "    #print(\"Sprode=\", Sprode)\n",
        "        \n",
        "    somaSprode = somaSprode + Sprode\n",
        "\n",
        "DprodePermutado = float(somaSprode)/float(np-1)\n",
        "print(\"Dprod permutado:\", DprodePermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dprod permutado: 43.55047607421875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCsb1yzdDLiV"
      },
      "source": [
        "#### Compara as médias do produto escalar dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Dprod Original : 230.61181640625\n",
        "Dprod Permutado: 227.75482177734375\n",
        "Documento original tem maior similaridade com o produto entre as sentenças!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3-AGwdpDLiV",
        "outputId": "aef41b74-76b4-4210-eebe-9df4dcf9f7e5"
      },
      "source": [
        "print(\"Dprode Original :\", DprodeOriginal)\n",
        "print(\"Dprode Permutado:\", DprodePermutado)\n",
        "\n",
        "if (DprodeOriginal < DprodePermutado):\n",
        "    print(\"Documento original tem maior similaridade com o produto entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade com o produto entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dprode Original : 43.7073974609375\n",
            "Dprode Permutado: 43.55047607421875\n",
            "Documento Permutado tem menor similaridade com o produto entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yDBI2T8DLiV"
      },
      "source": [
        "### Produto Escalar Absoluto entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX22s8RuDLiV"
      },
      "source": [
        "#### Calcula a média aritmética do produto escalar absoluto entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c2gHIUrDLiW",
        "outputId": "3f158506-392d-4dc7-a3bf-ba38daf2142d"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSprodeabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Produto escalar entre os embeddings de Si e Sj, pois Si e Sj possui uma única dimensão\n",
        "    # https://pytorch.org/docs/master/generated/torch.matmul.html#torch.matmul\n",
        "    # Entrada: <768 ou 1024>\n",
        "    Sprodeabs = torch.matmul(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real \n",
        "    #print(\"Sprodeabs=\", Sprodeabs.shape)\n",
        "\n",
        "    somaSprodeabs = somaSprodeabs + Sprodeabs\n",
        "\n",
        "DprodeabsOriginal = float(somaSprodeabs)/float(n-1)\n",
        "print(\"Dprodabs Original:\", DprodeabsOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dprodabs Original: 43.7073974609375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfO6QdLeDLiW"
      },
      "source": [
        "#### Calcula a média aritmética do produto escalar absoluto entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMhilBUvDLiW",
        "outputId": "43e6a0c6-628f-44e7-d6a7-c608d8bfcb50"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSprodeabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "    \n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "    \n",
        "    # Produto escalar entre os embeddings de Si e Sj, pois Si e Sj possui uma única dimensão\n",
        "    # https://pytorch.org/docs/master/generated/torch.matmul.html#torch.matmul\n",
        "    # Entrada: <768 ou 1024>\n",
        "    Sprodeabs = torch.matmul(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real \n",
        "    #print(\"Sprodeabs=\", Sprodeabs.shape)\n",
        "    \n",
        "    somaSprodeabs = somaSprodeabs + Sprodeabs\n",
        "\n",
        "DprodeabsPermutado = float(somaSprodeabs)/float(np-1)\n",
        "print(\"Dprodabs permutado:\", DprodeabsPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dprodabs permutado: 43.55047607421875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1YqJVs9DLiW"
      },
      "source": [
        "#### Compara as médias do produto escalar absoluto dos embeddings das sentenças do documento original e permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mKpaRh4DLiW",
        "outputId": "ead20170-5af6-415e-9646-485d141e7fb1"
      },
      "source": [
        "print(\"Dprodabs Original :\", DprodeabsOriginal)\n",
        "print(\"Dprodabs Permutado:\", DprodeabsPermutado)\n",
        "\n",
        "if (DprodeabsOriginal < DprodeabsPermutado):\n",
        "    print(\"Documento original tem maior similaridade com o produto absoluto entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade com o produto absoluto entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dprodabs Original : 43.7073974609375\n",
            "Dprodabs Permutado: 43.55047607421875\n",
            "Documento Permutado tem menor similaridade com o produto absoluto entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4bvEEhXDLiW"
      },
      "source": [
        "### Média entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx33nenRDLiX"
      },
      "source": [
        "#### Calcula a média aritmética entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb0q-6YwDLiX",
        "outputId": "42557109-be1b-49d3-da34-a5e46c97c682"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSavg = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Média entre os embeddings de Si e Sj\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    Savg = (mediaEmbeddingSi.add(mediaEmbeddingSj))/2.0  \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"Savg=\", Savg.shape)  \n",
        "\n",
        "    # Calcula a média dos embeddings do cálculo para realizar a soma\n",
        "    somaSavg = somaSavg + torch.mean(Savg)\n",
        "\n",
        "DavgOriginal = float(somaSavg)/float(n-1)\n",
        "print(\"Davg Original:\", DavgOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Davg Original: -0.002767913043498993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWCHtwwvDLiX"
      },
      "source": [
        "#### Calcula a média aritmética entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgE1kIaADLiX",
        "outputId": "a4347920-6957-47fb-abcf-32d8e5cb1e6f"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSavg = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Média entre os embeddings de Si e Sj\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    Savg = (mediaEmbeddingSi.add(mediaEmbeddingSj))/2.0  \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"Savg=\", Savg.shape)  \n",
        "\n",
        "    # Calcula a média dos embeddings do cálculo para realizar a soma\n",
        "    somaSavg = somaSavg + torch.mean(Savg)\n",
        "\n",
        "DavgPermutado = float(somaSavg)/float(np-1)\n",
        "print(\"Davg permutado:\", DavgPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Davg permutado: -0.002713705413043499\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-R3jqSqDLiX"
      },
      "source": [
        "#### Compara as médias dos embeddings das sentenças do documento original e permutado\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCELVBhXDLiX",
        "outputId": "558cf9e3-5f65-4aa4-eba8-ca674ca5039c"
      },
      "source": [
        "print(\"Davg Original :\", DavgOriginal)\n",
        "print(\"Davg Permutado:\", DavgPermutado)\n",
        "\n",
        "if (DavgOriginal < DavgPermutado):\n",
        "    print(\"Documento original tem maior similaridade com a média entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade com a mpedia entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Davg Original : -0.002767913043498993\n",
            "Davg Permutado: -0.002713705413043499\n",
            "Documento original tem maior similaridade com a média entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JplTToZvDLiX"
      },
      "source": [
        "### Similaridade de cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYUdCeaSDLiY"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCoseno(sentenca1, sentenca2):\n",
        "  similaridade = 1 - cosine(sentenca1, sentenca2)\n",
        "  return similaridade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av6tZHt6DLiY"
      },
      "source": [
        "#### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOQ9vWuADLiY",
        "outputId": "d128205f-bae7-4e10-d8e1-faced58b48e3"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCoseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "DcosOriginal = float(somaScos)/float(n-1)\n",
        "print(\"Dcos Original:\", DcosOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dcos Original: 0.6799910068511963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmTaSFZNDLiY"
      },
      "source": [
        "#### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYIO7AXCDLiY",
        "outputId": "ca5ac61c-655c-48ff-f0d8-cd58eb447a00"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "   # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "   # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCoseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "DcosPermutado = float(somaScos)/float(np-1)\n",
        "print(\"Dcos Original:\", DcosPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dcos Original: 0.696538249651591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiJ_9-KPDLiY"
      },
      "source": [
        "#### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 1.\n",
        "- Documentos com sentenças diferenntes resulta uma medida menor que 1.\n",
        "- Documento com sentenças muito diferentes apresentam valores menores que 1.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de similaridade.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ1pRGiEDLiY",
        "outputId": "a1658834-745e-420e-a859-90bfb42cc429"
      },
      "source": [
        "print(\"Dcos Original :\", DcosOriginal)\n",
        "print(\"Dcos Permutado:\", DcosPermutado)\n",
        "\n",
        "if (DcosOriginal > DcosPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dcos Original : 0.6799910068511963\n",
            "Dcos Permutado: 0.696538249651591\n",
            "Documento Permutado tem menor similaridade de cosseno entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tLfL6BFDLiZ"
      },
      "source": [
        "### Distância euclidiana entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância L2 ou norma L2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKrR5hMNDLiZ"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distanciaEuclidiana(sentenca1, sentenca2):\n",
        "  distancia = euclidean(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scio7VcxDLiZ"
      },
      "source": [
        "#### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qPGyX3WDLiZ",
        "outputId": "b422617b-f6ef-4d2f-f02c-e6492a206a55"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "DeucOriginal = float(somaSeuc)/float(n-1)\n",
        "print(\"Deuc Original:\", DeucOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Deuc Original: 6.340852737426758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eiJkQ9tDLiZ"
      },
      "source": [
        "#### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCa8HJAjDLiZ",
        "outputId": "113ed70f-48ca-4c45-9cb3-da641e22b593"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "DeucPermutado = float(somaSeuc)/float(np-1)\n",
        "print(\"Deuc Original:\", DeucPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Deuc Original: 6.154866059621175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz5eaOkEDLiZ"
      },
      "source": [
        "#### Compara as médias da distância euclidiana dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhelRMqGDLia",
        "outputId": "e79db937-8524-4f02-df0e-72fb806e9de0"
      },
      "source": [
        "print(\"Deuc Original :\", DeucOriginal)\n",
        "print(\"Deuc Permutado:\", DeucPermutado)\n",
        "\n",
        "if (DeucOriginal > DeucPermutado):\n",
        "    print(\"Documento original tem maior distância euclidiana entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor distância euclidiana entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deuc Original : 6.340852737426758\n",
            "Deuc Permutado: 6.154866059621175\n",
            "Documento original tem maior distância euclidiana entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqPCQJ24DLia"
      },
      "source": [
        "### Distância Manhattan entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCjpuWTRDLib"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def distanciaEManhattan(sentenca1, sentenca2):\n",
        "  distancia = cityblock(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKSaAqoZDLib"
      },
      "source": [
        "#### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFLah0Q9DLib",
        "outputId": "a8f6b47c-cccd-431a-abcb-9c39af8d05ce"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaEManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "DmanOriginal = float(somaSman)/float(n-1)\n",
        "print(\"Dman Original:\", DmanOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dman Original: 136.53579711914062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43BxjteRDLib"
      },
      "source": [
        "#### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E6e9k1YDLic",
        "outputId": "bca96a05-6201-4619-e8d3-ca66a978aebd"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaEManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "DmanPermutado = float(somaSman)/float(np-1)\n",
        "print(\"Deuc Original:\", DmanPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Deuc Original: 132.14489237467447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ-427FYDLic"
      },
      "source": [
        "#### Compara as médias da distância de manhattan dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3snpqLtIDLic",
        "outputId": "f1f0b071-73d0-47a7-b8c0-234a641726f6"
      },
      "source": [
        "print(\"Dman Original :\", DmanOriginal)\n",
        "print(\"Dman Permutado:\", DmanPermutado)\n",
        "\n",
        "if (DeucOriginal > DeucPermutado):\n",
        "    print(\"Documento original tem maior distância de manhattan entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor distância de manhattan entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dman Original : 136.53579711914062\n",
            "Dman Permutado: 132.14489237467447\n",
            "Documento original tem maior distância de manhattan entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJRDXLHua9ce"
      },
      "source": [
        "### Resumo\n",
        "\n",
        "Resultado das medidas utilizando a última camada do BERT.\n",
        "\n",
        "Base(MEAN):\n",
        "- Dsub       :   0.00074918          -0.00064076\n",
        "- Dubabs     :   0.17778097          0.17206367\n",
        "- Dprod      :   43.70739746          43.55047607\n",
        "- Dprodabs   :   43.70739746          43.55047607\n",
        "- Dprode     :   43.70739746          43.55047607\n",
        "- Dprodeabs  :   43.70739746          43.55047607\n",
        "- Davg       :   -0.00276791          -0.00271371\n",
        "- Dcos       :   0.67999101          0.69653825\n",
        "- Deuc       :   6.34085274          6.15486606\n",
        "- Dman       :   136.53579712          132.14489237\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4jEPcWRa9ce",
        "outputId": "4674ef09-7b39-4390-9509-050f7f2d7dd8"
      },
      "source": [
        "print(\"Resultado das medidas utilizando a última camada do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print('Dsub       :   {:.8f}          {:.8f}'.format(DsubOriginal,DsubPermutado))\n",
        "print('Dubabs     :   {:.8f}          {:.8f}'.format(DsubabsOriginal,DsubabsPermutado))\n",
        "print('Dprod      :   {:.8f}          {:.8f}'.format(DprodOriginal,DprodPermutado))\n",
        "print('Dprodabs   :   {:.8f}          {:.8f}'.format(DprodabsOriginal,DprodabsPermutado))\n",
        "print('Dprode     :   {:.8f}          {:.8f}'.format(DprodeOriginal,DprodePermutado))\n",
        "print('Dprodeabs  :   {:.8f}          {:.8f}'.format(DprodeabsOriginal,DprodeabsPermutado))\n",
        "print('Davg       :   {:.8f}          {:.8f}'.format(DavgOriginal,DavgPermutado))\n",
        "print('Dcos       :   {:.8f}          {:.8f}'.format(DcosOriginal,DcosPermutado))\n",
        "print('Deuc       :   {:.8f}          {:.8f}'.format(DeucOriginal,DeucPermutado))\n",
        "print('Dman       :   {:.8f}          {:.8f}'.format(DmanOriginal,DmanPermutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resultado das medidas utilizando a última camada do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Dsub       :   0.00074918          -0.00064076\n",
            "Dubabs     :   0.17778097          0.17206367\n",
            "Dprod      :   43.70739746          43.55047607\n",
            "Dprodabs   :   43.70739746          43.55047607\n",
            "Dprode     :   43.70739746          43.55047607\n",
            "Dprodeabs  :   43.70739746          43.55047607\n",
            "Davg       :   -0.00276791          -0.00271371\n",
            "Dcos       :   0.67999101          0.69653825\n",
            "Deuc       :   6.34085274          6.15486606\n",
            "Dman       :   136.53579712          132.14489237\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5NHv8JQ2Om8"
      },
      "source": [
        "## 7 - Exemplo sentenças de documento original e permutado utilizando embedding da última camada do BERT usando estratégia MAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuFaynIX2OnA"
      },
      "source": [
        "### Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ird39LBl2OnA",
        "outputId": "c8551af7-8cf6-4822-ae5e-6c042f53b2b4"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_original = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "stringDocumentoPermutado = ' '.join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + stringDocumentoPermutado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print('{:>3} {:<12} {:>6,}'.format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCazJyHf2OnB"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkzJFTWX2OnB",
        "outputId": "2dff519a-8c52-4e3f-ef8e-c74808325a01"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H624EpGv2OnB"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBfLtHz92OnB"
      },
      "source": [
        "# Importa a bibliteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM5nK5Zr2OnB"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0SPckuI2OnC"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceWI29Ij2OnC"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ReZJfwR2OnC",
        "outputId": "f0a53e55-61a5-4c23-dfda-1f773463bc2c"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysFToAty2OnC"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp8ImZM52OnC",
        "outputId": "b3093dc0-58cc-49e6-8314-281ad8d6832f"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoOriginal = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", embeddingDocumentoOriginal.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([26, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhiX25CW2OnC"
      },
      "source": [
        "Confirmando vetores dependentes do contexto\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7C8abOy2OnD",
        "outputId": "cb174361-ce45-45d3-d928-a46b768eb08e"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZCfusGL2OnD"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaOvIge52OnD",
        "outputId": "26f5dcef-2916-4cfa-9dc9-419b283ac1d8"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "embeddingSentenca1Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca1Original, tokenizer)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca2Original, tokenizer)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca3Original, tokenizer)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca4Original, tokenizer)\n",
        "\n",
        "print('Os primeiros 4 valores de cada sentença do documento original.')\n",
        "\n",
        "print('\\nSentença 1:', sentenca1Original,'-', str(embeddingSentenca1Original[:4]))\n",
        "print('Soma embedding Sentença1:', sentenca1Original,'-', str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print('\\nSentença 2:', sentenca2Original,'-', str(embeddingSentenca2Original[:4]))\n",
        "print('Soma embedding Sentença2:', sentenca2Original,'-', str(torch.sum(embeddingSentenca2Original[:4])))\n",
        "\n",
        "print('\\nSentença 3:', sentenca3Original,'-', str(embeddingSentenca3Original[:4]))\n",
        "print('Soma embedding Sentença3:', sentenca3Original,'-', str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print('\\nSentença 4:', sentenca4Original,'-', str(embeddingSentenca4Original[:4]))\n",
        "print('Soma embedding Sentença4:', sentenca4Original,'-', str(torch.sum(embeddingSentenca4Original[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: Bom Dia, professor. - tensor([[-0.0680, -0.4615,  0.3552,  ..., -0.3943, -0.1818, -0.4821],\n",
            "        [-0.1000, -0.0630,  0.0840,  ..., -0.6630,  0.1641, -0.8297],\n",
            "        [-0.3165,  0.4208,  0.2178,  ..., -0.4981,  0.1935, -0.3366],\n",
            "        [ 0.1248,  0.2383,  0.8987,  ..., -0.4940, -0.4578, -0.0353]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-13.8934)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.5894, -0.4310,  0.1449,  ...,  0.1601, -0.2918, -0.5303],\n",
            "        [ 0.1349, -0.2476,  0.4605,  ..., -0.3036, -0.6972,  0.2135],\n",
            "        [ 0.4359, -0.6972,  0.4066,  ...,  0.0177, -0.5852, -0.0615],\n",
            "        [ 0.0544,  0.1606,  0.4150,  ..., -0.3822, -0.1052, -0.0296]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-13.9466)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[-0.3652, -0.5015, -0.1626,  ...,  0.0161, -0.5967,  0.1675],\n",
            "        [ 0.1421,  0.1797, -0.0014,  ..., -0.5730, -0.5169,  0.3205],\n",
            "        [-0.1274, -0.0926, -0.1861,  ...,  0.4762, -0.4671,  0.2165],\n",
            "        [-0.2886,  0.4056,  0.6759,  ...,  0.5541, -0.3019, -0.1783]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-7.3502)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[-0.0835, -0.4042,  0.4330,  ..., -0.3271,  0.2262, -0.4599],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.4231, -0.3089, -0.5329],\n",
            "        [-0.5474, -0.0118,  0.3950,  ..., -0.0949, -0.3534, -0.3717],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ...,  0.1161,  0.0407, -0.6904]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-9.1314)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbGSdUzw2OnD"
      },
      "source": [
        "Examinando os embeddings do documento original\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okFUjfDG2OnD",
        "outputId": "fb09a305-2570-4f29-f5da-8a3c2fa356d1"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaOriginal = tokenizer.tokenize(sentenca1Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1TokenizadaOriginal)\n",
        "embeddingSentenca1Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca1Original, tokenizer)\n",
        "print('\\nSentença 1 Original=\\'', sentenca1Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca1Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaOriginal = tokenizer.tokenize(sentenca2Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2TokenizadaOriginal)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca2Original, tokenizer)\n",
        "print('\\nSentença 2 Original=\\'', sentenca2Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca2TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca2Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca2Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaOriginal = tokenizer.tokenize(sentenca3Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3TokenizadaOriginal)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca3Original, tokenizer)\n",
        "print('\\nSentença 3 Original=\\'', sentenca3Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca3TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca3Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca3Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaOriginal = tokenizer.tokenize(sentenca4Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4TokenizadaOriginal)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca4Original, tokenizer)\n",
        "print('\\nSentença 4 Original=\\'', sentenca4Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca4Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Original))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=' Bom Dia, professor. '\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 768])\n",
            "    Soma embeddings:  -15.02\n",
            "\n",
            "Sentença 2 Original=' Qual o conteúdo da prova? '\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -17.08\n",
            "\n",
            "Sentença 3 Original=' Vai cair tudo na prova? '\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -10.65\n",
            "\n",
            "Sentença 4 Original=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -12.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVL1lBnB2OnD"
      },
      "source": [
        "### Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uMC8h3F2OnE",
        "outputId": "5903a91a-c4c6-4095-95ab-0ccc00269812"
      },
      "source": [
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_permutado = [documento_original[3],   # \"Aguardo uma resposta, João.\",\n",
        "             documento_original[1],             # \"Qual o conteúdo da prova?\",              \n",
        "             documento_original[0],             # \"Vai cair tudo na prova?\",\n",
        "             documento_original[2]]             # \"Bom Dia, professor.\"]     \n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "stringDocumentoPermutado = ' '.join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + stringDocumentoPermutado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print('{:>3} {:<12} {:>6,}'.format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 Bom           8,399\n",
            " 15 Dia           3,616\n",
            " 16 ,               117\n",
            " 17 professor     2,917\n",
            " 18 .               119\n",
            " 19 Vai          20,805\n",
            " 20 cair          9,322\n",
            " 21 tudo          2,745\n",
            " 22 na              229\n",
            " 23 prova         2,310\n",
            " 24 ?               136\n",
            " 25 [SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPjSfaUR2OnE",
        "outputId": "d39c371f-4368-4662-c6e7-a72fa8436e94"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXp0WqZa2OnE"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuwXz5Ye2OnE"
      },
      "source": [
        "# Importa a bibliteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUmYOo7s2OnE"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gLUe6tT2OnE"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS0h6i4t2OnE"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1kcPJU82OnF",
        "outputId": "e12d9006-acb0-418f-d10b-b4d3adabfcd7"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Xu6swe2OnF"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOljOpq52OnF",
        "outputId": "8d7f49c9-0421-4567-af39-39346d19318e"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoPermutado = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", embeddingDocumentoPermutado.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([26, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BlaBKoE2OnF"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWAJMgs72OnF",
        "outputId": "954e4be2-28d1-451d-8f40-bd1bba5a2c2d"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca1Permutado, tokenizer)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca2Permutado, tokenizer)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca3Permutado, tokenizer)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca4Permutado, tokenizer)\n",
        "\n",
        "print('Os primeiros 4 valores de cada sentença do documento permutado.')\n",
        "\n",
        "print('\\nSentença 1:', sentenca1Permutado,'-', str(embeddingSentenca1Permutado[:4]))\n",
        "print('Soma embedding Sentença1:', sentenca1Original,'-', str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print('\\nSentença 2:', sentenca2Permutado,'-', str(embeddingSentenca2Permutado[:4]))\n",
        "print('Soma embedding Sentença2:', sentenca2Permutado,'-', str(torch.sum(embeddingSentenca2Permutado[:4])))\n",
        "\n",
        "print('\\nSentença 3:', sentenca3Permutado,'-', str(embeddingSentenca3Permutado[:4]))\n",
        "print('Soma embedding Sentença3:', sentenca3Permutado,'-', str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print('\\nSentença 4:', sentenca4Permutado,'-', str(embeddingSentenca4Permutado[:4]))\n",
        "print('Soma embedding Sentença4:', sentenca4Permutado,'-', str(torch.sum(embeddingSentenca4Permutado[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.4811,  0.2881, -0.4057],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.5534, -0.3164, -0.3582],\n",
            "        [-0.7358, -0.0988,  0.5145,  ..., -0.1814, -0.3193, -0.3159],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.0720, -0.0478, -0.4931]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-13.8934)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.4295, -0.3426,  0.1005,  ...,  0.1568, -0.3225, -0.3857],\n",
            "        [ 0.1047, -0.1946,  0.4177,  ..., -0.3388, -0.7529,  0.1693],\n",
            "        [ 0.4377, -0.5952,  0.5448,  ...,  0.0463, -0.5607, -0.1783],\n",
            "        [ 0.0898,  0.1063,  0.4610,  ..., -0.3313, -0.1764,  0.0920]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-13.8054)\n",
            "\n",
            "Sentença 3: Bom Dia, professor. - tensor([[-0.0458, -0.4268,  0.2399,  ..., -0.4504, -0.2225, -0.6343],\n",
            "        [ 0.0650, -0.1385,  0.0230,  ..., -0.7129,  0.1112, -0.8076],\n",
            "        [-0.1147,  0.4120,  0.0659,  ..., -0.3666,  0.0509, -0.2134],\n",
            "        [ 0.1290,  0.2181,  0.7986,  ..., -0.4771, -0.4328,  0.0041]])\n",
            "Soma embedding Sentença3: Bom Dia, professor. - tensor(-7.3502)\n",
            "\n",
            "Sentença 4: Vai cair tudo na prova? - tensor([[-0.0414, -0.5222, -0.1612,  ..., -0.0763, -0.6968,  0.0127],\n",
            "        [ 0.4540,  0.0075, -0.0953,  ..., -0.4919, -0.4762,  0.2358],\n",
            "        [ 0.0887, -0.1633, -0.3073,  ...,  0.4414, -0.5028,  0.0877],\n",
            "        [-0.0159,  0.2363,  0.7514,  ...,  0.3057, -0.4097, -0.2738]])\n",
            "Soma embedding Sentença4: Vai cair tudo na prova? - tensor(-4.3409)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jktC1hfL2OnF",
        "outputId": "bfe2ed44-6413-436f-adb6-f605b3b6054d"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaPermutado = tokenizer.tokenize(sentenca1Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1TokenizadaPermutado)\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca1Permutado, tokenizer)\n",
        "print('\\nSentença 1 Permutada=\\'', sentenca1Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca1Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaPermutado = tokenizer.tokenize(sentenca2Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2TokenizadaPermutado)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca2Permutado, tokenizer)\n",
        "print('\\nSentença 2 Permutada=\\'', sentenca2Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca2TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca2Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca2Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaPermutado = tokenizer.tokenize(sentenca3Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3TokenizadaPermutado)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca3Permutado, tokenizer)\n",
        "print('\\nSentença 3 Permutada=\\'', sentenca3Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca3TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca3Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca3Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaPermutado = tokenizer.tokenize(sentenca4Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4TokenizadaPermutado)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca4Permutado, tokenizer)\n",
        "print('\\nSentença 4 Permutada=\\'', sentenca4Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca4Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Permutado))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "\n",
            "Sentença 1 Permutada=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -14.89\n",
            "\n",
            "Sentença 2 Permutada=' Qual o conteúdo da prova? '\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -17.25\n",
            "\n",
            "Sentença 3 Permutada=' Bom Dia, professor. '\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 14 e término em 18\n",
            "    Formato modelo : torch.Size([5, 768])\n",
            "    Soma embeddings:  -15.11\n",
            "\n",
            "Sentença 4 Permutada=' Vai cair tudo na prova? '\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 19 e término em 24\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -6.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45HsR7ey2OnF"
      },
      "source": [
        "### Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9pbUlC72OnG",
        "outputId": "59965703-d9de-4e14-d60c-5a49602fd40b"
      },
      "source": [
        "print('\\nSentença 4 Original=\\'', sentenca4Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaOriginal)\n",
        "print('    Formato modelo :', embeddingSentenca4Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Original))\n",
        "print('    Os 4 primeiros embeddings:', str(embeddingSentenca4Original[:4]))\n",
        "\n",
        "print('\\nSentença 1 Permutada=\\'', sentenca1Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaPermutado)\n",
        "print('    Formato modelo :', embeddingSentenca1Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Permutado))\n",
        "print('    Os 4 primeiros embeddings:', str(embeddingSentenca1Permutado[:4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sentença 4 Original=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -12.13\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0835, -0.4042,  0.4330,  ..., -0.3271,  0.2262, -0.4599],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.4231, -0.3089, -0.5329],\n",
            "        [-0.5474, -0.0118,  0.3950,  ..., -0.0949, -0.3534, -0.3717],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ...,  0.1161,  0.0407, -0.6904]])\n",
            "\n",
            "Sentença 1 Permutada=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -14.89\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.4811,  0.2881, -0.4057],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.5534, -0.3164, -0.3582],\n",
            "        [-0.7358, -0.0988,  0.5145,  ..., -0.1814, -0.3193, -0.3159],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.0720, -0.0478, -0.4931]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p58PZ7qi2OnG"
      },
      "source": [
        "### Subtração entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX0B6Lbr2OnG"
      },
      "source": [
        "#### Calcula a média aritmética da subtração entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAG-TVlU2OnG",
        "outputId": "a2ae0aad-db76-4e18-dd92-a0d039bca150"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSsub = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "       \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "     # Entrada: (<768 ou 1024>) x (<768 ou 1024>) \n",
        "    Ssub = torch.sub(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: <768 ou 1024>  \n",
        "    #print(\"Ssub=\", Ssub.shape)\n",
        "    \n",
        "    # Calcula a média dos embeddings do subtração para realizar a soma\n",
        "    somaSsub = somaSsub + torch.mean(Ssub)\n",
        "\n",
        "DsubOriginal = float(somaSsub)/float(len(documento_original)-1)\n",
        "print(\"Ssub Original:\", DsubOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ssub Original: -0.027655382951100666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5-Ev68E2OnG"
      },
      "source": [
        "#### #### Calcula a média aritmética da subtração entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0GO503C2OnG",
        "outputId": "7a98d0da-8681-434c-f32b-8152481b895d"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSsub = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "       \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "     # Entrada: (<768 ou 1024>) x (<768 ou 1024>)  \n",
        "    Ssub = torch.sub(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: <768 ou 1024>  \n",
        "    #print(\"Ssub=\", Ssub.shape)\n",
        "        \n",
        "    # Calcula a média dos embeddings do subtração para realizar a soma\n",
        "    somaSsub = somaSsub + torch.mean(Ssub)\n",
        "\n",
        "DsubPermutado = float(somaSsub)/float(np-1)\n",
        "print(\"Ssub permutado:\", DsubPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ssub permutado: 0.023601315915584564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCCOBw0n2OnG"
      },
      "source": [
        "#### Compara as médias da subtração dos embeddings das sentenças do documento original e permutado.\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw1een6R2OnG",
        "outputId": "3c7cd746-d164-448e-a53a-32c82797a462"
      },
      "source": [
        "print(\"Dsub Original :\", DsubOriginal)\n",
        "print(\"Dsub Permutado:\", DsubPermutado)\n",
        "\n",
        "if (DsubOriginal <= DsubPermutado):\n",
        "    print(\"Documento original tem menor diferença entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor diferença entre as sentenças!\")\n",
        "\n",
        "if (DsubOriginal > DsubPermutado):\n",
        "    print(\"Documento original tem menor diferença entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor diferença entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dsub Original : -0.027655382951100666\n",
            "Dsub Permutado: 0.023601315915584564\n",
            "Documento original tem menor diferença entre as sentenças!\n",
            "Documento Permutado tem menor diferença entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsOQHAqm2OnH"
      },
      "source": [
        "### Subtração absoluta entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ipoYuNW2OnH"
      },
      "source": [
        "#### Calcula a média aritmética da subtração absoluta entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPXJoQQ32OnH",
        "outputId": "830abe7b-7333-48e6-f9e1-b4a07226fcab"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSsubabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "       \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Ssubabs = abs(torch.sub(maiorEmbeddingSi, maiorEmbeddingSj))\n",
        "    # Saída: <768 ou 1024>  \n",
        "    #print(\"Ssubabs=\", Ssubabs.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings do subtração para realizar a soma\n",
        "    somaSsubabs = somaSsubabs + torch.mean(Ssubabs)\n",
        "\n",
        "DsubabsOriginal = float(somaSsubabs)/float(n-1)\n",
        "print(\"Dsubabs Original:\", DsubabsOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dsubabs Original: 0.2602133552233378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK7z5u_v2OnH"
      },
      "source": [
        "#### Calcula a média aritmética da subtração absoluta entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmiTHZQ62OnH",
        "outputId": "fba867a2-7d2d-4e33-90d2-e2d017c6e5a0"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSsubabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "   # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "       \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "     # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Ssubabs = abs(torch.sub(maiorEmbeddingSi, maiorEmbeddingSj))\n",
        "    # Saída: <768 ou 1024>  \n",
        "    #print(\"Ssub=\", Ssub.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings do subtração para realizar a soma\n",
        "    somaSsubabs = somaSsubabs + torch.mean(Ssubabs)\n",
        "\n",
        "DsubabsPermutado = float(somaSsubabs)/float(np-1)\n",
        "print(\"Dsubabs permutado:\", DsubabsPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dsubabs permutado: 0.2577057679494222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC3Aee8b2OnH"
      },
      "source": [
        "#### Compara as médias da subtração absoluta dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3Kepl8n2OnH",
        "outputId": "bfd8f0a1-b974-4bf1-987c-fd1beba90862"
      },
      "source": [
        "print(\"Dsubabs Original :\", DsubabsOriginal)\n",
        "print(\"Dsubabs Permutado:\", DsubabsPermutado)\n",
        "\n",
        "if (DsubabsOriginal <= DsubabsPermutado):\n",
        "    print(\"Documento original tem menor diferença absoluta entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor diferença absoluta entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dsubabs Original : 0.2602133552233378\n",
            "Dsubabs Permutado: 0.2577057679494222\n",
            "Documento Permutado tem menor diferença absoluta entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFHjx_TV2OnI"
      },
      "source": [
        "### Produto entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enrIspZV2OnI"
      },
      "source": [
        "#### Calcula a média aritmética do produto das matrizes dos tokens das sentenças do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1Bstrmu2OnI",
        "outputId": "7918cc35-db2d-487d-a0e5-e23c0e966e93"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSprod = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Para a multiplicação pode ocorrer número de colunas(qtde_tokens) de Si tem que ser igual ao número de linhas(embeddings) de Sj.\n",
        "    # Permute realiza a troca da dimensão 1(qtde_embeddings) pela 0(qtde_tokesn) na sentença Sj para que seja possível realizar o produto. \n",
        "    # Ocorre a troca da qtde_tokens pela quantidade de embeddings.\n",
        "    # Entrada: (<qtde_tokensSi> x <768 ou 1024>) x (<qtde_tokensSj> x <768 ou 1024>)\n",
        "    # Permute: <qtde_tokensSi> x <768 ou 1024>) x (<768 ou 1024> x <qtde_tokensSj>)\n",
        "    Sprod = torch.matmul(embeddingSi, embeddingSj.permute(1,0))    \n",
        "    # Saída: <qtde_tokensSi> x  <qtde_tokensSj>\n",
        "    #print(\"Sprod=\", Sprod.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings do produto para realizar a soma\n",
        "    somaSprod = somaSprod + torch.mean(Sprod)\n",
        "\n",
        "DprodOriginal = float(somaSprod)/float(n-1)\n",
        "print(\"Dprod Original:\", DprodOriginal)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dprod Original: 43.7073974609375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDMBllJU2OnI"
      },
      "source": [
        "#### Calcula a média aritmética do produto das matrizes dos tokens das sentenças do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpKHXIsx2OnI",
        "outputId": "74a119e0-5596-46fe-b293-acc7f509bccb"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSprod = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Para a multiplicação pode ocorrer número de colunas(qtde_tokens) de Si tem que ser igual ao número de linhas(embeddings) de Sj.\n",
        "    # Permute realiza a troca da dimensão 1(qtde_embeddings) pela 0(qtde_tokesn) na sentença Sj para que seja possível realizar o produto. \n",
        "    # Ocorre a troca da qtde_tokens pela quantidade de embeddings.\n",
        "    # Entrada: (<qtde_tokensSi> x <768 ou 1024>) x (<qtde_tokensSj> x <768 ou 1024>)\n",
        "    # Permute: <qtde_tokensSi> x <768 ou 1024>) x (<768 ou 1024> x <qtde_tokensSj>)\n",
        "    Sprod = torch.matmul(embeddingSi, embeddingSj.permute(1,0))    \n",
        "    # Saída: <qtde_tokensSi> x  <qtde_tokensSj>\n",
        "    #print(\"Sprod=\", Sprod.shape)\n",
        "    \n",
        "    # Encontra os maiores embeddings do produto para realizar a soma\n",
        "    somaSprod = somaSprod + torch.mean(Sprod)\n",
        "\n",
        "DprodPermutado = float(somaSprod)/float(np-1)\n",
        "print(\"Dprod permutado:\", DprodPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dprod permutado: 43.55047607421875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDyFFrTN2OnI"
      },
      "source": [
        "#### Compara as médias do produto dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Dois documentos(vetores) que apontam em uma direção semelhante retornam um produto escalar positivo.\n",
        "- Dois documentos perpendiculares retornam um produto escalar de zero.\n",
        "- Dois documentos(vetores) que apontam em direções opostas retornam um produto escalar negativo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmvkTxDd2OnI",
        "outputId": "ae22922b-d51c-4f2e-b2ed-a7593091f48a"
      },
      "source": [
        "print(\"Dprod Original :\", DprodOriginal)\n",
        "print(\"Dprod Permutado:\", DprodPermutado)\n",
        "\n",
        "if (DprodOriginal > 0 and DprodPermutado > 0):\n",
        "    print(\"As sentenças do documento original e as sentenças do documento permutado estão relacionadas!\")\n",
        "else:\n",
        "  if (DprodOriginal < 0 and DprodPermutado > 0) or (DprodOriginal > 0 and DprodPermutado < 0):\n",
        "    print(\"As sentenças do documento original e as sentenças do documento permutado não estão relacionadas!\")\n",
        "  else:  \n",
        "    print(\"As sentenças do documento original e as sentenças do documento permutado não possuem relação!\")              "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dprod Original : 43.7073974609375\n",
            "Dprod Permutado: 43.55047607421875\n",
            "As sentenças do documento original e as sentenças do documento permutado estão relacionadas!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdRc0_F52OnJ"
      },
      "source": [
        "### Produto Absoluto entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-fm-h502OnJ"
      },
      "source": [
        "#### Calcula a média aritmética do produto absoluto das matrizes dos tokens das sentenças do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6ucB1Mt2OnJ",
        "outputId": "ab3f867d-ca18-4039-85f7-bfd26544631c"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSprodabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "    \n",
        "    # # Para a multiplicação pode ocorrer número de colunas(qtde_tokens) de Si tem que ser igual ao número de linhas(embeddings) de Sj.\n",
        "    # Permute realiza a troca da dimensão 1(qtde_embeddings) pela 0(qtde_tokesn) na sentença Sj para que seja possível realizar o produto. \n",
        "    # Ocorre a troca da qtde_tokens pela quantidade de embeddings.\n",
        "    # Entrada: (<qtde_tokensSi> x <768 ou 1024>) x (<qtde_tokensSj> x <768 ou 1024>)\n",
        "    # Permute: <qtde_tokensSi> x <768 ou 1024>) x (<768 ou 1024> x <qtde_tokensSj>)\n",
        "    Sprodabs = abs(torch.matmul(embeddingSi, embeddingSj.permute(1,0)))   \n",
        "    # Saída: <qtde_tokensSi> x  <qtde_tokensSj>\n",
        "    #print(\"Sprodabs=\", Sprodabs.shape)\n",
        "   \n",
        "    # Encontra os maiores embeddings absolutos do produto para realizar a soma\n",
        "    somaSprodabs = somaSprodabs + torch.mean(Sprodabs)\n",
        "\n",
        "DprodabsOriginal = float(somaSprodabs)/float(n-1)\n",
        "print(\"Dprodabs Original:\", DprodabsOriginal)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dprodabs Original: 43.7073974609375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzkAzEtA2OnJ"
      },
      "source": [
        "#### Calcula a média aritmética do produto absoluto das matrizes dos tokens das sentenças do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CUfIhp82OnJ",
        "outputId": "d03a50ef-c96d-4794-bcd3-76f46d2e1e12"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSprodabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        " \n",
        "     # Para a multiplicação pode ocorrer número de colunas(qtde_tokens) de Si tem que ser igual ao número de linhas(embeddings) de Sj.\n",
        "    # Permute realiza a troca da dimensão 1(qtde_embeddings) pela 0(qtde_tokesn) na sentença Sj para que seja possível realizar o produto. \n",
        "    # Ocorre a troca da qtde_tokens pela quantidade de embeddings.\n",
        "    # Entrada: (<qtde_tokensSi> x <768 ou 1024>) x (<qtde_tokensSj> x <768 ou 1024>)\n",
        "    # Permute: <qtde_tokensSi> x <768 ou 1024>) x (<768 ou 1024> x <qtde_tokensSj>)\n",
        "    Sprodabs = abs(torch.matmul(embeddingSi, embeddingSj.permute(1,0)))    \n",
        "    # Saída: <qtde_tokensSi> x  <qtde_tokensSj>\n",
        "    #print(\"Sprodabs=\", Sprodabs.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings absolutos do produto para realizar a soma\n",
        "    somaSprodabs = somaSprodabs + torch.mean(Sprodabs)\n",
        "\n",
        "DprodabsPermutado = float(somaSprodabs)/float(np-1)\n",
        "print(\"Dprodabs permutado:\", DprodabsPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dprodabs permutado: 43.55047607421875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ9DjG_J2OnJ"
      },
      "source": [
        "#### Compara as médias do produto absoluto dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Dois documentos(vetores) que apontam em uma direção semelhante retornam um produto escalar positivo.\n",
        "- Dois documentos perpendiculares retornam um produto escalar de zero.\n",
        "- Dois documentos(vetores) que apontam em direções opostas retornam um produto escalar negativo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0BLjh902OnJ",
        "outputId": "8e86a463-629d-4a4b-91a3-2181d204d0f9"
      },
      "source": [
        "print(\"Dprodabs Original :\", DprodabsOriginal)\n",
        "print(\"Dprodabs Permutado:\", DprodabsPermutado)\n",
        "\n",
        "if (DprodabsOriginal > 0 and DprodabsPermutado > 0):\n",
        "    print(\"As sentenças do documento original e as sentenças do documento permutado estão relacionadas!\")\n",
        "else:\n",
        "  if (DprodabsOriginal < 0 and DprodabsPermutado > 0) or (DprodabsOriginal > 0 and DprodabsPermutado < 0):\n",
        "    print(\"As sentenças do documento original e as sentenças do documento permutado não estão relacionadas!\")\n",
        "  else:  \n",
        "    print(\"As sentenças do documento original e as sentenças do documento permutado não possuem relação!\")          "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dprodabs Original : 43.7073974609375\n",
            "Dprodabs Permutado: 43.55047607421875\n",
            "As sentenças do documento original e as sentenças do documento permutado estão relacionadas!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dof_qqDDJuUj"
      },
      "source": [
        "### Produto Escalar entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-83QP1LiJuUr"
      },
      "source": [
        "#### Calcula a média aritmética do produto escalar entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento orginal. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVTbYGp4JuUr",
        "outputId": "ab25b8a3-79d5-4af8-dce7-f07f366031d8"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSprode = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "   # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "   # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "\n",
        "    # Produto escalar entre os embeddings de Si e Sj, pois Si e Sj possui uma única dimensão\n",
        "    # https://pytorch.org/docs/master/generated/torch.matmul.html#torch.matmul\n",
        "     # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sprode = torch.matmul(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real \n",
        "    #print(\"Sprode=\", Sprode)\n",
        "        \n",
        "    somaSprode = somaSprode + Sprode\n",
        "\n",
        "DprodeOriginal = float(somaSprode)/float(n-1)\n",
        "print(\"Dprode Original:\", DprodeOriginal)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dprode Original: 200.03959147135416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXtjluE4JuUr"
      },
      "source": [
        "#### Calcula a média aritmética do produto escalar entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh0ye5JoJuUs",
        "outputId": "c0ae00f0-5266-4d52-828b-db5fe817b96f"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSprode = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Produto escalar entre os embeddings de Si e Sj, pois Si e Sj possui uma única dimensão\n",
        "    # https://pytorch.org/docs/master/generated/torch.matmul.html#torch.matmul\n",
        "     # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sprode = torch.matmul(maiorEmbeddingSi, maiorEmbeddingSj)    \n",
        "    # Saída: Um número real \n",
        "    #print(\"Sprode=\", Sprode)\n",
        "    \n",
        "    somaSprode = somaSprode + Sprode\n",
        "\n",
        "DprodePermutado = float(somaSprode)/float(np-1)\n",
        "print(\"Dprode permutado:\", DprodePermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dprode permutado: 199.48846435546875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnTW5Wu4JuUs"
      },
      "source": [
        "#### Compara as médias do produto escalar dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Dprod Original : 230.61181640625\n",
        "Dprod Permutado: 227.75482177734375\n",
        "Documento original tem maior similaridade com o produto entre as sentenças!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vb92srYJuUs",
        "outputId": "1079e2f2-a0c4-41d3-e394-059d78e33808"
      },
      "source": [
        "print(\"Dprode Original :\", DprodeOriginal)\n",
        "print(\"Dprode Permutado:\", DprodePermutado)\n",
        "\n",
        "if (DprodeOriginal < DprodePermutado):\n",
        "    print(\"Documento original tem maior similaridade com o produto entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade com o produto entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dprode Original : 200.03959147135416\n",
            "Dprode Permutado: 199.48846435546875\n",
            "Documento Permutado tem menor similaridade com o produto entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sro0syoSJuUs"
      },
      "source": [
        "### Produto Escalar Absoluto entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o-kEQHPJuUs"
      },
      "source": [
        "#### Calcula a média aritmética do produto escalar absoluto entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XCdJk_LJuUt",
        "outputId": "f3d45a10-10d1-4648-ef31-0a368e9aaf6f"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSprodeabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "     # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Produto escalar entre os embeddings de Si e Sj, pois Si e Sj possui uma única dimensão\n",
        "    # https://pytorch.org/docs/master/generated/torch.matmul.html#torch.matmul\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sprodeabs = abs(torch.matmul(maiorEmbeddingSi, maiorEmbeddingSj))\n",
        "    # Saída: Um número real \n",
        "    #print(\"Sprodeabs=\", Sprodeabs)\n",
        "\n",
        "    somaSprodeabs = somaSprodeabs + Sprodeabs\n",
        "\n",
        "DprodeabsOriginal = float(somaSprodeabs)/float(n-1)\n",
        "print(\"Dprodabs Original:\", DprodeabsOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dprodabs Original: 200.03959147135416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fuNsMSoJuUt"
      },
      "source": [
        "#### Calcula a média aritmética do produto escalar absoluto entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EdEl_B7JuUt",
        "outputId": "eed18892-82a9-4513-f085-075939ed0d95"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSprodeabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "    \n",
        "     # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "    \n",
        "    # Produto escalar entre os embeddings de Si e Sj, pois Si e Sj possui uma única dimensão\n",
        "    # https://pytorch.org/docs/master/generated/torch.matmul.html#torch.matmul\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sprodeabs = abs(torch.matmul(maiorEmbeddingSi, maiorEmbeddingSj))\n",
        "    # Saída: Um número real \n",
        "    #print(\"Sprodeabs=\", Sprodeabs)\n",
        "    \n",
        "    somaSprodeabs = somaSprodeabs + Sprodeabs\n",
        "\n",
        "DprodeabsPermutado = float(somaSprodeabs)/float(np-1)\n",
        "print(\"Dprodabs permutado:\", DprodeabsPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dprodabs permutado: 199.48846435546875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85rXQYmMJuUt"
      },
      "source": [
        "#### Compara as médias do produto escalar absoluto dos embeddings das sentenças do documento original e permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaoVAtqOJuUt",
        "outputId": "44b97f90-4b27-4622-c8a1-5bfc9d3811c2"
      },
      "source": [
        "print(\"Dprodabs Original :\", DprodeabsOriginal)\n",
        "print(\"Dprodabs Permutado:\", DprodeabsPermutado)\n",
        "\n",
        "if (DprodeabsOriginal < DprodeabsPermutado):\n",
        "    print(\"Documento original tem maior similaridade com o produto absoluto entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade com o produto absoluto entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dprodabs Original : 200.03959147135416\n",
            "Dprodabs Permutado: 199.48846435546875\n",
            "Documento Permutado tem menor similaridade com o produto absoluto entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv4lY4MnJuUu"
      },
      "source": [
        "### Média entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jis2S3BRJuUu"
      },
      "source": [
        "#### Calcula a média aritmética entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KRQ9XDrJuUu",
        "outputId": "c7d5884c-0f97-4b4d-cf2e-721ce234252f"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSavg = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Média entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Savg = (maiorEmbeddingSi.add(maiorEmbeddingSj))/2.0  \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"Savg=\", Savg.shape)  \n",
        "\n",
        "    # Calcula a média dos embeddings do cálculo para realizar a soma\n",
        "    somaSavg = somaSavg + torch.mean(Savg)\n",
        "\n",
        "DavgOriginal = float(somaSavg)/float(n-1)\n",
        "print(\"Davg Original:\", DavgOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Davg Original: 0.43032602469126385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSnRFBtaJuUu"
      },
      "source": [
        "#### Calcula a média aritmética entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVYwJIEGJuUu",
        "outputId": "98934ad1-3f64-42fd-f162-97c949e3625c"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSavg = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Média entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)  \n",
        "    Savg = (maiorEmbeddingSi.add(maiorEmbeddingSj))/2.0  \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"Savg=\", Savg.shape)  \n",
        "\n",
        "    # Calcula a média dos embeddings do cálculo para realizar a soma\n",
        "    somaSavg = somaSavg + torch.mean(Savg)\n",
        "\n",
        "DavgPermutado = float(somaSavg)/float(np-1)\n",
        "print(\"Davg permutado:\", DavgPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Davg permutado: 0.4282990296681722\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09OMROmTJuUu"
      },
      "source": [
        "#### Compara as médias dos embeddings das sentenças do documento original e permutado\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrFzann7JuUu",
        "outputId": "05da9a0e-cf49-432c-aad2-a40a1de33e83"
      },
      "source": [
        "print(\"Davg Original :\", DavgOriginal)\n",
        "print(\"Davg Permutado:\", DavgPermutado)\n",
        "\n",
        "if (DavgOriginal < DavgPermutado):\n",
        "    print(\"Documento original tem maior similaridade com a média entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade com a mpedia entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Davg Original : 0.43032602469126385\n",
            "Davg Permutado: 0.4282990296681722\n",
            "Documento Permutado tem menor similaridade com a mpedia entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd8YKgnyJuUv"
      },
      "source": [
        "### Similaridade de cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cREd7N1JuUv"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCoseno(sentenca1, sentenca2):\n",
        "  similaridade = 1 - cosine(sentenca1, sentenca2)\n",
        "  return similaridade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auu-XqHOJuUv"
      },
      "source": [
        "#### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bNpbR0QJuUv",
        "outputId": "abe52d2c-19c1-4e57-8292-3e4acf570528"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCoseno(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "DcosOriginal = float(somaScos)/float(n-1)\n",
        "print(\"Dcos Original:\", DcosOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dcos Original: 0.819700022538503\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWNj-6i5JuUv"
      },
      "source": [
        "#### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQEsVKJ1JuUv",
        "outputId": "aa650783-13b1-48bd-afea-baa916986aff"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCoseno(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "DcosPermutado = float(somaScos)/float(np-1)\n",
        "print(\"Dcos Original:\", DcosPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dcos Original: 0.8229710857073466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ecHZJrCJuUw"
      },
      "source": [
        "#### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 1.\n",
        "- Documentos com sentenças diferenntes resulta uma medida menor que 1.\n",
        "- Documento com sentenças muito diferentes apresentam valores menores que 1.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de similaridade.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTxA53uUJuUw",
        "outputId": "7be49a1f-a1a5-4c94-a74c-a433bdc6e999"
      },
      "source": [
        "print(\"Dcos Original :\", DcosOriginal)\n",
        "print(\"Dcos Permutado:\", DcosPermutado)\n",
        "\n",
        "if (DcosOriginal > DcosPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dcos Original : 0.819700022538503\n",
            "Dcos Permutado: 0.8229710857073466\n",
            "Documento Permutado tem menor similaridade de cosseno entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPQwuHf5JuUw"
      },
      "source": [
        "### Distância euclidiana entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância L2 ou norma L2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RmL_qXCJuUw"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distanciaEuclidiana(sentenca1, sentenca2):\n",
        "  distancia = euclidean(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a61ckdmoJuUw"
      },
      "source": [
        "#### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yu-uTlhJuUw",
        "outputId": "2f9a4930-1b16-4d08-b4b2-0546dca5f338"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "DeucOriginal = float(somaSeuc)/float(n-1)\n",
        "print(\"Deuc Original:\", DeucOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Deuc Original: 9.424604415893555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBRvVkI4JuUw"
      },
      "source": [
        "#### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNgY1epXJuUw",
        "outputId": "16484567-36c6-4353-fa0d-7f21d6939ad7"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "DeucPermutado = float(somaSeuc)/float(np-1)\n",
        "print(\"Deuc Original:\", DeucPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Deuc Original: 9.29481569925944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI4qwrbWJuUx"
      },
      "source": [
        "#### Compara as médias da distância euclidiana dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeJFDd5mJuUx",
        "outputId": "f6a348d9-9f36-48ff-9648-9b740b59c3c7"
      },
      "source": [
        "print(\"Deuc Original :\", DeucOriginal)\n",
        "print(\"Deuc Permutado:\", DeucPermutado)\n",
        "\n",
        "if (DeucOriginal > DeucPermutado):\n",
        "    print(\"Documento original tem maior distância euclidiana entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor distância euclidiana entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deuc Original : 9.424604415893555\n",
            "Deuc Permutado: 9.29481569925944\n",
            "Documento original tem maior distância euclidiana entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EUoPQNyJuUx"
      },
      "source": [
        "### Distância Manhattan entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IydTlzptJuUx"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def distanciaEManhattan(sentenca1, sentenca2):\n",
        "  distancia = cityblock(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nuu93cfvJuUx"
      },
      "source": [
        "#### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1DavITCJuUx",
        "outputId": "de5b47e5-7d37-443b-c8aa-9e8634e33483"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj)) \n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaEManhattan(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "DmanOriginal = float(somaSman)/float(n-1)\n",
        "print(\"Dman Original:\", DmanOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dman Original: 199.8438466389974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcWmFXmQJuUx"
      },
      "source": [
        "#### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6mpG5x0JuUy",
        "outputId": "695fc265-864c-46ca-a71a-5430687f4f90"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj)) \n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaEManhattan(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "DmanPermutado = float(somaSman)/float(np-1)\n",
        "print(\"Deuc Original:\", DmanPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Deuc Original: 197.91803995768228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa8RFKDVJuUy"
      },
      "source": [
        "#### Compara as médias da distância de manhattan dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB_oWMJ0JuUy",
        "outputId": "d1955d3f-09ba-44ae-f676-35fab5edef3d"
      },
      "source": [
        "print(\"Dman Original :\", DmanOriginal)\n",
        "print(\"Dman Permutado:\", DmanPermutado)\n",
        "\n",
        "if (DeucOriginal > DeucPermutado):\n",
        "    print(\"Documento original tem maior distância de manhattan entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor distância de manhattan entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dman Original : 199.8438466389974\n",
            "Dman Permutado: 197.91803995768228\n",
            "Documento original tem maior distância de manhattan entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRdr2bqZ2OnQ"
      },
      "source": [
        "### Resumo\n",
        "\n",
        "Resultado das medidas utilizando a última camada do BERT.\n",
        "\n",
        "Base(MEAN):\n",
        "- Dsub       :   0.00074918          -0.00064076\n",
        "- Dubabs     :   0.17778097          0.17206367\n",
        "- Dprod      :   43.70739746          43.55047607\n",
        "- Dprodabs   :   43.70739746          43.55047607\n",
        "- Dprode     :   43.70739746          43.55047607\n",
        "- Dprodeabs  :   43.70739746          43.55047607\n",
        "- Davg       :   -0.00276791          -0.00271371\n",
        "- Dcos       :   0.67999101          0.69653825\n",
        "- Deuc       :   6.34085274          6.15486606\n",
        "- Dman       :   136.53579712          132.14489237\n",
        "\n",
        "Base(MAX):\n",
        "- Dsub       :   -0.02765538          0.02360132\n",
        "- Dubabs     :   0.26021336          0.25770577\n",
        "- Dprod      :   43.70739746          43.55047607\n",
        "- Dprodabs   :   43.70739746          43.55047607\n",
        "- Dprode     :   200.03959147          199.48846436\n",
        "- Dprodeabs  :   200.03959147          199.48846436\n",
        "- Davg       :   0.43032602          0.42829903\n",
        "- Dcos       :   0.81970002          0.82297109\n",
        "- Deuc       :   9.42460442          9.29481570\n",
        "- Dman       :   199.84384664          197.91803996"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW0cKGq52OnQ",
        "outputId": "a21b104c-f3e3-4504-da6a-423be1059d82"
      },
      "source": [
        "print(\"Resultado das medidas utilizando a última camada do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print('Dsub       :   {:.8f}          {:.8f}'.format(DsubOriginal,DsubPermutado))\n",
        "print('Dubabs     :   {:.8f}          {:.8f}'.format(DsubabsOriginal,DsubabsPermutado))\n",
        "print('Dprod      :   {:.8f}          {:.8f}'.format(DprodOriginal,DprodPermutado))\n",
        "print('Dprodabs   :   {:.8f}          {:.8f}'.format(DprodabsOriginal,DprodabsPermutado))\n",
        "print('Dprode     :   {:.8f}          {:.8f}'.format(DprodeOriginal,DprodePermutado))\n",
        "print('Dprodeabs  :   {:.8f}          {:.8f}'.format(DprodeabsOriginal,DprodeabsPermutado))\n",
        "print('Davg       :   {:.8f}          {:.8f}'.format(DavgOriginal,DavgPermutado))\n",
        "print('Dcos       :   {:.8f}          {:.8f}'.format(DcosOriginal,DcosPermutado))\n",
        "print('Deuc       :   {:.8f}          {:.8f}'.format(DeucOriginal,DeucPermutado))\n",
        "print('Dman       :   {:.8f}          {:.8f}'.format(DmanOriginal,DmanPermutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resultado das medidas utilizando a última camada do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Dsub       :   -0.02765538          0.02360132\n",
            "Dubabs     :   0.26021336          0.25770577\n",
            "Dprod      :   43.70739746          43.55047607\n",
            "Dprodabs   :   43.70739746          43.55047607\n",
            "Dprode     :   200.03959147          199.48846436\n",
            "Dprodeabs  :   200.03959147          199.48846436\n",
            "Davg       :   0.43032602          0.42829903\n",
            "Dcos       :   0.81970002          0.82297109\n",
            "Deuc       :   9.42460442          9.29481570\n",
            "Dman       :   199.84384664          197.91803996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru02mC0Estsb"
      },
      "source": [
        "## 8 - Exemplo sentenças de documento original e permutado utilizando embedding a concatenação das 4 últimas camadas do BERT usando estratégia a MEAN\n",
        "\n",
        "Como estamos utilizando os embeddings concatenado das 4 últimas camadas onde ocorre 768 entenda-se 3072 que é o resultado de 768 por 4 que é a dimensão do MCL BERT de tamanho base. E onde ocorre 1024 entenda-se 4096 que é o resultado de 1024 por 4 que é a dimensão do MCL BERT de tamanho large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1vDmYJTstsg"
      },
      "source": [
        "### Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJOUyEpistsg",
        "outputId": "659cc666-285c-470f-8292-d626140c8e84"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_original = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "stringDocumentoPermutado = ' '.join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + stringDocumentoPermutado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print('{:>3} {:<12} {:>6,}'.format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4HrZqBfstsh"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0tDxh3Mstsh",
        "outputId": "76617481-21af-491e-cb62-e2f28c2d5d59"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAQf9nM8stsh"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFqFcnx2stsh"
      },
      "source": [
        "# Importa a bibliteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJbHPnoAstsi"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda todas as camadas da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51R6f4Mistsi"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx2_AvR8tbnZ"
      },
      "source": [
        "Recupera a saída e concatena as 4 últimas camada do BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C0KcRUHstsj",
        "outputId": "748434b0-df1e-4f30-86fb-628f70714915"
      },
      "source": [
        "# Cria uma lista com os tensores a serem concatenados\n",
        "# Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "# Lista com os tensores a serem concatenados\n",
        "listaConcat = []\n",
        "# Percorre os 4 últimos\n",
        "for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "# Realiza a concatenação dos embeddings de todos as camadas\n",
        "# Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "# Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "\n",
        "print (\"O vetor da  concatenação das 4 últimas camadas oculta tem o formato:\", concat4_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor da  concatenação das 4 últimas camadas oculta tem o formato: torch.Size([1, 26, 3072])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM4luftrstsk"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f18k_o-stsk",
        "outputId": "98f3346c-a207-4ff2-b064-f1c36a03fb67"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoOriginal = torch.squeeze(concat4_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", embeddingDocumentoOriginal.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([26, 3072])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g0E9s2Rstsk"
      },
      "source": [
        "Confirmando vetores dependentes do contexto\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQGOQF-kstsk",
        "outputId": "812645ad-339c-4b4e-945b-f7deae1f7101"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veMZsnAsstsl"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9RSPe2Hstsl",
        "outputId": "9da4b6ce-c89d-48f0-f398-38c4e5db9f6c"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "embeddingSentenca1Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca1Original, tokenizer)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca2Original, tokenizer)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca3Original, tokenizer)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca4Original, tokenizer)\n",
        "\n",
        "print('Os primeiros 4 valores de cada sentença do documento original.')\n",
        "\n",
        "print('\\nSentença 1:', sentenca1Original,'-', str(embeddingSentenca1Original[:4]))\n",
        "print('Soma embedding Sentença1:', sentenca1Original,'-', str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print('\\nSentença 2:', sentenca2Original,'-', str(embeddingSentenca2Original[:4]))\n",
        "print('Soma embedding Sentença2:', sentenca2Original,'-', str(torch.sum(embeddingSentenca2Original[:4])))\n",
        "\n",
        "print('\\nSentença 3:', sentenca3Original,'-', str(embeddingSentenca3Original[:4]))\n",
        "print('Soma embedding Sentença3:', sentenca3Original,'-', str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print('\\nSentença 4:', sentenca4Original,'-', str(embeddingSentenca4Original[:4]))\n",
        "print('Soma embedding Sentença4:', sentenca4Original,'-', str(torch.sum(embeddingSentenca4Original[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: Bom Dia, professor. - tensor([[-0.0680, -0.4615,  0.3552,  ...,  0.1657,  0.3478,  0.4525],\n",
            "        [-0.1000, -0.0630,  0.0840,  ..., -0.3926,  0.7971,  0.3287],\n",
            "        [-0.3165,  0.4208,  0.2178,  ..., -0.3834,  0.9520, -0.2181],\n",
            "        [ 0.1248,  0.2383,  0.8987,  ..., -0.1989,  0.1001,  0.1140]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-327.0100)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.5894, -0.4310,  0.1449,  ...,  0.2399, -0.8495, -0.1672],\n",
            "        [ 0.1349, -0.2476,  0.4605,  ..., -0.1353, -0.1674,  0.4057],\n",
            "        [ 0.4359, -0.6972,  0.4066,  ...,  0.5851, -0.0441, -0.0027],\n",
            "        [ 0.0544,  0.1606,  0.4150,  ..., -0.8060, -0.3698,  0.0466]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-315.7253)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[-0.3652, -0.5015, -0.1626,  ..., -0.5431, -0.5805,  0.4617],\n",
            "        [ 0.1421,  0.1797, -0.0014,  ..., -0.7004, -0.4356,  0.7459],\n",
            "        [-0.1274, -0.0926, -0.1861,  ...,  0.6395, -0.4126,  0.7672],\n",
            "        [-0.2886,  0.4056,  0.6759,  ..., -0.2171, -0.4010, -0.5805]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-311.2223)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[-0.0835, -0.4042,  0.4330,  ...,  0.2881,  0.1448,  0.7778],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.0102,  0.1851,  0.4587],\n",
            "        [-0.5474, -0.0118,  0.3950,  ...,  0.1932, -0.0735,  0.2649],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ..., -0.0370,  0.3236, -0.3671]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-307.8872)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGUkprWbstsl"
      },
      "source": [
        "Examinando os embeddings do documento original\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL-c0aqKstsl",
        "outputId": "80cefa90-7bb2-42f1-9d2a-78bd93d2f50b"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaOriginal = tokenizer.tokenize(sentenca1Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1TokenizadaOriginal)\n",
        "embeddingSentenca1Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca1Original, tokenizer)\n",
        "print('\\nSentença 1 Original=\\'', sentenca1Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca1Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaOriginal = tokenizer.tokenize(sentenca2Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2TokenizadaOriginal)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca2Original, tokenizer)\n",
        "print('\\nSentença 2 Original=\\'', sentenca2Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca2TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca2Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca2Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaOriginal = tokenizer.tokenize(sentenca3Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3TokenizadaOriginal)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca3Original, tokenizer)\n",
        "print('\\nSentença 3 Original=\\'', sentenca3Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca3TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca3Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca3Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaOriginal = tokenizer.tokenize(sentenca4Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4TokenizadaOriginal)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca4Original, tokenizer)\n",
        "print('\\nSentença 4 Original=\\'', sentenca4Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca4Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Original))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=' Bom Dia, professor. '\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 3072])\n",
            "    Soma embeddings:  -398.53\n",
            "\n",
            "Sentença 2 Original=' Qual o conteúdo da prova? '\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 3072])\n",
            "    Soma embeddings:  -467.44\n",
            "\n",
            "Sentença 3 Original=' Vai cair tudo na prova? '\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 3072])\n",
            "    Soma embeddings:  -466.27\n",
            "\n",
            "Sentença 4 Original=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 3072])\n",
            "    Soma embeddings:  -525.26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRdBVlt9stsm"
      },
      "source": [
        "### Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xsxt0Jistsm",
        "outputId": "95fa7d5b-d493-4867-9057-f4d6a37a14dc"
      },
      "source": [
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_permutado = [documento_original[3],   # \"Aguardo uma resposta, João.\",\n",
        "             documento_original[1],             # \"Qual o conteúdo da prova?\",              \n",
        "             documento_original[0],             # \"Vai cair tudo na prova?\",\n",
        "             documento_original[2]]             # \"Bom Dia, professor.\"]     \n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "stringDocumentoPermutado = ' '.join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + stringDocumentoPermutado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print('{:>3} {:<12} {:>6,}'.format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 Bom           8,399\n",
            " 15 Dia           3,616\n",
            " 16 ,               117\n",
            " 17 professor     2,917\n",
            " 18 .               119\n",
            " 19 Vai          20,805\n",
            " 20 cair          9,322\n",
            " 21 tudo          2,745\n",
            " 22 na              229\n",
            " 23 prova         2,310\n",
            " 24 ?               136\n",
            " 25 [SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM5NtnGRuAsU"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GDGmFc_stsm",
        "outputId": "7da44288-bfe6-482c-ba76-7352322874ed"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4WehJSxuDvo"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrvEtHp7stsm"
      },
      "source": [
        "# Importa a bibliteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQDiqh5UuMnc"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda todas as camadas da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tGQsVMpstsm"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5CmoS1at0YA"
      },
      "source": [
        "Recupera a saída e concatena as 4 últimas camada do BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRDYmUq9stsn"
      },
      "source": [
        "# Cria uma lista com os tensores a serem concatenados\n",
        "# Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "# Lista com os tensores a serem concatenados\n",
        "listaConcat = []\n",
        "# Percorre os 4 últimos\n",
        "for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "# Realiza a concatenação dos embeddings de todos as camadas\n",
        "# Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "# Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm9EVbDauSiI"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXNqk-oQstsn",
        "outputId": "c2e3f9e4-788e-4304-9855-90cb1e388dfa"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoPermutado = torch.squeeze(concat4_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", embeddingDocumentoPermutado.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([26, 3072])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbLgVumnstso"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1nUZ7OLstso",
        "outputId": "213e3782-3ba9-4cee-ebe4-801fa2606923"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca1Permutado, tokenizer)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca2Permutado, tokenizer)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca3Permutado, tokenizer)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca4Permutado, tokenizer)\n",
        "\n",
        "print('Os primeiros 4 valores de cada sentença do documento permutado.')\n",
        "\n",
        "print('\\nSentença 1:', sentenca1Permutado,'-', str(embeddingSentenca1Permutado[:4]))\n",
        "print('Soma embedding Sentença1:', sentenca1Original,'-', str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print('\\nSentença 2:', sentenca2Permutado,'-', str(embeddingSentenca2Permutado[:4]))\n",
        "print('Soma embedding Sentença2:', sentenca2Permutado,'-', str(torch.sum(embeddingSentenca2Permutado[:4])))\n",
        "\n",
        "print('\\nSentença 3:', sentenca3Permutado,'-', str(embeddingSentenca3Permutado[:4]))\n",
        "print('Soma embedding Sentença3:', sentenca3Permutado,'-', str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print('\\nSentença 4:', sentenca4Permutado,'-', str(embeddingSentenca4Permutado[:4]))\n",
        "print('Soma embedding Sentença4:', sentenca4Permutado,'-', str(torch.sum(embeddingSentenca4Permutado[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.1775,  0.2231,  0.8036],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.3783,  0.1529,  0.5125],\n",
            "        [-0.7358, -0.0988,  0.5145,  ...,  0.0164, -0.1749,  0.2557],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.2708,  0.1942, -0.0740]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-327.0100)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.4295, -0.3426,  0.1005,  ...,  0.2608, -0.8301, -0.1044],\n",
            "        [ 0.1047, -0.1946,  0.4177,  ..., -0.1734, -0.2517,  0.4004],\n",
            "        [ 0.4377, -0.5952,  0.5448,  ...,  0.4539, -0.1388, -0.0339],\n",
            "        [ 0.0898,  0.1063,  0.4610,  ..., -0.5308, -0.4884,  0.2119]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-314.9913)\n",
            "\n",
            "Sentença 3: Bom Dia, professor. - tensor([[-0.0458, -0.4268,  0.2399,  ...,  0.4282,  0.3110,  0.0241],\n",
            "        [ 0.0650, -0.1385,  0.0230,  ..., -0.3674,  0.5677,  0.0532],\n",
            "        [-0.1147,  0.4120,  0.0659,  ..., -0.0354,  0.8405, -0.2598],\n",
            "        [ 0.1290,  0.2181,  0.7986,  ..., -0.1129,  0.0972,  0.1581]])\n",
            "Soma embedding Sentença3: Bom Dia, professor. - tensor(-311.2223)\n",
            "\n",
            "Sentença 4: Vai cair tudo na prova? - tensor([[-0.0414, -0.5222, -0.1612,  ..., -0.5124, -0.7616,  0.1765],\n",
            "        [ 0.4540,  0.0075, -0.0953,  ..., -0.7207, -0.3354,  0.4786],\n",
            "        [ 0.0887, -0.1633, -0.3073,  ...,  0.6813, -0.4292,  0.4572],\n",
            "        [-0.0159,  0.2363,  0.7514,  ..., -0.2911, -0.2646, -0.9683]])\n",
            "Soma embedding Sentença4: Vai cair tudo na prova? - tensor(-299.0247)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E_hSMahstso",
        "outputId": "bd90b067-e999-49dc-e047-306b9cfe580a"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaPermutado = tokenizer.tokenize(sentenca1Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1TokenizadaPermutado)\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca1Permutado, tokenizer)\n",
        "print('\\nSentença 1 Permutada=\\'', sentenca1Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca1Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaPermutado = tokenizer.tokenize(sentenca2Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2TokenizadaPermutado)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca2Permutado, tokenizer)\n",
        "print('\\nSentença 2 Permutada=\\'', sentenca2Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca2TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca2Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca2Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaPermutado = tokenizer.tokenize(sentenca3Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3TokenizadaPermutado)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca3Permutado, tokenizer)\n",
        "print('\\nSentença 3 Permutada=\\'', sentenca3Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca3TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca3Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca3Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaPermutado = tokenizer.tokenize(sentenca4Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4TokenizadaPermutado)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca4Permutado, tokenizer)\n",
        "print('\\nSentença 4 Permutada=\\'', sentenca4Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca4Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Permutado))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "\n",
            "Sentença 1 Permutada=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 3072])\n",
            "    Soma embeddings:  -532.56\n",
            "\n",
            "Sentença 2 Permutada=' Qual o conteúdo da prova? '\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 3072])\n",
            "    Soma embeddings:  -467.65\n",
            "\n",
            "Sentença 3 Permutada=' Bom Dia, professor. '\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 14 e término em 18\n",
            "    Formato modelo : torch.Size([5, 3072])\n",
            "    Soma embeddings:  -402.97\n",
            "\n",
            "Sentença 4 Permutada=' Vai cair tudo na prova? '\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 19 e término em 24\n",
            "    Formato modelo : torch.Size([6, 3072])\n",
            "    Soma embeddings:  -447.06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CsH6v4Dstso"
      },
      "source": [
        "### Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p5iSl9Nstso",
        "outputId": "a473e605-f54f-402d-ed1d-aa7950a069d0"
      },
      "source": [
        "print('\\nSentença 4 Original=\\'', sentenca4Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaOriginal)\n",
        "print('    Formato modelo :', embeddingSentenca4Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Original))\n",
        "print('    Os 4 primeiros embeddings:', str(embeddingSentenca4Original[:4]))\n",
        "\n",
        "print('\\nSentença 1 Permutada=\\'', sentenca1Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaPermutado)\n",
        "print('    Formato modelo :', embeddingSentenca1Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Permutado))\n",
        "print('    Os 4 primeiros embeddings:', str(embeddingSentenca1Permutado[:4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sentença 4 Original=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 3072])\n",
            "    Soma embeddings:  -525.26\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0835, -0.4042,  0.4330,  ...,  0.2881,  0.1448,  0.7778],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.0102,  0.1851,  0.4587],\n",
            "        [-0.5474, -0.0118,  0.3950,  ...,  0.1932, -0.0735,  0.2649],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ..., -0.0370,  0.3236, -0.3671]])\n",
            "\n",
            "Sentença 1 Permutada=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 3072])\n",
            "    Soma embeddings:  -532.56\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.1775,  0.2231,  0.8036],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.3783,  0.1529,  0.5125],\n",
            "        [-0.7358, -0.0988,  0.5145,  ...,  0.0164, -0.1749,  0.2557],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.2708,  0.1942, -0.0740]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4D-oLW69UFI"
      },
      "source": [
        "### Subtração entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNcAx0Hv9UFK"
      },
      "source": [
        "#### Calcula a média aritmética da subtração entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2Tu-7Fb9UFL",
        "outputId": "3b1c1243-3d9e-4c49-c562-f47e4e85bcc2"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSsub = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Ssub = torch.sub(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: <768 ou 1024>  \n",
        "    #print(\"Ssub=\", Ssub.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings do subtração para realizar a soma\n",
        "    somaSsub = somaSsub + torch.mean(Ssub)\n",
        "\n",
        "DsubOriginal = float(somaSsub)/float(len(documento_original)-1)\n",
        "print(\"Ssub Original:\", DsubOriginal)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ssub Original: 0.0002815294816779594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVILakiD9UFL"
      },
      "source": [
        "#### Calcula a média aritmética da subtração entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSp_rmHO9UFL",
        "outputId": "d357779a-f3dc-4d7c-beab-36db13ec684c"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSsub = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Ssub = torch.sub(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: <768 ou 1024>  \n",
        "    #print(\"Ssub=\", Ssub.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings do subtração para realizar a soma\n",
        "    somaSsub = somaSsub + torch.mean(Ssub)\n",
        "\n",
        "DsubPermutado = float(somaSsub)/float(np-1)\n",
        "print(\"Ssub permutado:\", DsubPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ssub permutado: -0.0005001813018073639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voilJP8C9UFM"
      },
      "source": [
        "#### Compara as médias da subtração dos embeddings das sentenças do documento original e permutado.\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSVjYcht9UFM",
        "outputId": "df663591-8c21-4bca-9156-36ea00f642d5"
      },
      "source": [
        "print(\"Dsub Original :\", DsubOriginal)\n",
        "print(\"Dsub Permutado:\", DsubPermutado)\n",
        "\n",
        "if (DsubOriginal < DsubPermutado):\n",
        "    print(\"Documento original tem menor subtração entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor subtração entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dsub Original : 0.0002815294816779594\n",
            "Dsub Permutado: -0.0005001813018073639\n",
            "Documento Permutado tem menor subtração entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L6EzrXo9UFM"
      },
      "source": [
        "### Subtração absoluta entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXZNuRLY9UFM"
      },
      "source": [
        "#### Calcula a média aritmética da subtração absoluta entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2bt8q-Y9UFN",
        "outputId": "43079b87-ab8b-4128-f82c-62e528b5cf30"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSsubabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Ssubabs = abs(torch.sub(mediaEmbeddingSi, mediaEmbeddingSj))\n",
        "    # Saída: <768 ou 1024>  \n",
        "    #print(\"Ssubabs=\", Ssubabs.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings do subtração para realizar a soma\n",
        "    somaSsubabs = somaSsubabs + torch.mean(Ssubabs)\n",
        "\n",
        "DsubabsOriginal = float(somaSsubabs)/float(n-1)\n",
        "print(\"Dsubabs Original:\", DsubabsOriginal)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dsubabs Original: 0.26571635405222577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsSeRR5R9UFN"
      },
      "source": [
        "#### Calcula a média aritmética da subtração absoluta entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWhjg9yK9UFN",
        "outputId": "cd3df1da-0eb2-4042-c805-222c0483b660"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSsubabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Ssubabs = abs(torch.sub(mediaEmbeddingSi, mediaEmbeddingSj))\n",
        "    # Saída: <768 ou 1024>  \n",
        "    #print(\"Ssubabs=\", Ssubabs.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings do subtração para realizar a soma\n",
        "    somaSsubabs = somaSsubabs + torch.mean(Ssubabs)\n",
        "\n",
        "DsubabsPermutado = float(somaSsubabs)/float(np-1)\n",
        "print(\"Dsubabs permutado:\", DsubabsPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dsubabs permutado: 0.24592039982477823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q1sUXYk9UFN"
      },
      "source": [
        "#### Compara as médias da subtração absoluta dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci0hgUcr9UFN",
        "outputId": "8a0ffafe-7c22-437a-e6c6-22271c4ea75b"
      },
      "source": [
        "print(\"Dsubabs Original :\", DsubabsOriginal)\n",
        "print(\"Dsubabs Permutado:\", DsubabsPermutado)\n",
        "\n",
        "if (DsubabsOriginal < DsubabsPermutado):\n",
        "    print(\"Documento original tem menor subtração absoluta entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor subtração absoluta entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dsubabs Original : 0.26571635405222577\n",
            "Dsubabs Permutado: 0.24592039982477823\n",
            "Documento Permutado tem menor subtração absoluta entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDm-kfdX9UFO"
      },
      "source": [
        "### Produto entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGG64QC_9UFO"
      },
      "source": [
        "#### Calcula a média aritmética do produto das matrizes dos tokens das sentenças do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU5_Uzkn9UFO",
        "outputId": "52689cd6-eb69-40ba-9bc1-0011f3d51c34"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSprod = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Para a multiplicação pode ocorrer número de colunas(qtde_tokens) de Si tem que ser igual ao número de linhas(embeddings) de Sj.\n",
        "    # Permute realiza a troca da dimensão 1(qtde_embeddings) pela 0(qtde_tokesn) na sentença Sj para que seja possível realizar o produto. \n",
        "    # Ocorre a troca da qtde_tokens pela quantidade de embeddings.\n",
        "    # Entrada: (<qtde_tokensSi> x <768 ou 1024>) x (<qtde_tokensSj> x <768 ou 1024>)\n",
        "    # Permute: <qtde_tokensSi> x <768 ou 1024>) x (<768 ou 1024> x <qtde_tokensSj>)\n",
        "    Sprod = torch.matmul(embeddingSi, embeddingSj.permute(1,0))    \n",
        "    # Saída: <qtde_tokensSi> x  <qtde_tokensSj    #print(\"Sprod=\", Sprod.shape)\n",
        "    \n",
        "    # Calcula a média dos embeddings do produto para realizar a soma\n",
        "    somaSprod = somaSprod + torch.mean(Sprod)\n",
        "\n",
        "DprodOriginal = float(somaSprod)/float(n-1)\n",
        "print(\"Dprod Original:\", DprodOriginal)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dprod Original: 932.7980143229166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLvjf-Zs9UFO"
      },
      "source": [
        "#### Calcula a média aritmética do produto das matrizes dos tokens das sentenças do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBztHSPu9UFO",
        "outputId": "2e030ad8-e0a4-46d0-c3e6-3c4ced4e7af6"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSprod = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        " \n",
        "    # Produto entre os embeddings de Si e Sj    \n",
        "    # Para a multiplicação pode ocorrer número de colunas(qtde_tokens) de Si tem que ser igual ao número de linhas(embeddings) de Sj.\n",
        "    # Permute realiza a troca da dimensão 1(qtde_embeddings) pela 0(qtde_tokesn) na sentença Sj para que seja possível realizar o produto. \n",
        "    # Ocorre a troca da qtde_tokens pela quantidade de embeddings.\n",
        "    # Entrada: (<qtde_tokensSi> x <768 ou 1024>) x (<qtde_tokensSj> x <768 ou 1024>)\n",
        "    # Permute: <qtde_tokensSi> x <768 ou 1024>) x (<768 ou 1024> x <qtde_tokensSj>)\n",
        "    Sprod = torch.matmul(embeddingSi, embeddingSj.permute(1,0))    \n",
        "    # Saída: <qtde_tokensSi> x  <qtde_tokensSj>\n",
        "    #print(\"Sprod=\", Sprod.shape)\n",
        "        \n",
        "    # Calcula a média dos embeddings do produto para realizar a soma\n",
        "    somaSprod = somaSprod + torch.mean(Sprod)\n",
        "\n",
        "DprodPermutado = float(somaSprod)/float(np-1)\n",
        "print(\"Dprod permutado:\", DprodPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dprod permutado: 942.978515625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG6KoIta9UFO"
      },
      "source": [
        "#### Compara as médias do produto dos embeddings das sentenças do documento original e permutado\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Vbb4cRp9UFO",
        "outputId": "421bbbee-fdfc-4845-e979-ab22a1019064"
      },
      "source": [
        "print(\"Dprod Original :\", DprodOriginal)\n",
        "print(\"Dprod Permutado:\", DprodPermutado)\n",
        "\n",
        "if (DprodOriginal < DprodPermutado):\n",
        "    print(\"Documento original tem maior similaridade com o produto entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade com o produto entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dprod Original : 932.7980143229166\n",
            "Dprod Permutado: 942.978515625\n",
            "Documento original tem maior similaridade com o produto entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yal-y-_h9UFP"
      },
      "source": [
        "### Produto Absoluto entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCHnG10W9UFP"
      },
      "source": [
        "#### Calcula a média aritmética do produto das matrizes entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8_GGqrY9UFP",
        "outputId": "2b41b4b5-8bc5-4b54-9aab-28431c980786"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSprodabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "    \n",
        "    # Produto entre os embeddings de Si e Sj\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>\n",
        "    # Para a multiplicação pode ocorrer número de colunas(qtde_tokens) de Si tem que ser igual ao número de linhas(embeddings) de Sj.\n",
        "    # Permute realiza a troca da dimensão 1(qtde_embeddings) pela 0(qtde_tokesn) na sentença Sj para que seja possível realizar o produto. \n",
        "    # Ocorre a troca da qtde_tokens pela quantidade de embeddings.\n",
        "    # Entrada: (<qtde_tokensSi> x <768 ou 1024>) x (<qtde_tokensSj> x <768 ou 1024>)\n",
        "    # Permute: <qtde_tokensSi> x <768 ou 1024>) x (<768 ou 1024> x <qtde_tokensSj>)\n",
        "    Sprodabs = abs(torch.matmul(embeddingSi, embeddingSj.permute(1,0)))   \n",
        "    # Saída: <qtde_tokensSi> x  <qtde_tokensSj>\n",
        "    #print(\"Sprodabs=\", Sprodabs.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings do produto para realizar a soma\n",
        "    somaSprodabs = somaSprodabs + torch.mean(Sprodabs)\n",
        "\n",
        "DprodabsOriginal = float(somaSprodabs)/float(n-1)\n",
        "print(\"Dprodabs Original:\", DprodabsOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dprodabs Original: 932.7980143229166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIYbcLIu9UFP"
      },
      "source": [
        "#### Calcula a média aritmética do produto das matrizes entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpzIgQT99UFP",
        "outputId": "a5056a7f-d15d-4f45-c615-e427380aeb8b"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSprodabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        " \n",
        "    # Produto entre os embeddings de Si e Sj\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>\n",
        "    # Para a multiplicação pode ocorrer número de colunas(qtde_tokens) de Si tem que ser igual ao número de linhas(embeddings) de Sj.\n",
        "    # Permute realiza a troca da dimensão 1(qtde_embeddings) pela 0(qtde_tokesn) na sentença Sj para que seja possível realizar o produto. \n",
        "    # Ocorre a troca da qtde_tokens pela quantidade de embeddings.\n",
        "    # Entrada: (<qtde_tokensSi> x <768 ou 1024>) x (<qtde_tokensSj> x <768 ou 1024>)\n",
        "    # Permute: <qtde_tokensSi> x <768 ou 1024>) x (<768 ou 1024> x <qtde_tokensSj>)\n",
        "    Sprodabs = abs(torch.matmul(embeddingSi, embeddingSj.permute(1,0)))    \n",
        "    # Saída: <qtde_tokensSi> x  <qtde_tokensSj>\n",
        "    #print(\"Sprodabs=\", Sprodabs.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings do produto para realizar a soma\n",
        "    somaSprodabs = somaSprodabs + torch.mean(Sprodabs)\n",
        "\n",
        "DprodabsPermutado = float(somaSprodabs)/float(np-1)\n",
        "print(\"Dprodabs permutado:\", DprodabsPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dprodabs permutado: 942.978515625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4SycQ8V9UFP"
      },
      "source": [
        "#### Compara as médias do produto absoluto dos embeddings das sentenças do documento original e permutado\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WgYdIQt9UFQ",
        "outputId": "4732158f-d63a-4635-84b3-00f486385ca6"
      },
      "source": [
        "print(\"Dprodabs Original :\", DprodabsOriginal)\n",
        "print(\"Dprodabs Permutado:\", DprodabsPermutado)\n",
        "\n",
        "if (DprodabsOriginal < DprodabsPermutado):\n",
        "    print(\"Documento original tem maior similaridade com o produto absoluto entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade com o produto absoluto entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dprodabs Original : 932.7980143229166\n",
            "Dprodabs Permutado: 942.978515625\n",
            "Documento original tem maior similaridade com o produto absoluto entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-QjFWm79UFQ"
      },
      "source": [
        "### Produto Escalar entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jjYL-__9UFQ"
      },
      "source": [
        "#### Calcula a média aritmética do produto escalar entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento orginal. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRbl1m1f9UFQ",
        "outputId": "ff11d2b1-9dda-48ce-d770-62be1737331a"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSprode = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "\n",
        "    # Produto escalar entre os embeddings de Si e Sj, pois Si e Sj possui uma única dimensão\n",
        "    # https://pytorch.org/docs/master/generated/torch.matmul.html#torch.matmul\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sprode = torch.matmul(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real \n",
        "    #print(\"Sprode=\", Sprode.shape)\n",
        "        \n",
        "    somaSprode = somaSprode + Sprode\n",
        "\n",
        "DprodeOriginal = float(somaSprode)/float(n-1)\n",
        "print(\"Dprode Original:\", DprodeOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dprode Original: 932.798095703125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SndrH3Rm9UFQ"
      },
      "source": [
        "#### Calcula a média aritmética do produto escalar entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1NcHs4L9UFQ",
        "outputId": "d4e8a52e-2742-42cc-e44d-bd0abd205b55"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSprode = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Produto escalar entre os embeddings de Si e Sj, pois Si e Sj possui uma única dimensão\n",
        "    # https://pytorch.org/docs/master/generated/torch.matmul.html#torch.matmul\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sprode = torch.matmul(mediaEmbeddingSi, mediaEmbeddingSj)    \n",
        "    # Saída: Um número real \n",
        "    #print(\"Sprode=\", Sprode)\n",
        "        \n",
        "    somaSprode = somaSprode + Sprode\n",
        "\n",
        "DprodePermutado = float(somaSprode)/float(np-1)\n",
        "print(\"Dprod permutado:\", DprodePermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dprod permutado: 942.9785970052084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNA1vDqO9UFQ"
      },
      "source": [
        "#### Compara as médias do produto escalar dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Dprod Original : 230.61181640625\n",
        "Dprod Permutado: 227.75482177734375\n",
        "Documento original tem maior similaridade com o produto entre as sentenças!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxLy9rgE9UFR",
        "outputId": "d000e98f-f4da-4e5f-82ea-e0c2726b0677"
      },
      "source": [
        "print(\"Dprode Original :\", DprodeOriginal)\n",
        "print(\"Dprode Permutado:\", DprodePermutado)\n",
        "\n",
        "if (DprodeOriginal < DprodePermutado):\n",
        "    print(\"Documento original tem maior similaridade com o produto entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade com o produto entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dprode Original : 932.798095703125\n",
            "Dprode Permutado: 942.9785970052084\n",
            "Documento original tem maior similaridade com o produto entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_-S2qr_9UFR"
      },
      "source": [
        "### Produto Escalar Absoluto entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZo5LYym9UFR"
      },
      "source": [
        "#### Calcula a média aritmética do produto escalar absoluto entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ocfC5nd9UFR",
        "outputId": "4701b9e0-6b0d-4e1a-986d-8da0977d573f"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSprodeabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Produto escalar entre os embeddings de Si e Sj, pois Si e Sj possui uma única dimensão\n",
        "    # https://pytorch.org/docs/master/generated/torch.matmul.html#torch.matmul\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sprodeabs = torch.matmul(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real \n",
        "    #print(\"Sprodeabs=\", Sprodeabs.shape)\n",
        "\n",
        "    somaSprodeabs = somaSprodeabs + Sprodeabs\n",
        "\n",
        "DprodeabsOriginal = float(somaSprodeabs)/float(n-1)\n",
        "print(\"Dprodabs Original:\", DprodeabsOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dprodabs Original: 932.798095703125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykOLp8bT9UFR"
      },
      "source": [
        "#### Calcula a média aritmética do produto escalar absoluto entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSh4ZRBn9UFR",
        "outputId": "7db8257e-4460-48d1-c743-8912d98f18c1"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSprodeabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "    \n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "    \n",
        "    # Produto escalar entre os embeddings de Si e Sj, pois Si e Sj possui uma única dimensão\n",
        "    # https://pytorch.org/docs/master/generated/torch.matmul.html#torch.matmul\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sprodeabs = torch.matmul(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real \n",
        "    #print(\"Sprodeabs=\", Sprodeabs.shape)\n",
        "    \n",
        "    somaSprodeabs = somaSprodeabs + Sprodeabs\n",
        "\n",
        "DprodeabsPermutado = float(somaSprodeabs)/float(np-1)\n",
        "print(\"Dprodabs permutado:\", DprodeabsPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dprodabs permutado: 942.9785970052084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ33IDIW9UFR"
      },
      "source": [
        "#### Compara as médias do produto escalar absoluto dos embeddings das sentenças do documento original e permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkQi1YDC9UFR",
        "outputId": "154bc95b-16a2-4001-e1c0-142b7af5247d"
      },
      "source": [
        "print(\"Dprodabs Original :\", DprodeabsOriginal)\n",
        "print(\"Dprodabs Permutado:\", DprodeabsPermutado)\n",
        "\n",
        "if (DprodeabsOriginal < DprodeabsPermutado):\n",
        "    print(\"Documento original tem maior similaridade com o produto absoluto entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade com o produto absoluto entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dprodabs Original : 932.798095703125\n",
            "Dprodabs Permutado: 942.9785970052084\n",
            "Documento original tem maior similaridade com o produto absoluto entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz9AlmKi9UFS"
      },
      "source": [
        "### Média entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dy689af9UFS"
      },
      "source": [
        "#### Calcula a média aritmética entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDU8udT29UFS",
        "outputId": "88683b1b-1317-407c-b982-688e7b6860ef"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSavg = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Média entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Savg = (mediaEmbeddingSi.add(mediaEmbeddingSj))/2.0  \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"Savg=\", Savg.shape)  \n",
        "\n",
        "    # Calcula a média dos embeddings do cálculo para realizar a soma\n",
        "    somaSavg = somaSavg + torch.mean(Savg)\n",
        "\n",
        "DavgOriginal = float(somaSavg)/float(n-1)\n",
        "print(\"Davg Original:\", DavgOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Davg Original: -0.02508241931597392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJLduYPA9UFS"
      },
      "source": [
        "#### Calcula a média aritmética entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9trnvi49UFS",
        "outputId": "e5b491f7-1738-4db3-babb-67fcd61f895e"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSavg = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Média entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)  \n",
        "    Savg = (mediaEmbeddingSi.add(mediaEmbeddingSj))/2.0  \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"Savg=\", Savg.shape)  \n",
        "\n",
        "    # Calcula a média dos embeddings do cálculo para realizar a soma\n",
        "    somaSavg = somaSavg + torch.mean(Savg)\n",
        "\n",
        "DavgPermutado = float(somaSavg)/float(np-1)\n",
        "print(\"Davg permutado:\", DavgPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Davg permutado: -0.025191746652126312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXXXZyVW9UFS"
      },
      "source": [
        "#### Compara as médias dos embeddings das sentenças do documento original e permutado\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy18nPa49UFS",
        "outputId": "4e4247f8-b4f5-4cb7-93cf-e8cdc98a22bf"
      },
      "source": [
        "print(\"Davg Original :\", DavgOriginal)\n",
        "print(\"Davg Permutado:\", DavgPermutado)\n",
        "\n",
        "if (DavgOriginal < DavgPermutado):\n",
        "    print(\"Documento original tem maior similaridade com a média entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade com a mpedia entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Davg Original : -0.02508241931597392\n",
            "Davg Permutado: -0.025191746652126312\n",
            "Documento Permutado tem menor similaridade com a mpedia entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX-s-jyF9UFT"
      },
      "source": [
        "### Similaridade de cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSSZT6m9UFT"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCoseno(sentenca1, sentenca2):\n",
        "  similaridade = 1 - cosine(sentenca1, sentenca2)\n",
        "  return similaridade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2xmrGvd9UFT"
      },
      "source": [
        "#### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbyuNamR9UFT",
        "outputId": "4efea4bc-1336-4a34-8170-5c3a4f3b8867"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>) \n",
        "    Scos = similaridadeCoseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "DcosOriginal = float(somaScos)/float(n-1)\n",
        "print(\"Dcos Original:\", DcosOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dcos Original: 0.8301623066266378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WUjUZw_9UFT"
      },
      "source": [
        "#### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unc7fF7i9UFT",
        "outputId": "e7c2b4c8-d4cc-47d3-d74b-e9a8d9dbc855"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCoseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "DcosPermutado = float(somaScos)/float(np-1)\n",
        "print(\"Dcos Original:\", DcosPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dcos Original: 0.8546958367029825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPygSiX_9UFT"
      },
      "source": [
        "#### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 1.\n",
        "- Documentos com sentenças diferenntes resulta uma medida menor que 1.\n",
        "- Documento com sentenças muito diferentes apresentam valores menores que 1.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de similaridade.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zD7FTPz9UFT",
        "outputId": "8215f084-82c2-46f7-cb99-a0a85d9a835e"
      },
      "source": [
        "print(\"Dcos Original :\", DcosOriginal)\n",
        "print(\"Dcos Permutado:\", DcosPermutado)\n",
        "\n",
        "if (DcosOriginal > DcosPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dcos Original : 0.8301623066266378\n",
            "Dcos Permutado: 0.8546958367029825\n",
            "Documento Permutado tem menor similaridade de cosseno entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVcRGoM09UFU"
      },
      "source": [
        "### Distância euclidiana entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância L2 ou norma L2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqK7DcTJ9UFU"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distanciaEuclidiana(sentenca1, sentenca2):\n",
        "  distancia = euclidean(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtzn3Ll69UFU"
      },
      "source": [
        "#### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McejBc0P9UFU",
        "outputId": "e65341cc-0791-4831-aff3-cd0f2e57bab2"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Distância euclidiana entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)  \n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "DeucOriginal = float(somaSeuc)/float(n-1)\n",
        "print(\"Deuc Original:\", DeucOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Deuc Original: 19.280046463012695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7_eRCMV9UFU"
      },
      "source": [
        "#### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBEbKqiH9UFU",
        "outputId": "642ad9a4-ba28-4953-901b-e119e2973ad5"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Distância euclidiana entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>) \n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "DeucPermutado = float(somaSeuc)/float(np-1)\n",
        "print(\"Deuc Original:\", DeucPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Deuc Original: 17.829952239990234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5qzdOVy9UFU"
      },
      "source": [
        "#### Compara as médias da distância euclidiana dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnBVeDrv9UFU",
        "outputId": "68b5759f-4f0c-41a6-b2cf-13e8d39565f4"
      },
      "source": [
        "print(\"Deuc Original :\", DeucOriginal)\n",
        "print(\"Deuc Permutado:\", DeucPermutado)\n",
        "\n",
        "if (DeucOriginal > DeucPermutado):\n",
        "    print(\"Documento original tem maior distância euclidiana entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor distância euclidiana entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deuc Original : 19.280046463012695\n",
            "Deuc Permutado: 17.829952239990234\n",
            "Documento original tem maior distância euclidiana entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojaQA2C49UFV"
      },
      "source": [
        "### Distância Manhattan entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4VBoLbH9UFV"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def distanciaEManhattan(sentenca1, sentenca2):\n",
        "  distancia = cityblock(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN96DPuI9UFV"
      },
      "source": [
        "#### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyAvO6WV9UFV",
        "outputId": "265fc928-0ebe-4677-84ce-223c9d39b824"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Distância de manhattan entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>) \n",
        "    Sman = distanciaEManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "DmanOriginal = float(somaSman)/float(n-1)\n",
        "print(\"Dman Original:\", DmanOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dman Original: 816.2806193033854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oenwZkqO9UFV"
      },
      "source": [
        "#### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg81Wnik9UFV",
        "outputId": "b6e390e2-7b0e-4e9c-f136-789d85322279"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Distância de manhattan entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaEManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "DmanPermutado = float(somaSman)/float(np-1)\n",
        "print(\"Deuc Original:\", DmanPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Deuc Original: 755.4674682617188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcTxjUcc9UFV"
      },
      "source": [
        "#### Compara as médias da distância de manhattan dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JzGE3he9UFV",
        "outputId": "374e14a4-3c96-4c81-8052-5832a592f8e0"
      },
      "source": [
        "print(\"Dman Original :\", DmanOriginal)\n",
        "print(\"Dman Permutado:\", DmanPermutado)\n",
        "\n",
        "if (DmanOriginal > DmanPermutado):\n",
        "    print(\"Documento original tem maior distância de manhattan entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor distância de manhattan entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dman Original : 816.2806193033854\n",
            "Dman Permutado: 755.4674682617188\n",
            "Documento original tem maior distância de manhattan entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaWJ6lU1a3RN"
      },
      "source": [
        "### Resumo\n",
        "\n",
        "Resultado das medidas utilizando as quatro últimas camadas do BERT.\n",
        "\n",
        "Base(MEAN):\n",
        "- Dsub       :   0.00028153          -0.00050018\n",
        "- Dubabs     :   0.26571635          0.24592040\n",
        "- Dprod      :   932.79801432          942.97851562\n",
        "- Dprodabs   :   932.79801432          942.97851562\n",
        "- Dprode     :   932.79809570          942.97859701\n",
        "- Dprodeabs  :   932.79809570          942.97859701\n",
        "- Davg       :   -0.02508242          -0.02519175\n",
        "- Dcos       :   0.83016231          0.85469584\n",
        "- Deuc       :   19.28004646          17.82995224\n",
        "- Dman       :   816.28061930          755.46746826\n",
        "----------------------\n",
        "Resultado das medidas utilizando a última camada do BERT.\n",
        "\n",
        "Base(MEAN):\n",
        "- Dsub       :   0.00074918          -0.00064076\n",
        "- Dubabs     :   0.17778097          0.17206367\n",
        "- Dprod      :   43.70739746          43.55047607\n",
        "- Dprodabs   :   43.70739746          43.55047607\n",
        "- Dprode     :   43.70739746          43.55047607\n",
        "- Dprodeabs  :   43.70739746          43.55047607\n",
        "- Davg       :   -0.00276791          -0.00271371\n",
        "- Dcos       :   0.67999101          0.69653825\n",
        "- Deuc       :   6.34085274          6.15486606\n",
        "- Dman       :   136.53579712          132.14489237\n",
        "\n",
        "Base(MAX):\n",
        "- Dsub       :   -0.02765538          0.02360132\n",
        "- Dubabs     :   0.26021336          0.25770577\n",
        "- Dprod      :   43.70739746          43.55047607\n",
        "- Dprodabs   :   43.70739746          43.55047607\n",
        "- Dprode     :   200.03959147          199.48846436\n",
        "- Dprodeabs  :   200.03959147          199.48846436\n",
        "- Davg       :   0.43032602          0.42829903\n",
        "- Dcos       :   0.81970002          0.82297109\n",
        "- Deuc       :   9.42460442          9.29481570\n",
        "- Dman       :   199.84384664          197.91803996"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhAP-gfja3RN",
        "outputId": "1d49bd1b-1cbc-4207-d45a-5fa6e39b62e9"
      },
      "source": [
        "print(\"Resultado das medidas utilizando as quatro últimas camada do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print('Dsub       :   {:.8f}          {:.8f}'.format(DsubOriginal,DsubPermutado))\n",
        "print('Dubabs     :   {:.8f}          {:.8f}'.format(DsubabsOriginal,DsubabsPermutado))\n",
        "print('Dprod      :   {:.8f}          {:.8f}'.format(DprodOriginal,DprodPermutado))\n",
        "print('Dprodabs   :   {:.8f}          {:.8f}'.format(DprodabsOriginal,DprodabsPermutado))\n",
        "print('Dprode     :   {:.8f}          {:.8f}'.format(DprodeOriginal,DprodePermutado))\n",
        "print('Dprodeabs  :   {:.8f}          {:.8f}'.format(DprodeabsOriginal,DprodeabsPermutado))\n",
        "print('Davg       :   {:.8f}          {:.8f}'.format(DavgOriginal,DavgPermutado))\n",
        "print('Dcos       :   {:.8f}          {:.8f}'.format(DcosOriginal,DcosPermutado))\n",
        "print('Deuc       :   {:.8f}          {:.8f}'.format(DeucOriginal,DeucPermutado))\n",
        "print('Dman       :   {:.8f}          {:.8f}'.format(DmanOriginal,DmanPermutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resultado das medidas utilizando as quatro últimas camada do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Dsub       :   0.00028153          -0.00050018\n",
            "Dubabs     :   0.26571635          0.24592040\n",
            "Dprod      :   932.79801432          942.97851562\n",
            "Dprodabs   :   932.79801432          942.97851562\n",
            "Dprode     :   932.79809570          942.97859701\n",
            "Dprodeabs  :   932.79809570          942.97859701\n",
            "Davg       :   -0.02508242          -0.02519175\n",
            "Dcos       :   0.83016231          0.85469584\n",
            "Deuc       :   19.28004646          17.82995224\n",
            "Dman       :   816.28061930          755.46746826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJKcCTMPL341"
      },
      "source": [
        "## 9 - Exemplo sentenças de documento original e permutado utilizando embedding da concatenação das 4 últimas camadas do BERT usando estratégia MAX\n",
        "\n",
        "Como estamos utilizando os embeddings concatenado das 4 últimas camadas onde ocorre 768 entenda-se 3072 que é o resultado de 768 por 4 que é a dimensão do MCL BERT de tamanho base. E onde ocorre 1024 entenda-se 4096 que é o resultado de 1024 por 4 que é a dimensão do MCL BERT de tamanho large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC_lpJhbM0iL"
      },
      "source": [
        "### Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzLY--ZCM0iS",
        "outputId": "55effa43-face-4017-977a-3b63350ddf1f"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_original = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "stringDocumentoPermutado = ' '.join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + stringDocumentoPermutado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print('{:>3} {:<12} {:>6,}'.format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWF6G0EqM0iT"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPokJjkHM0iT",
        "outputId": "5b28fcb9-3386-454c-a2b3-bb0403d5e693"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvGb4Lr9M0iT"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x23ik0ehM0iU"
      },
      "source": [
        "# Importa a bibliteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpDIoOYgM0iU"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda todas as camadas da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mGvXQN2M0iU"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VajVWPxSM0iU",
        "outputId": "1b60d3f0-1c78-4e6f-964f-1f7e3e2724c2"
      },
      "source": [
        "# Cria uma lista com os tensores a serem concatenados\n",
        "# Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "# Lista com os tensores a serem concatenados\n",
        "listaConcat = []\n",
        "# Percorre os 4 últimos\n",
        "for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "# Realiza a concatenação dos embeddings de todos as camadas\n",
        "# Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "# Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "\n",
        "print (\"O vetor da  concatenação das 4 últimas camadas oculta tem o formato:\", concat4_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor da  concatenação das 4 últimas camadas oculta tem o formato: torch.Size([1, 26, 3072])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f-UKTRuM0iU"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ue_2PX2M0iV",
        "outputId": "53e145fc-b469-42e6-8f3a-91388532b1b4"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoOriginal = torch.squeeze(concat4_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", embeddingDocumentoOriginal.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([26, 3072])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCwadAdmM0iV"
      },
      "source": [
        "Confirmando vetores dependentes do contexto\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb8Cm77zM0iV",
        "outputId": "e09cb07a-8c4a-44b8-b755-f27b8ba0a14b"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGjE2y_pM0iV"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W28pu2G9M0iV",
        "outputId": "eb8e6e82-9cda-49de-f399-c42d8732df57"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "embeddingSentenca1Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca1Original, tokenizer)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca2Original, tokenizer)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca3Original, tokenizer)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca4Original, tokenizer)\n",
        "\n",
        "print('Os primeiros 4 valores de cada sentença do documento original.')\n",
        "\n",
        "print('\\nSentença 1:', sentenca1Original,'-', str(embeddingSentenca1Original[:4]))\n",
        "print('Soma embedding Sentença1:', sentenca1Original,'-', str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print('\\nSentença 2:', sentenca2Original,'-', str(embeddingSentenca2Original[:4]))\n",
        "print('Soma embedding Sentença2:', sentenca2Original,'-', str(torch.sum(embeddingSentenca2Original[:4])))\n",
        "\n",
        "print('\\nSentença 3:', sentenca3Original,'-', str(embeddingSentenca3Original[:4]))\n",
        "print('Soma embedding Sentença3:', sentenca3Original,'-', str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print('\\nSentença 4:', sentenca4Original,'-', str(embeddingSentenca4Original[:4]))\n",
        "print('Soma embedding Sentença4:', sentenca4Original,'-', str(torch.sum(embeddingSentenca4Original[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: Bom Dia, professor. - tensor([[-0.0680, -0.4615,  0.3552,  ...,  0.1657,  0.3478,  0.4525],\n",
            "        [-0.1000, -0.0630,  0.0840,  ..., -0.3926,  0.7971,  0.3287],\n",
            "        [-0.3165,  0.4208,  0.2178,  ..., -0.3834,  0.9520, -0.2181],\n",
            "        [ 0.1248,  0.2383,  0.8987,  ..., -0.1989,  0.1001,  0.1140]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-327.0100)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.5894, -0.4310,  0.1449,  ...,  0.2399, -0.8495, -0.1672],\n",
            "        [ 0.1349, -0.2476,  0.4605,  ..., -0.1353, -0.1674,  0.4057],\n",
            "        [ 0.4359, -0.6972,  0.4066,  ...,  0.5851, -0.0441, -0.0027],\n",
            "        [ 0.0544,  0.1606,  0.4150,  ..., -0.8060, -0.3698,  0.0466]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-315.7253)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[-0.3652, -0.5015, -0.1626,  ..., -0.5431, -0.5805,  0.4617],\n",
            "        [ 0.1421,  0.1797, -0.0014,  ..., -0.7004, -0.4356,  0.7459],\n",
            "        [-0.1274, -0.0926, -0.1861,  ...,  0.6395, -0.4126,  0.7672],\n",
            "        [-0.2886,  0.4056,  0.6759,  ..., -0.2171, -0.4010, -0.5805]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-311.2223)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[-0.0835, -0.4042,  0.4330,  ...,  0.2881,  0.1448,  0.7778],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.0102,  0.1851,  0.4587],\n",
            "        [-0.5474, -0.0118,  0.3950,  ...,  0.1932, -0.0735,  0.2649],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ..., -0.0370,  0.3236, -0.3671]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-307.8872)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDAG_xYHM0iV"
      },
      "source": [
        "Examinando os embeddings do documento original\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN1ZnWWLM0iW",
        "outputId": "5f59250a-8253-4271-a9d2-3830c2fbb91b"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaOriginal = tokenizer.tokenize(sentenca1Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1TokenizadaOriginal)\n",
        "embeddingSentenca1Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca1Original, tokenizer)\n",
        "print('\\nSentença 1 Original=\\'', sentenca1Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca1Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaOriginal = tokenizer.tokenize(sentenca2Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2TokenizadaOriginal)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca2Original, tokenizer)\n",
        "print('\\nSentença 2 Original=\\'', sentenca2Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca2TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca2Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca2Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaOriginal = tokenizer.tokenize(sentenca3Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3TokenizadaOriginal)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca3Original, tokenizer)\n",
        "print('\\nSentença 3 Original=\\'', sentenca3Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca3TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca3Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca3Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaOriginal = tokenizer.tokenize(sentenca4Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4TokenizadaOriginal)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, sentenca4Original, tokenizer)\n",
        "print('\\nSentença 4 Original=\\'', sentenca4Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca4Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Original))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=' Bom Dia, professor. '\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 3072])\n",
            "    Soma embeddings:  -398.53\n",
            "\n",
            "Sentença 2 Original=' Qual o conteúdo da prova? '\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 3072])\n",
            "    Soma embeddings:  -467.44\n",
            "\n",
            "Sentença 3 Original=' Vai cair tudo na prova? '\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 3072])\n",
            "    Soma embeddings:  -466.27\n",
            "\n",
            "Sentença 4 Original=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 3072])\n",
            "    Soma embeddings:  -525.26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWXVLJ1bM0iW"
      },
      "source": [
        "### Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPWjgUWeM0iW",
        "outputId": "9d0e937c-5034-4f16-85a3-b6507825e196"
      },
      "source": [
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_permutado = [documento_original[3],   # \"Aguardo uma resposta, João.\",\n",
        "             documento_original[1],             # \"Qual o conteúdo da prova?\",              \n",
        "             documento_original[0],             # \"Vai cair tudo na prova?\",\n",
        "             documento_original[2]]             # \"Bom Dia, professor.\"]     \n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "stringDocumentoPermutado = ' '.join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + stringDocumentoPermutado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print('{:>3} {:<12} {:>6,}'.format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 Bom           8,399\n",
            " 15 Dia           3,616\n",
            " 16 ,               117\n",
            " 17 professor     2,917\n",
            " 18 .               119\n",
            " 19 Vai          20,805\n",
            " 20 cair          9,322\n",
            " 21 tudo          2,745\n",
            " 22 na              229\n",
            " 23 prova         2,310\n",
            " 24 ?               136\n",
            " 25 [SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN2heM1aM0iW"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pjakxc4xM0iW",
        "outputId": "af94ea58-092e-4f8f-d675-80057f582abd"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkJeVNRgM0iW"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67NUmOkIM0iX"
      },
      "source": [
        "# Importa a bibliteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TIWhsT3M0iX"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda todas as camadas da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07xtO-rnM0iX"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvV4gue4M0iX"
      },
      "source": [
        "Recupera a saída e concatena as 4 últimas camada do BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svq_3keEM0iX"
      },
      "source": [
        "# Cria uma lista com os tensores a serem concatenados\n",
        "# Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "# Lista com os tensores a serem concatenados\n",
        "listaConcat = []\n",
        "# Percorre os 4 últimos\n",
        "for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "# Realiza a concatenação dos embeddings de todos as camadas\n",
        "# Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "# Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybtNemHbM0iX"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSav8HKIM0iX",
        "outputId": "20e56416-9735-4a88-a2f6-db502b7504dc"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoPermutado = torch.squeeze(concat4_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", embeddingDocumentoPermutado.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([26, 3072])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2EqEuq6M0iY"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_hCQhZnM0iY",
        "outputId": "e7a276dd-a14a-49ea-ba70-58ba982e19b3"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca1Permutado, tokenizer)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca2Permutado, tokenizer)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca3Permutado, tokenizer)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca4Permutado, tokenizer)\n",
        "\n",
        "print('Os primeiros 4 valores de cada sentença do documento permutado.')\n",
        "\n",
        "print('\\nSentença 1:', sentenca1Permutado,'-', str(embeddingSentenca1Permutado[:4]))\n",
        "print('Soma embedding Sentença1:', sentenca1Original,'-', str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print('\\nSentença 2:', sentenca2Permutado,'-', str(embeddingSentenca2Permutado[:4]))\n",
        "print('Soma embedding Sentença2:', sentenca2Permutado,'-', str(torch.sum(embeddingSentenca2Permutado[:4])))\n",
        "\n",
        "print('\\nSentença 3:', sentenca3Permutado,'-', str(embeddingSentenca3Permutado[:4]))\n",
        "print('Soma embedding Sentença3:', sentenca3Permutado,'-', str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print('\\nSentença 4:', sentenca4Permutado,'-', str(embeddingSentenca4Permutado[:4]))\n",
        "print('Soma embedding Sentença4:', sentenca4Permutado,'-', str(torch.sum(embeddingSentenca4Permutado[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.1775,  0.2231,  0.8036],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.3783,  0.1529,  0.5125],\n",
            "        [-0.7358, -0.0988,  0.5145,  ...,  0.0164, -0.1749,  0.2557],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.2708,  0.1942, -0.0740]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-327.0100)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.4295, -0.3426,  0.1005,  ...,  0.2608, -0.8301, -0.1044],\n",
            "        [ 0.1047, -0.1946,  0.4177,  ..., -0.1734, -0.2517,  0.4004],\n",
            "        [ 0.4377, -0.5952,  0.5448,  ...,  0.4539, -0.1388, -0.0339],\n",
            "        [ 0.0898,  0.1063,  0.4610,  ..., -0.5308, -0.4884,  0.2119]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-314.9913)\n",
            "\n",
            "Sentença 3: Bom Dia, professor. - tensor([[-0.0458, -0.4268,  0.2399,  ...,  0.4282,  0.3110,  0.0241],\n",
            "        [ 0.0650, -0.1385,  0.0230,  ..., -0.3674,  0.5677,  0.0532],\n",
            "        [-0.1147,  0.4120,  0.0659,  ..., -0.0354,  0.8405, -0.2598],\n",
            "        [ 0.1290,  0.2181,  0.7986,  ..., -0.1129,  0.0972,  0.1581]])\n",
            "Soma embedding Sentença3: Bom Dia, professor. - tensor(-311.2223)\n",
            "\n",
            "Sentença 4: Vai cair tudo na prova? - tensor([[-0.0414, -0.5222, -0.1612,  ..., -0.5124, -0.7616,  0.1765],\n",
            "        [ 0.4540,  0.0075, -0.0953,  ..., -0.7207, -0.3354,  0.4786],\n",
            "        [ 0.0887, -0.1633, -0.3073,  ...,  0.6813, -0.4292,  0.4572],\n",
            "        [-0.0159,  0.2363,  0.7514,  ..., -0.2911, -0.2646, -0.9683]])\n",
            "Soma embedding Sentença4: Vai cair tudo na prova? - tensor(-299.0247)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXJIz3jPM0iY",
        "outputId": "01ef37a0-5a6a-4115-885f-643d25ecbe65"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaPermutado = tokenizer.tokenize(sentenca1Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1TokenizadaPermutado)\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca1Permutado, tokenizer)\n",
        "print('\\nSentença 1 Permutada=\\'', sentenca1Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca1Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaPermutado = tokenizer.tokenize(sentenca2Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2TokenizadaPermutado)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca2Permutado, tokenizer)\n",
        "print('\\nSentença 2 Permutada=\\'', sentenca2Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca2TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca2Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca2Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaPermutado = tokenizer.tokenize(sentenca3Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3TokenizadaPermutado)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca3Permutado, tokenizer)\n",
        "print('\\nSentença 3 Permutada=\\'', sentenca3Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca3TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca3Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca3Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaPermutado = tokenizer.tokenize(sentenca4Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4TokenizadaPermutado)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoPermutado, stringDocumentoPermutado, sentenca4Permutado, tokenizer)\n",
        "print('\\nSentença 4 Permutada=\\'', sentenca4Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca4Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Permutado))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "\n",
            "Sentença 1 Permutada=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 3072])\n",
            "    Soma embeddings:  -532.56\n",
            "\n",
            "Sentença 2 Permutada=' Qual o conteúdo da prova? '\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 3072])\n",
            "    Soma embeddings:  -467.65\n",
            "\n",
            "Sentença 3 Permutada=' Bom Dia, professor. '\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 14 e término em 18\n",
            "    Formato modelo : torch.Size([5, 3072])\n",
            "    Soma embeddings:  -402.97\n",
            "\n",
            "Sentença 4 Permutada=' Vai cair tudo na prova? '\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 19 e término em 24\n",
            "    Formato modelo : torch.Size([6, 3072])\n",
            "    Soma embeddings:  -447.06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyOCD7COM0iY"
      },
      "source": [
        "### Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ucrFp95M0iY",
        "outputId": "b544a7ad-6ec4-4b51-9c67-2064e500df11"
      },
      "source": [
        "print('\\nSentença 4 Original=\\'', sentenca4Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaOriginal)\n",
        "print('    Formato modelo :', embeddingSentenca4Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Original))\n",
        "print('    Os 4 primeiros embeddings:', str(embeddingSentenca4Original[:4]))\n",
        "\n",
        "print('\\nSentença 1 Permutada=\\'', sentenca1Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaPermutado)\n",
        "print('    Formato modelo :', embeddingSentenca1Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Permutado))\n",
        "print('    Os 4 primeiros embeddings:', str(embeddingSentenca1Permutado[:4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sentença 4 Original=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 3072])\n",
            "    Soma embeddings:  -525.26\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0835, -0.4042,  0.4330,  ...,  0.2881,  0.1448,  0.7778],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.0102,  0.1851,  0.4587],\n",
            "        [-0.5474, -0.0118,  0.3950,  ...,  0.1932, -0.0735,  0.2649],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ..., -0.0370,  0.3236, -0.3671]])\n",
            "\n",
            "Sentença 1 Permutada=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 3072])\n",
            "    Soma embeddings:  -532.56\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.1775,  0.2231,  0.8036],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.3783,  0.1529,  0.5125],\n",
            "        [-0.7358, -0.0988,  0.5145,  ...,  0.0164, -0.1749,  0.2557],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.2708,  0.1942, -0.0740]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnMMfXGg9rW0"
      },
      "source": [
        "### Subtração entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY8NYOlq9rW2"
      },
      "source": [
        "#### Calcula a média aritmética da subtração entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRgv5kmN9rW3",
        "outputId": "5b46fd1b-4c4b-44bc-8a9c-29fac31eefa8"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSsub = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "       \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Ssub = torch.sub(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: <768 ou 1024>  \n",
        "    #print(\"Ssub=\", Ssub.shape)\n",
        "    \n",
        "    # Calcula a média dos embeddings do subtração para realizar a soma\n",
        "    somaSsub = somaSsub + torch.mean(Ssub)\n",
        "\n",
        "DsubOriginal = float(somaSsub)/float(len(documento_original)-1)\n",
        "print(\"Ssub Original:\", DsubOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ssub Original: -0.023183703422546387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yytO-8JX9rW4"
      },
      "source": [
        "#### #### Calcula a média aritmética da subtração entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cG9yNBd9rW4",
        "outputId": "9540eda9-2ebe-4a80-dc4e-f963e63dbfc9"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSsub = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "       \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Ssub = torch.sub(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: <768 ou 1024>  \n",
        "    #print(\"Ssub=\", Ssub.shape)\n",
        "        \n",
        "    # Calcula a média dos embeddings do subtração para realizar a soma\n",
        "    somaSsub = somaSsub + torch.mean(Ssub)\n",
        "\n",
        "DsubPermutado = float(somaSsub)/float(np-1)\n",
        "print(\"Ssub permutado:\", DsubPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ssub permutado: 0.03090549260377884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CHxTN8x9rW4"
      },
      "source": [
        "#### Compara as médias da subtração dos embeddings das sentenças do documento original e permutado.\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwcUxpC_9rW5",
        "outputId": "d5a4dc1d-313a-4810-a303-1704c0620f6b"
      },
      "source": [
        "print(\"Dsub Original :\", DsubOriginal)\n",
        "print(\"Dsub Permutado:\", DsubPermutado)\n",
        "\n",
        "if (DsubOriginal <= DsubPermutado):\n",
        "    print(\"Documento original tem menor diferença entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor diferença entre as sentenças!\")\n",
        "\n",
        "if (DsubOriginal > DsubPermutado):\n",
        "    print(\"Documento original tem menor diferença entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor diferença entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dsub Original : -0.023183703422546387\n",
            "Dsub Permutado: 0.03090549260377884\n",
            "Documento original tem menor diferença entre as sentenças!\n",
            "Documento Permutado tem menor diferença entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNb0Be-C9rW5"
      },
      "source": [
        "### Subtração absoluta entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G_Ibtqr9rW5"
      },
      "source": [
        "#### Calcula a média aritmética da subtração absoluta entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTZl_6jC9rW5",
        "outputId": "5c60cbf3-63fa-46d9-95ca-926ffa08e440"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSsubabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "       \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>) \n",
        "    Ssubabs = abs(torch.sub(maiorEmbeddingSi, maiorEmbeddingSj))\n",
        "    # Saída: <768 ou 1024>  \n",
        "    #print(\"Ssubabs=\", Ssubabs.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings do subtração para realizar a soma\n",
        "    somaSsubabs = somaSsubabs + torch.mean(Ssubabs)\n",
        "\n",
        "DsubabsOriginal = float(somaSsubabs)/float(n-1)\n",
        "print(\"Dsubabs Original:\", DsubabsOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dsubabs Original: 0.3687093257904053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhqWM04A9rW6"
      },
      "source": [
        "#### Calcula a média aritmética da subtração absoluta entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSfknHk79rW6",
        "outputId": "cd2b11b0-8127-4331-f457-14e42982911f"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSsubabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "       \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    Ssubabs = abs(torch.sub(maiorEmbeddingSi, maiorEmbeddingSj))\n",
        "    # Saída: <768 ou 1024>  \n",
        "    #print(\"Ssub=\", Ssub.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings do subtração para realizar a soma\n",
        "    somaSsubabs = somaSsubabs + torch.mean(Ssubabs)\n",
        "\n",
        "DsubabsPermutado = float(somaSsubabs)/float(np-1)\n",
        "print(\"Dsubabs permutado:\", DsubabsPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dsubabs permutado: 0.35224469502766925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii3SUPsr9rW6"
      },
      "source": [
        "#### Compara as médias da subtração absoluta dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifYFuHfc9rW6",
        "outputId": "78bf4fa5-7176-40f5-ea8d-1f31d845bc07"
      },
      "source": [
        "print(\"Dsubabs Original :\", DsubabsOriginal)\n",
        "print(\"Dsubabs Permutado:\", DsubabsPermutado)\n",
        "\n",
        "if (DsubabsOriginal <= DsubabsPermutado):\n",
        "    print(\"Documento original tem menor diferença absoluta entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor diferença absoluta entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dsubabs Original : 0.3687093257904053\n",
            "Dsubabs Permutado: 0.35224469502766925\n",
            "Documento Permutado tem menor diferença absoluta entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pleFoGVL9rW7"
      },
      "source": [
        "### Produto entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_2VJvy99rW7"
      },
      "source": [
        "#### Calcula a média aritmética do produto das matrizes dos tokens das sentenças do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-n34tC6C9rW7",
        "outputId": "45d4a5ad-2d10-4d45-8ff4-e104b7c45b42"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSprod = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "   # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Para a multiplicação pode ocorrer número de colunas(qtde_tokens) de Si tem que ser igual ao número de linhas(embeddings) de Sj.\n",
        "    # Permute realiza a troca da dimensão 1(qtde_embeddings) pela 0(qtde_tokesn) na sentença Sj para que seja possível realizar o produto. \n",
        "    # Ocorre a troca da qtde_tokens pela quantidade de embeddings.\n",
        "    # Entrada: (<qtde_tokensSi> x <768 ou 1024>) x (<qtde_tokensSj> x <768 ou 1024>)\n",
        "    # Permute: <qtde_tokensSi> x <768 ou 1024>) x (<768 ou 1024> x <qtde_tokensSj>)\n",
        "    Sprod = torch.matmul(embeddingSi, embeddingSj.permute(1,0))    \n",
        "    # Saída: <qtde_tokensSi> x  <qtde_tokensSj>\n",
        "    #print(\"Sprod=\", Sprod.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings do produto para realizar a soma\n",
        "    somaSprod = somaSprod + torch.mean(Sprod)\n",
        "\n",
        "DprodOriginal = float(somaSprod)/float(n-1)\n",
        "print(\"Dprod Original:\", DprodOriginal)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dprod Original: 932.7980143229166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9dFdAk09rW7"
      },
      "source": [
        "#### Calcula a média aritmética do produto das matrizes dos tokens das sentenças do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lCbYqm69rW7",
        "outputId": "c29ac763-8b61-4693-ed17-0ec836e199fa"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSprod = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Para a multiplicação pode ocorrer número de colunas(qtde_tokens) de Si tem que ser igual ao número de linhas(embeddings) de Sj.\n",
        "    # Permute realiza a troca da dimensão 1(qtde_embeddings) pela 0(qtde_tokesn) na sentença Sj para que seja possível realizar o produto. \n",
        "    # Ocorre a troca da qtde_tokens pela quantidade de embeddings.\n",
        "    # Entrada: (<qtde_tokensSi> x <768 ou 1024>) x (<qtde_tokensSj> x <768 ou 1024>)\n",
        "    # Permute: <qtde_tokensSi> x <768 ou 1024>) x (<768 ou 1024> x <qtde_tokensSj>)\n",
        "    Sprod = torch.matmul(embeddingSi, embeddingSj.permute(1,0))    \n",
        "    # Saída: <qtde_tokensSi> x  <qtde_tokensSj>\n",
        "    #print(\"Sprod=\", Sprod.shape)\n",
        "    \n",
        "    # Encontra os maiores embeddings do produto para realizar a soma\n",
        "    somaSprod = somaSprod + torch.mean(Sprod)\n",
        "\n",
        "DprodPermutado = float(somaSprod)/float(np-1)\n",
        "print(\"Dprod permutado:\", DprodPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dprod permutado: 942.978515625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SNwytaA9rW7"
      },
      "source": [
        "#### Compara as médias do produto dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Dois documentos(vetores) que apontam em uma direção semelhante retornam um produto escalar positivo.\n",
        "- Dois documentos perpendiculares retornam um produto escalar de zero.\n",
        "- Dois documentos(vetores) que apontam em direções opostas retornam um produto escalar negativo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he5QkdJ_9rW8",
        "outputId": "fe1de97d-a8ac-4e92-9e59-21b3c11a7520"
      },
      "source": [
        "print(\"Dprod Original :\", DprodOriginal)\n",
        "print(\"Dprod Permutado:\", DprodPermutado)\n",
        "\n",
        "if (DprodOriginal > 0 and DprodPermutado > 0):\n",
        "    print(\"As sentenças do documento original e as sentenças do documento permutado estão relacionadas!\")\n",
        "else:\n",
        "  if (DprodOriginal < 0 and DprodPermutado > 0) or (DprodOriginal > 0 and DprodPermutado < 0):\n",
        "    print(\"As sentenças do documento original e as sentenças do documento permutado não estão relacionadas!\")\n",
        "  else:  \n",
        "    print(\"As sentenças do documento original e as sentenças do documento permutado não possuem relação!\")              "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dprod Original : 932.7980143229166\n",
            "Dprod Permutado: 942.978515625\n",
            "As sentenças do documento original e as sentenças do documento permutado estão relacionadas!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMHu0Zfo9rW8"
      },
      "source": [
        "### Produto Absoluto entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npiVtoyv9rW8"
      },
      "source": [
        "#### Calcula a média aritmética do produto absoluto das matrizes dos tokens das sentenças do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ha0bMTI9rW8",
        "outputId": "f58eb429-6255-4855-9ea2-835b5f72c792"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSprodabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "    \n",
        "    # # Para a multiplicação pode ocorrer número de colunas(qtde_tokens) de Si tem que ser igual ao número de linhas(embeddings) de Sj.\n",
        "    # Permute realiza a troca da dimensão 1(qtde_embeddings) pela 0(qtde_tokesn) na sentença Sj para que seja possível realizar o produto. \n",
        "    # Ocorre a troca da qtde_tokens pela quantidade de embeddings.\n",
        "    # Entrada: (<qtde_tokensSi> x <768 ou 1024>) x (<qtde_tokensSj> x <768 ou 1024>)\n",
        "    # Permute: <qtde_tokensSi> x <768 ou 1024>) x (<768 ou 1024> x <qtde_tokensSj>)\n",
        "    Sprodabs = abs(torch.matmul(embeddingSi, embeddingSj.permute(1,0)))   \n",
        "    # Saída: <qtde_tokensSi> x  <qtde_tokensSj>\n",
        "    #print(\"Sprodabs=\", Sprodabs.shape)\n",
        "   \n",
        "    # Encontra os maiores embeddings absolutos do produto para realizar a soma\n",
        "    somaSprodabs = somaSprodabs + torch.mean(Sprodabs)\n",
        "\n",
        "DprodabsOriginal = float(somaSprodabs)/float(n-1)\n",
        "print(\"Dprodabs Original:\", DprodabsOriginal)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dprodabs Original: 932.7980143229166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9lDE3Ri9rW8"
      },
      "source": [
        "#### Calcula a média aritmética do produto absoluto das matrizes dos tokens das sentenças do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtLM8btq9rW8",
        "outputId": "bde9b036-b74d-4a6b-b55f-3c2abe83e1ec"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSprodabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        " \n",
        "     # Para a multiplicação pode ocorrer número de colunas(qtde_tokens) de Si tem que ser igual ao número de linhas(embeddings) de Sj.\n",
        "    # Permute realiza a troca da dimensão 1(qtde_embeddings) pela 0(qtde_tokesn) na sentença Sj para que seja possível realizar o produto. \n",
        "    # Ocorre a troca da qtde_tokens pela quantidade de embeddings.\n",
        "    # Entrada: (<qtde_tokensSi> x <768 ou 1024>) x (<qtde_tokensSj> x <768 ou 1024>)\n",
        "    # Permute: <qtde_tokensSi> x <768 ou 1024>) x (<768 ou 1024> x <qtde_tokensSj>)\n",
        "    Sprodabs = abs(torch.matmul(embeddingSi, embeddingSj.permute(1,0)))    \n",
        "    # Saída: <qtde_tokensSi> x  <qtde_tokensSj>\n",
        "    #print(\"Sprodabs=\", Sprodabs.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings absolutos do produto para realizar a soma\n",
        "    somaSprodabs = somaSprodabs + torch.mean(Sprodabs)\n",
        "\n",
        "DprodabsPermutado = float(somaSprodabs)/float(np-1)\n",
        "print(\"Dprodabs permutado:\", DprodabsPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dprodabs permutado: 942.978515625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cKEk9ZG9rW8"
      },
      "source": [
        "#### Compara as médias do produto absoluto dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Dois documentos(vetores) que apontam em uma direção semelhante retornam um produto escalar positivo.\n",
        "- Dois documentos perpendiculares retornam um produto escalar de zero.\n",
        "- Dois documentos(vetores) que apontam em direções opostas retornam um produto escalar negativo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69q1TzXv9rW9",
        "outputId": "572dd61f-d452-4cf4-dbfe-3fdf81db4448"
      },
      "source": [
        "print(\"Dprodabs Original :\", DprodabsOriginal)\n",
        "print(\"Dprodabs Permutado:\", DprodabsPermutado)\n",
        "\n",
        "if (DprodabsOriginal > 0 and DprodabsPermutado > 0):\n",
        "    print(\"As sentenças do documento original e as sentenças do documento permutado estão relacionadas!\")\n",
        "else:\n",
        "  if (DprodabsOriginal < 0 and DprodabsPermutado > 0) or (DprodabsOriginal > 0 and DprodabsPermutado < 0):\n",
        "    print(\"As sentenças do documento original e as sentenças do documento permutado não estão relacionadas!\")\n",
        "  else:  \n",
        "    print(\"As sentenças do documento original e as sentenças do documento permutado não possuem relação!\")          "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dprodabs Original : 932.7980143229166\n",
            "Dprodabs Permutado: 942.978515625\n",
            "As sentenças do documento original e as sentenças do documento permutado estão relacionadas!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV0r5GHc9rW9"
      },
      "source": [
        "### Produto Escalar entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYaImuYQ9rW9"
      },
      "source": [
        "#### Calcula a média aritmética do produto escalar entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento orginal. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Eo4ruRb9rW9",
        "outputId": "5941121c-ce2b-490d-80b4-8ba4cc872954"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSprode = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "   # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "\n",
        "    # Produto escalar entre os embeddings de Si e Sj, pois Si e Sj possui uma única dimensão\n",
        "    # https://pytorch.org/docs/master/generated/torch.matmul.html#torch.matmul\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sprode = torch.matmul(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real \n",
        "    #print(\"Sprode=\", Sprode)\n",
        "        \n",
        "    somaSprode = somaSprode + Sprode\n",
        "\n",
        "DprodeOriginal = float(somaSprode)/float(n-1)\n",
        "print(\"Dprode Original:\", DprodeOriginal)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dprode Original: 1986.7194010416667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSmdeZJ-9rW9"
      },
      "source": [
        "#### Calcula a média aritmética do produto escalar entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEPnlLrc9rW9",
        "outputId": "a3131a93-8a89-4835-e556-a2c4ed950e2b"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSprode = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Produto escalar entre os embeddings de Si e Sj, pois Si e Sj possui uma única dimensão\n",
        "    # https://pytorch.org/docs/master/generated/torch.matmul.html#torch.matmul\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sprode = torch.matmul(maiorEmbeddingSi, maiorEmbeddingSj)    \n",
        "    # Saída: Um número real \n",
        "    #print(\"Sprode=\", Sprode)\n",
        "    \n",
        "    somaSprode = somaSprode + Sprode\n",
        "\n",
        "DprodePermutado = float(somaSprode)/float(np-1)\n",
        "print(\"Dprode permutado:\", DprodePermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dprode permutado: 2025.1311848958333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWUaJGRk9rW-"
      },
      "source": [
        "#### Compara as médias do produto escalar dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Dprod Original : 230.61181640625\n",
        "Dprod Permutado: 227.75482177734375\n",
        "Documento original tem maior similaridade com o produto entre as sentenças!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1G5Up0y9rW-",
        "outputId": "f608bf34-0f3d-4f71-da60-6b05fe5c267d"
      },
      "source": [
        "print(\"Dprode Original :\", DprodeOriginal)\n",
        "print(\"Dprode Permutado:\", DprodePermutado)\n",
        "\n",
        "if (DprodeOriginal < DprodePermutado):\n",
        "    print(\"Documento original tem maior similaridade com o produto entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade com o produto entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dprode Original : 1986.7194010416667\n",
            "Dprode Permutado: 2025.1311848958333\n",
            "Documento original tem maior similaridade com o produto entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3w5pjjI9rW-"
      },
      "source": [
        "### Produto Escalar Absoluto entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9Nt2DOl9rW-"
      },
      "source": [
        "#### Calcula a média aritmética do produto escalar absoluto entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1Nx0TrW9rW-",
        "outputId": "a2f8dd22-ce5f-4177-a2f0-707633480f31"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSprodeabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "     # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Produto escalar entre os embeddings de Si e Sj, pois Si e Sj possui uma única dimensão\n",
        "    # https://pytorch.org/docs/master/generated/torch.matmul.html#torch.matmul\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sprodeabs = abs(torch.matmul(maiorEmbeddingSi, maiorEmbeddingSj))\n",
        "    # Saída: Um número real \n",
        "    #print(\"Sprodeabs=\", Sprodeabs)\n",
        "\n",
        "    somaSprodeabs = somaSprodeabs + Sprodeabs\n",
        "\n",
        "DprodeabsOriginal = float(somaSprodeabs)/float(n-1)\n",
        "print(\"Dprodabs Original:\", DprodeabsOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dprodabs Original: 1986.7194010416667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfDHt9Oj9rW-"
      },
      "source": [
        "#### Calcula a média aritmética do produto escalar absoluto entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDxSCqX-9rW-",
        "outputId": "b8bc5065-a2b2-412f-c695-cfc7c74e609c"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSprodeabs = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "    \n",
        "     # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "    \n",
        "    # Produto escalar entre os embeddings de Si e Sj, pois Si e Sj possui uma única dimensão\n",
        "    # https://pytorch.org/docs/master/generated/torch.matmul.html#torch.matmul\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sprodeabs = abs(torch.matmul(maiorEmbeddingSi, maiorEmbeddingSj))\n",
        "    # Saída: Um número real \n",
        "    #print(\"Sprodeabs=\", Sprodeabs)\n",
        "    \n",
        "    somaSprodeabs = somaSprodeabs + Sprodeabs\n",
        "\n",
        "DprodeabsPermutado = float(somaSprodeabs)/float(np-1)\n",
        "print(\"Dprodabs permutado:\", DprodeabsPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dprodabs permutado: 2025.1311848958333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp2YU1lP9rW_"
      },
      "source": [
        "#### Compara as médias do produto escalar absoluto dos embeddings das sentenças do documento original e permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nkq0MBmT9rW_",
        "outputId": "f4f91f50-6b28-4682-9d23-8598ab40cd75"
      },
      "source": [
        "print(\"Dprodabs Original :\", DprodeabsOriginal)\n",
        "print(\"Dprodabs Permutado:\", DprodeabsPermutado)\n",
        "\n",
        "if (DprodeabsOriginal < DprodeabsPermutado):\n",
        "    print(\"Documento original tem maior similaridade com o produto absoluto entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade com o produto absoluto entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dprodabs Original : 1986.7194010416667\n",
            "Dprodabs Permutado: 2025.1311848958333\n",
            "Documento original tem maior similaridade com o produto absoluto entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aqfpli79rW_"
      },
      "source": [
        "### Média entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zOlTYv09rW_"
      },
      "source": [
        "#### Calcula a média aritmética entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsd1K6iI9rW_",
        "outputId": "0326a5af-39cc-44d9-c5cb-124f22812d94"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSavg = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Média entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Savg = (maiorEmbeddingSi.add(maiorEmbeddingSj))/2.0  \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"Savg=\", Savg.shape)  \n",
        "\n",
        "    # Calcula a média dos embeddings do cálculo para realizar a soma\n",
        "    somaSavg = somaSavg + torch.mean(Savg)\n",
        "\n",
        "DavgOriginal = float(somaSavg)/float(n-1)\n",
        "print(\"Davg Original:\", DavgOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Davg Original: 0.5656667153040568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tpjp1Ic9rW_"
      },
      "source": [
        "#### Calcula a média aritmética entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS-rWYdc9rW_",
        "outputId": "67131da7-a2be-4295-938c-3631ebd729e0"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSavg = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Média entre os embeddings de Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>) \n",
        "    Savg = (maiorEmbeddingSi.add(maiorEmbeddingSj))/2.0  \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"Savg=\", Savg.shape)  \n",
        "\n",
        "    # Calcula a média dos embeddings do cálculo para realizar a soma\n",
        "    somaSavg = somaSavg + torch.mean(Savg)\n",
        "\n",
        "DavgPermutado = float(somaSavg)/float(np-1)\n",
        "print(\"Davg permutado:\", DavgPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Davg permutado: 0.5695276260375977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4DTlDK59rXA"
      },
      "source": [
        "#### Compara as médias dos embeddings das sentenças do documento original e permutado\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa2OIWIz9rXA",
        "outputId": "18a39c4a-93df-4da9-b962-81150bdc2b12"
      },
      "source": [
        "print(\"Davg Original :\", DavgOriginal)\n",
        "print(\"Davg Permutado:\", DavgPermutado)\n",
        "\n",
        "if (DavgOriginal < DavgPermutado):\n",
        "    print(\"Documento original tem maior similaridade com a média entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade com a mpedia entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Davg Original : 0.5656667153040568\n",
            "Davg Permutado: 0.5695276260375977\n",
            "Documento original tem maior similaridade com a média entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJLbGeGl9rXA"
      },
      "source": [
        "### Similaridade de cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txsn241y9rXA"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCoseno(sentenca1, sentenca2):\n",
        "  similaridade = 1 - cosine(sentenca1, sentenca2)\n",
        "  return similaridade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5MH4W1E9rXA"
      },
      "source": [
        "#### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q30qFwSb9rXA",
        "outputId": "327c9f80-a0df-47e3-a8ec-e702e7099a1e"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCoseno(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "DcosOriginal = float(somaScos)/float(n-1)\n",
        "print(\"Dcos Original:\", DcosOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dcos Original: 0.846483051776886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZLT9K6r9rXA"
      },
      "source": [
        "#### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF5f2DSb9rXA",
        "outputId": "fff41900-bc13-426f-96de-1aaa877c9ba0"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCoseno(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "DcosPermutado = float(somaScos)/float(np-1)\n",
        "print(\"Dcos Original:\", DcosPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Dcos Original: 0.8591363628705343\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIKvfbWo9rXB"
      },
      "source": [
        "#### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 1.\n",
        "- Documentos com sentenças diferenntes resulta uma medida menor que 1.\n",
        "- Documento com sentenças muito diferentes apresentam valores menores que 1.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de similaridade.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCITgUD59rXB",
        "outputId": "b62aa4cf-b7be-4ec6-ddbb-be3242f713e5"
      },
      "source": [
        "print(\"Dcos Original :\", DcosOriginal)\n",
        "print(\"Dcos Permutado:\", DcosPermutado)\n",
        "\n",
        "if (DcosOriginal > DcosPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dcos Original : 0.846483051776886\n",
            "Dcos Permutado: 0.8591363628705343\n",
            "Documento Permutado tem menor similaridade de cosseno entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poXedMfZ9rXB"
      },
      "source": [
        "### Distância euclidiana entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância L2 ou norma L2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgMLv7Xr9rXB"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distanciaEuclidiana(sentenca1, sentenca2):\n",
        "  distancia = euclidean(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFcLYcv_9rXB"
      },
      "source": [
        "#### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLPkoJU49rXB",
        "outputId": "ef2a765e-8e4e-464f-b3fe-8b47e86306a8"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "DeucOriginal = float(somaSeuc)/float(n-1)\n",
        "print(\"Deuc Original:\", DeucOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Deuc Original: 26.750072479248047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYF1Bm-g9rXB"
      },
      "source": [
        "#### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1rSfTES9rXB",
        "outputId": "25ecb449-8809-49f1-c6e9-f9b25ba77630"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "DeucPermutado = float(somaSeuc)/float(np-1)\n",
        "print(\"Deuc Original:\", DeucPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Deuc Original: 25.777802149454754\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_kqtJxd9rXC"
      },
      "source": [
        "#### Compara as médias da distância euclidiana dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC4IlKjS9rXC",
        "outputId": "fd7757b2-cf94-4927-e3d3-6e369e221208"
      },
      "source": [
        "print(\"Deuc Original :\", DeucOriginal)\n",
        "print(\"Deuc Permutado:\", DeucPermutado)\n",
        "\n",
        "if (DeucOriginal > DeucPermutado):\n",
        "    print(\"Documento original tem maior distância euclidiana entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor distância euclidiana entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deuc Original : 26.750072479248047\n",
            "Deuc Permutado: 25.777802149454754\n",
            "Documento original tem maior distância euclidiana entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ron62mX39rXC"
      },
      "source": [
        "### Distância Manhattan entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU8WUSNJ9rXC"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def distanciaEManhattan(sentenca1, sentenca2):\n",
        "  distancia = cityblock(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp7lYFkQ9rXC"
      },
      "source": [
        "#### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZsETQjN9rXC",
        "outputId": "196b86bf-92a6-4e8b-a208-53a6ccba7cfe"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj)) \n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaEManhattan(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "DmanOriginal = float(somaSman)/float(n-1)\n",
        "print(\"Dman Original:\", DmanOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dman Original: 1132.6749877929688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hraWMnO49rXC"
      },
      "source": [
        "#### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1lpFQpW9rXC",
        "outputId": "008d47a2-b39c-48a9-9b52-22ce75db0838"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSi, tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, texto documento,  SentencaSj, tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(embeddingDocumentoOriginal, stringDocumentoPermutado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj)) \n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaEManhattan(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "DmanPermutado = float(somaSman)/float(np-1)\n",
        "print(\"Deuc Original:\", DmanPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Deuc Original: 1082.0957641601562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW9oqevL9rXD"
      },
      "source": [
        "#### Compara as médias da distância de manhattan dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E71-ic6l9rXD",
        "outputId": "890bb7aa-023e-47bb-d528-fb389c5348b0"
      },
      "source": [
        "print(\"Dman Original :\", DmanOriginal)\n",
        "print(\"Dman Permutado:\", DmanPermutado)\n",
        "\n",
        "if (DeucOriginal > DeucPermutado):\n",
        "    print(\"Documento original tem maior distância de manhattan entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor distância de manhattan entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dman Original : 1132.6749877929688\n",
            "Dman Permutado: 1082.0957641601562\n",
            "Documento original tem maior distância de manhattan entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqVSa_XBL35H"
      },
      "source": [
        "### Resumo\n",
        "\n",
        "Resultado das medidas utilizando as quatro últimas camadas do BERT.\n",
        "\n",
        "Base(MEAN):\n",
        "- Dsub       :   0.00028153          -0.00050018\n",
        "- Dubabs     :   0.26571635          0.24592040\n",
        "- Dprod      :   932.79801432          942.97851562\n",
        "- Dprodabs   :   932.79801432          942.97851562\n",
        "- Dprode     :   932.79809570          942.97859701\n",
        "- Dprodeabs  :   932.79809570          942.97859701\n",
        "- Davg       :   -0.02508242          -0.02519175\n",
        "- Dcos       :   0.83016231          0.85469584\n",
        "- Deuc       :   19.28004646          17.82995224\n",
        "- Dman       :   816.28061930          755.46746826\n",
        "\n",
        "Base(MAX):\n",
        "- Dsub       :   -0.02318370          0.03090549\n",
        "- Dubabs     :   0.36870933          0.35224470\n",
        "- Dprod      :   932.79801432          942.97851562\n",
        "- Dprodabs   :   932.79801432          942.97851562\n",
        "- Dprode     :   1986.71940104          2025.13118490\n",
        "- Dprodeabs  :   1986.71940104          2025.13118490\n",
        "- Davg       :   0.56566672          0.56952763\n",
        "- Dcos       :   0.84648305          0.85913636\n",
        "- Deuc       :   26.75007248          25.77780215\n",
        "- Dman       :   1132.67498779          1082.09576416\n",
        "\n",
        "----------------------\n",
        "Resultado das medidas utilizando a última camada do BERT.\n",
        "\n",
        "Base(MEAN):\n",
        "- Dsub       :   0.00074918          -0.00064076\n",
        "- Dubabs     :   0.17778097          0.17206367\n",
        "- Dprod      :   43.70739746          43.55047607\n",
        "- Dprodabs   :   43.70739746          43.55047607\n",
        "- Dprode     :   43.70739746          43.55047607\n",
        "- Dprodeabs  :   43.70739746          43.55047607\n",
        "- Davg       :   -0.00276791          -0.00271371\n",
        "- Dcos       :   0.67999101          0.69653825\n",
        "- Deuc       :   6.34085274          6.15486606\n",
        "- Dman       :   136.53579712          132.14489237\n",
        "\n",
        "Base(MAX):\n",
        "- Dsub       :   -0.02765538          0.02360132\n",
        "- Dubabs     :   0.26021336          0.25770577\n",
        "- Dprod      :   43.70739746          43.55047607\n",
        "- Dprodabs   :   43.70739746          43.55047607\n",
        "- Dprode     :   200.03959147          199.48846436\n",
        "- Dprodeabs  :   200.03959147          199.48846436\n",
        "- Davg       :   0.43032602          0.42829903\n",
        "- Dcos       :   0.81970002          0.82297109\n",
        "- Deuc       :   9.42460442          9.29481570\n",
        "- Dman       :   199.84384664          197.91803996"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yac6Etu7L35I",
        "outputId": "9eb367ac-39ed-427e-a75c-5f17b7cfd558"
      },
      "source": [
        "print(\"Resultado das medidas utilizando a última camada do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print('Dsub       :   {:.8f}          {:.8f}'.format(DsubOriginal,DsubPermutado))\n",
        "print('Dubabs     :   {:.8f}          {:.8f}'.format(DsubabsOriginal,DsubabsPermutado))\n",
        "print('Dprod      :   {:.8f}          {:.8f}'.format(DprodOriginal,DprodPermutado))\n",
        "print('Dprodabs   :   {:.8f}          {:.8f}'.format(DprodabsOriginal,DprodabsPermutado))\n",
        "print('Dprode     :   {:.8f}          {:.8f}'.format(DprodeOriginal,DprodePermutado))\n",
        "print('Dprodeabs  :   {:.8f}          {:.8f}'.format(DprodeabsOriginal,DprodeabsPermutado))\n",
        "print('Davg       :   {:.8f}          {:.8f}'.format(DavgOriginal,DavgPermutado))\n",
        "print('Dcos       :   {:.8f}          {:.8f}'.format(DcosOriginal,DcosPermutado))\n",
        "print('Deuc       :   {:.8f}          {:.8f}'.format(DeucOriginal,DeucPermutado))\n",
        "print('Dman       :   {:.8f}          {:.8f}'.format(DmanOriginal,DmanPermutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resultado das medidas utilizando a última camada do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Dsub       :   -0.02318370          0.03090549\n",
            "Dubabs     :   0.36870933          0.35224470\n",
            "Dprod      :   932.79801432          942.97851562\n",
            "Dprodabs   :   932.79801432          942.97851562\n",
            "Dprode     :   1986.71940104          2025.13118490\n",
            "Dprodeabs  :   1986.71940104          2025.13118490\n",
            "Davg       :   0.56566672          0.56952763\n",
            "Dcos       :   0.84648305          0.85913636\n",
            "Deuc       :   26.75007248          25.77780215\n",
            "Dman       :   1132.67498779          1082.09576416\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}