{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExemplosWordEmbeddingContextualBERT_pt_br_sentenca.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bda2281604e6463fbe119bee63c050f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96722210d376441c9602f08b6ebd6082",
              "IPY_MODEL_8817f969a93542b3ab283283097f9a67",
              "IPY_MODEL_009fe7b2bfc64d5faccf81bb4d03ec54"
            ],
            "layout": "IPY_MODEL_5d76cd69079047f09423792fe6de26b6"
          }
        },
        "96722210d376441c9602f08b6ebd6082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_115e94047d314d8c87f7a7c94a0ffb86",
            "placeholder": "​",
            "style": "IPY_MODEL_83ee1b3d7e164a78a2c5a3ea99e8e45b",
            "value": "Downloading: 100%"
          }
        },
        "8817f969a93542b3ab283283097f9a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e30204f7f2c425fa3413404c00ec5a3",
            "max": 209528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35864dc58419441283a785091941e2b1",
            "value": 209528
          }
        },
        "009fe7b2bfc64d5faccf81bb4d03ec54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4657b381ad0c4a01b7443762e1e1a3e3",
            "placeholder": "​",
            "style": "IPY_MODEL_e9773b7ebe0843699cd55ddea89c4550",
            "value": " 210k/210k [00:00&lt;00:00, 1.48MB/s]"
          }
        },
        "5d76cd69079047f09423792fe6de26b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "115e94047d314d8c87f7a7c94a0ffb86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83ee1b3d7e164a78a2c5a3ea99e8e45b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e30204f7f2c425fa3413404c00ec5a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35864dc58419441283a785091941e2b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4657b381ad0c4a01b7443762e1e1a3e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9773b7ebe0843699cd55ddea89c4550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "075904972429442ba696edc066937595": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b5eb14fa68a4c588537ff24c49b17b8",
              "IPY_MODEL_53e9064e6cf9411691a7ffb90da587d8",
              "IPY_MODEL_97e7488ab4d847b99e900db07dc3b536"
            ],
            "layout": "IPY_MODEL_81b147c1866a408a8d2ec09a697c4c76"
          }
        },
        "1b5eb14fa68a4c588537ff24c49b17b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48c23043de004e33b58f84af877971e7",
            "placeholder": "​",
            "style": "IPY_MODEL_80172e6e3d984e148f11874370b3b5dc",
            "value": "Downloading: 100%"
          }
        },
        "53e9064e6cf9411691a7ffb90da587d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9554271b938496795b6c405f53b2bfb",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e711cded8ab4facafec8a708a8b0c6a",
            "value": 2
          }
        },
        "97e7488ab4d847b99e900db07dc3b536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82bf51769809487a848c74c764f5dd7d",
            "placeholder": "​",
            "style": "IPY_MODEL_4002c8498ff148d9802a7e591b4ca3a5",
            "value": " 2.00/2.00 [00:00&lt;00:00, 29.2B/s]"
          }
        },
        "81b147c1866a408a8d2ec09a697c4c76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48c23043de004e33b58f84af877971e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80172e6e3d984e148f11874370b3b5dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9554271b938496795b6c405f53b2bfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e711cded8ab4facafec8a708a8b0c6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82bf51769809487a848c74c764f5dd7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4002c8498ff148d9802a7e591b4ca3a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5197ade8f4e24efaabbd2686b3852591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f88f8cf95c5408c8f14645428afb6ac",
              "IPY_MODEL_528250300cd84b29a263b641eb826522",
              "IPY_MODEL_21d46b37a039404d9deefbe9d2f814bc"
            ],
            "layout": "IPY_MODEL_bb7414ce48ae45f99bab63b3b05eea03"
          }
        },
        "2f88f8cf95c5408c8f14645428afb6ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19e152645c444b67ac6b310659e83572",
            "placeholder": "​",
            "style": "IPY_MODEL_847a847872c9445596b9d58a630f7fe4",
            "value": "Downloading: 100%"
          }
        },
        "528250300cd84b29a263b641eb826522": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a305a291b22437d8bc9ec84a224e7f0",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de9bf5ec621e4e31a01c10407702575c",
            "value": 112
          }
        },
        "21d46b37a039404d9deefbe9d2f814bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed62361b45244e8c93db79d327a968ad",
            "placeholder": "​",
            "style": "IPY_MODEL_dc977178191f4688a68a3a80fc1664a9",
            "value": " 112/112 [00:00&lt;00:00, 910B/s]"
          }
        },
        "bb7414ce48ae45f99bab63b3b05eea03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19e152645c444b67ac6b310659e83572": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "847a847872c9445596b9d58a630f7fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a305a291b22437d8bc9ec84a224e7f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de9bf5ec621e4e31a01c10407702575c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed62361b45244e8c93db79d327a968ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc977178191f4688a68a3a80fc1664a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2b6156ceead4cf5aa114f6c728fc34d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9091ffde3fd34212800e72156f7e8c42",
              "IPY_MODEL_5a143172235a4580a8d44457d95a2f8a",
              "IPY_MODEL_898a0ab743d44657bb1a3a36bdbc8a13"
            ],
            "layout": "IPY_MODEL_c21e98bce6614ba68b1007fe2b8efea3"
          }
        },
        "9091ffde3fd34212800e72156f7e8c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3637f8380b754c24821babe6519bf803",
            "placeholder": "​",
            "style": "IPY_MODEL_f9fc3bda2e8a4ff1a60f562574f3a68a",
            "value": "Downloading: 100%"
          }
        },
        "5a143172235a4580a8d44457d95a2f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60d1ce817ce540cc984bd7557ebc3710",
            "max": 155,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb84a9aa696c4998b1805c8769cee5b5",
            "value": 155
          }
        },
        "898a0ab743d44657bb1a3a36bdbc8a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63424cff679b45bebc901f6cf6c70333",
            "placeholder": "​",
            "style": "IPY_MODEL_16f965444caf458a905341f931fb3773",
            "value": " 155/155 [00:00&lt;00:00, 1.59kB/s]"
          }
        },
        "c21e98bce6614ba68b1007fe2b8efea3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3637f8380b754c24821babe6519bf803": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9fc3bda2e8a4ff1a60f562574f3a68a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60d1ce817ce540cc984bd7557ebc3710": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb84a9aa696c4998b1805c8769cee5b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63424cff679b45bebc901f6cf6c70333": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16f965444caf458a905341f931fb3773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c87de27311847098f7587174b3295a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2a66e65c611407c924127f4b9882c50",
              "IPY_MODEL_d16ec84b5b5542c390501622d0adbd48",
              "IPY_MODEL_c27e7653d7b74cf09a6b3a526e7b8b12"
            ],
            "layout": "IPY_MODEL_aabb859bd1c14bb7b851bf1419f3affe"
          }
        },
        "b2a66e65c611407c924127f4b9882c50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_686c900f43844af6ab00052717a8a874",
            "placeholder": "​",
            "style": "IPY_MODEL_6168e842211b499fb22bc941db6f62f2",
            "value": "Downloading: 100%"
          }
        },
        "d16ec84b5b5542c390501622d0adbd48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d11451168d344fc58eb453b657311b2b",
            "max": 648,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8875ae7dfbf4e279a296b37f0af7a0b",
            "value": 648
          }
        },
        "c27e7653d7b74cf09a6b3a526e7b8b12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_919854e8d54c47948ac52db4ca5f7fa7",
            "placeholder": "​",
            "style": "IPY_MODEL_9914f02641874582902154f218324865",
            "value": " 648/648 [00:00&lt;00:00, 17.4kB/s]"
          }
        },
        "aabb859bd1c14bb7b851bf1419f3affe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "686c900f43844af6ab00052717a8a874": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6168e842211b499fb22bc941db6f62f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d11451168d344fc58eb453b657311b2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8875ae7dfbf4e279a296b37f0af7a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "919854e8d54c47948ac52db4ca5f7fa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9914f02641874582902154f218324865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02e1ade7333d490aa1eb4a36a19cbec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fc1506a384a4d4abf58150e2f91954d",
              "IPY_MODEL_fca1d7b1d8e643868a6fffbde54f3ed5",
              "IPY_MODEL_88879927300b47eface5be4069a50150"
            ],
            "layout": "IPY_MODEL_66e436f4e06f48cab11c691bcc463ee2"
          }
        },
        "0fc1506a384a4d4abf58150e2f91954d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b54a91fa4934e87a785f4c9a9670afe",
            "placeholder": "​",
            "style": "IPY_MODEL_72d2e412b7624636a80c1038971caeb8",
            "value": "Downloading: 100%"
          }
        },
        "fca1d7b1d8e643868a6fffbde54f3ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e71f7da0c0d14f1a80960c977af135c1",
            "max": 1342014951,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e854cb384da541d786cb815dda9de4c0",
            "value": 1342014951
          }
        },
        "88879927300b47eface5be4069a50150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d077133fd4542bd8676afd231825e07",
            "placeholder": "​",
            "style": "IPY_MODEL_ddda468942cf449d8ba80638f29b14cf",
            "value": " 1.34G/1.34G [00:51&lt;00:00, 29.5MB/s]"
          }
        },
        "66e436f4e06f48cab11c691bcc463ee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b54a91fa4934e87a785f4c9a9670afe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72d2e412b7624636a80c1038971caeb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e71f7da0c0d14f1a80960c977af135c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e854cb384da541d786cb815dda9de4c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d077133fd4542bd8676afd231825e07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddda468942cf449d8ba80638f29b14cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osmarbraz/exemplos_BERT/blob/main/ExemplosWordEmbeddingContextualBERT_pt_br_sentenca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78HE8FLsKN9Q"
      },
      "source": [
        "#Exemplo de comparação de sentenças e o contexto baseado nas sentenças(pt-br) usando BERT Transformers by HuggingFace\n",
        "\n",
        "# **A execução pode ser feita através do menu Ambiente de Execução opção Executar tudo.**\n",
        "\n",
        "Exemplos de **Comparação de Sentenças** usando **BERT** em documentos originais e permutados utilizando suas sentenças. No final do notebook estão os exemplos com os documentos:\n",
        "\n",
        "*   documento original e permutado\n",
        "\n",
        "**Link biblioteca Huggingface:**\n",
        "https://github.com/huggingface/transformers\n",
        "\n",
        "\n",
        "**Artigo original BERT Jacob Devlin:**\n",
        "https://arxiv.org/pdf/1506.06724.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyxb5Px3p1-e"
      },
      "source": [
        "# 0 - Preparação do ambiente\n",
        "Preparação do ambiente para execução do exemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAPVtRXQqDim"
      },
      "source": [
        "###Tratamento de logs\n",
        "\n",
        "Método para tratamento dos logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcopxbGZqDip"
      },
      "source": [
        "# Biblioteca de logging\n",
        "import logging\n",
        "\n",
        "# Formatando a mensagem de logging\n",
        "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GjYtXcMnSAe"
      },
      "source": [
        "## Identificando o ambiente Colab\n",
        "\n",
        "Cria uma variável para identificar que o notebook está sendo executado no Google Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMiH0E3OnRa1"
      },
      "source": [
        "# Se estiver executando no Google Colaboratory\n",
        "import sys\n",
        "\n",
        "# Retorna true ou false se estiver no Google Colaboratory\n",
        "IN_COLAB = \"google.colab\" in sys.modules"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RufkKnojlwzu"
      },
      "source": [
        "## Instalação do spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0LeiOTx0Dlk"
      },
      "source": [
        "https://spacy.io/\n",
        "\n",
        "Modelos do spaCy para português:\n",
        "https://spacy.io/models/pt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Fvx0TVRQUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63a2e9bc-f947-4f90-93f3-6fc8b8f6abf5"
      },
      "source": [
        "# Instala o spacy\n",
        "!pip install -U spacy==2.3.5"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==2.3.5\n",
            "  Downloading spacy-2.3.5-cp37-cp37m-manylinux2014_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (0.9.1)\n",
            "Collecting thinc<7.5.0,>=7.4.1\n",
            "  Downloading thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 38.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (57.4.0)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (4.64.0)\n",
            "Collecting srsly<1.1.0,>=1.0.2\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 44.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.21.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (0.7.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (2.0.6)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (3.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (1.24.3)\n",
            "Installing collected packages: srsly, plac, catalogue, thinc, spacy\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.3\n",
            "    Uninstalling srsly-2.4.3:\n",
            "      Successfully uninstalled srsly-2.4.3\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.7\n",
            "    Uninstalling catalogue-2.0.7:\n",
            "      Successfully uninstalled catalogue-2.0.7\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.0.17\n",
            "    Uninstalling thinc-8.0.17:\n",
            "      Successfully uninstalled thinc-8.0.17\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.3.1\n",
            "    Uninstalling spacy-3.3.1:\n",
            "      Successfully uninstalled spacy-3.3.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.3.0 requires spacy<3.4.0,>=3.3.0.dev0, but you have spacy 2.3.5 which is incompatible.\u001b[0m\n",
            "Successfully installed catalogue-1.0.0 plac-1.1.3 spacy-2.3.5 srsly-1.0.5 thinc-7.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35GwcgkOlWi3"
      },
      "source": [
        "Realiza o download e carrega os modelos necessários a biblioteca\n",
        "\n",
        "https://spacy.io/models/pt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4LqE5kTwDYm"
      },
      "source": [
        "# Definição do nome do arquivo do modelo\n",
        "#ARQUIVOMODELO = \"pt_core_news_sm\"\n",
        "#ARQUIVOMODELO = \"pt_core_news_md\"\n",
        "ARQUIVOMODELO = \"pt_core_news_lg\"\n",
        "\n",
        "# Definição da versão da spaCy\n",
        "#VERSAOSPACY = \"-3.0.0a0\"\n",
        "VERSAOSPACY = \"-2.3.0\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ2KB3UCp-ws"
      },
      "source": [
        "#Baixa automaticamente o arquivo do modelo.\n",
        "#!python -m spacy download {ARQUIVOMODELO}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASk5iFeUp9LE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ecaf82-bee4-4205-a5b4-f0d3a3a00092"
      },
      "source": [
        "# Realiza o download do arquivo do modelo para o diretório corrente\n",
        "!wget https://github.com/explosion/spacy-models/releases/download/{ARQUIVOMODELO}{VERSAOSPACY}/{ARQUIVOMODELO}{VERSAOSPACY}.tar.gz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-02 12:28:55--  https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-2.3.0/pt_core_news_lg-2.3.0.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/84940268/a899e480-ab07-11ea-831b-b5aa9cc04510?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220702T122855Z&X-Amz-Expires=300&X-Amz-Signature=19a49a0e7b709a2ae5cfde3786f4c9cccc5771b5cdf4ce16c261c9050c53fc4e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Dpt_core_news_lg-2.3.0.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-07-02 12:28:55--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/84940268/a899e480-ab07-11ea-831b-b5aa9cc04510?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220702T122855Z&X-Amz-Expires=300&X-Amz-Signature=19a49a0e7b709a2ae5cfde3786f4c9cccc5771b5cdf4ce16c261c9050c53fc4e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Dpt_core_news_lg-2.3.0.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 576599832 (550M) [application/octet-stream]\n",
            "Saving to: ‘pt_core_news_lg-2.3.0.tar.gz’\n",
            "\n",
            "pt_core_news_lg-2.3 100%[===================>] 549.89M  42.6MB/s    in 13s     \n",
            "\n",
            "2022-07-02 12:29:08 (42.8 MB/s) - ‘pt_core_news_lg-2.3.0.tar.gz’ saved [576599832/576599832]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu_LkF7Nfm8_"
      },
      "source": [
        "Descompacta o arquivo do modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9fCQQJGeVEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e198ee-4478-465f-bc14-e5b816acd564"
      },
      "source": [
        "# Descompacta o arquivo do modelo\n",
        "!tar -xvf  /content/{ARQUIVOMODELO}{VERSAOSPACY}.tar.gz"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pt_core_news_lg-2.3.0/\n",
            "pt_core_news_lg-2.3.0/PKG-INFO\n",
            "pt_core_news_lg-2.3.0/setup.py\n",
            "pt_core_news_lg-2.3.0/setup.cfg\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/dependency_links.txt\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/PKG-INFO\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/SOURCES.txt\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/requires.txt\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/top_level.txt\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/not-zip-safe\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/__init__.py\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/parser/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/parser/cfg\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/parser/moves\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/parser/model\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/ner/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/ner/cfg\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/ner/moves\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/ner/model\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tokenizer\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/lookups.bin\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/vectors\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/key2row\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/lookups_extra.bin\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/strings.json\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/accuracy.json\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tagger/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tagger/cfg\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tagger/tag_map\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tagger/model\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/meta.json\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/meta.json\n",
            "pt_core_news_lg-2.3.0/MANIFEST.in\n",
            "pt_core_news_lg-2.3.0/meta.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovOx-3Wb-JJW"
      },
      "source": [
        "# Coloca a pasta do modelo descompactado em uma pasta de nome mais simples\n",
        "!mv /content/{ARQUIVOMODELO}{VERSAOSPACY}/{ARQUIVOMODELO}/{ARQUIVOMODELO}{VERSAOSPACY} /content/{ARQUIVOMODELO}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STHT2c89qvwK"
      },
      "source": [
        "Carrega o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbELnrpgA4T1"
      },
      "source": [
        "import spacy\n",
        "\n",
        "CAMINHOMODELO = \"/content/\" + ARQUIVOMODELO\n",
        "\n",
        "#nlp = spacy.load(CAMINHOMODELO)\n",
        "# Necessário \"tagger\" para encontrar os substantivos\n",
        "nlp = spacy.load(CAMINHOMODELO, disable=[\"tokenizer\", \"lemmatizer\", \"ner\", \"parser\", \"textcat\", \"custom\"])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFTTdqxKQ1Ay"
      },
      "source": [
        "Recupera os stopwords do spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBInu7ayQ31J"
      },
      "source": [
        "def getStopwords(nlp):\n",
        "    \"\"\"\n",
        "    Recupera as stop words do nlp(Spacy).\n",
        "    \n",
        "    Parâmetros:\n",
        "    `nlp` - Um modelo spaCy carregado.           \n",
        "    \"\"\"\n",
        "    \n",
        "    spacy_stopwords = nlp.Defaults.stop_words\n",
        "\n",
        "    return spacy_stopwords "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_EYNu-_RX7k"
      },
      "source": [
        "Lista dos stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUSaUJEWRbnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "479b0d12-c8b9-4eae-dc75-1f624637fd3d"
      },
      "source": [
        "print(\"Quantidade de stopwords:\", len(getStopwords(nlp)))\n",
        "\n",
        "print(getStopwords(nlp))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de stopwords: 413\n",
            "{'fazes', 'cinco', 'nada', 'pelos', 'eu', 'meus', 'aquilo', 'tão', 'nova', 'final', 'maioria', 'foi', 'do', 'no', 'por', 'após', 'aqui', 'uma', 'aquela', 'oitava', 'tiveram', 'saber', 'ademais', 'seis', 'deve', 'ali', 'era', 'cá', 'segunda', 'sétima', 'estivemos', 'todos', 'próxima', 'só', 'agora', 'tem', 'para', 'oito', 'comprido', 'maiorias', 'sistema', 'dá', 'tentar', 'nossos', 'dão', 'ao', 'vais', 'usar', 'em', 'lhe', 'você', 'vai', 'às', 'baixo', 'quarto', 'fora', 'nove', 'duas', 'isso', 'aquelas', 'vós', 'outra', 'mal', 'nem', 'doze', 'tua', 'sete', 'comprida', 'fui', 'outras', 'naquela', 'três', 'maior', 'qual', 'mas', 'vez', 'dezanove', 'tendes', 'sob', 'quero', 'teve', 'portanto', 'seria', 'isto', 'estão', 'debaixo', 'depois', 'qualquer', 'sois', 'sou', 'os', 'tu', 'lá', 'relação', 'está', 'elas', 'coisa', 'se', 'logo', 'aquele', 'quarta', 'tentaram', 'vezes', 'bom', 'questão', 'deste', 'conhecido', 'as', 'dezoito', 'mil', 'grande', 'uns', 'cento', 'área', 'ambos', 'vossos', 'catorze', 'das', 'apoio', 'máximo', 'daquela', 'novo', 'querem', 'esta', 'cada', 'perto', 'aos', 'quieto', 'faz', 'umas', 'fazer', 'alguns', 'naquele', 'ponto', 'és', 'nunca', 'ele', 'quem', 'quinto', 'puderam', 'sexta', 'zero', 'adeus', 'fará', 'mês', 'partir', 'sétimo', 'pela', 'demais', 'fomos', 'pouco', 'veja', 'certamente', 'ver', 'tenho', 'vários', 'têm', 'grupo', 'falta', 'que', 'onde', 'tentei', 'vosso', 'porém', 'estas', 'forma', 'ou', 'custa', 'cujo', 'desde', 'este', 'caminho', 'daquele', 'breve', 'somente', 'onze', 'quais', 'porquanto', 'num', 'quatro', 'ir', 'segundo', 'estás', 'posição', 'sabe', 'estará', 'dar', 'menos', 'dizer', 'esses', 'como', 'tarde', 'dezassete', 'todo', 'estes', 'ter', 'faço', 'fostes', 'tais', 'mais', 'ela', 'nesse', 'estou', 'até', 'quinta', 'irá', 'vinda', 'antes', 'suas', 'estiveste', 'longe', 'também', 'te', 'nenhuma', 'um', 'minha', 'porque', 'algumas', 'números', 'deverá', 'tive', 'terceira', 'exemplo', 'ainda', 'aí', 'temos', 'fazemos', 'numa', 'desta', 'nosso', 'quinze', 'terceiro', 'muito', 'tipo', 'talvez', 'da', 'sempre', 'dois', 'contra', 'pegar', 'estava', 'fazem', 'obrigado', 'pôde', 'posso', 'boa', 'neste', 'põem', 'outros', 'disso', 'porquê', 'sei', 'de', 'ser', 'esteve', 'sobre', 'vossa', 'próprio', 'sim', 'contudo', 'todas', 'dez', 'vindo', 'com', 'novas', 'vens', 'valor', 'nessa', 'tens', 'pois', 'estiveram', 'à', 'estive', 'esse', 'apoia', 'tente', 'possivelmente', 'vocês', 'iniciar', 'pouca', 'meses', 'vossas', 'tuas', 'direita', 'dentro', 'tiveste', 'dizem', 'embora', 'fim', 'eles', 'não', 'diante', 'quê', 'apenas', 'nos', 'teu', 'minhas', 'certeza', 'vinte', 'pelo', 'seus', 'tal', 'foste', 'diz', 'local', 'parte', 'vem', 'inclusive', 'quieta', 'tanta', 'poder', 'primeira', 'treze', 'estar', 'nossas', 'sem', 'meio', 'for', 'somos', 'oitavo', 'muitos', 'tivestes', 'quanto', 'dessa', 'toda', 'tempo', 'desse', 'atrás', 'número', 'grandes', 'tudo', 'possível', 'teus', 'ambas', 'entre', 'dos', 'nós', 'cima', 'já', 'foram', 'nesta', 'aqueles', 'menor', 'ligado', 'pelas', 'enquanto', 'na', 'favor', 'nas', 'quer', 'estado', 'eventual', 'momento', 'primeiro', 'acerca', 'dezasseis', 'conselho', 'parece', 'último', 'apontar', 'bem', 'próximo', 'corrente', 'inicio', 'podem', 'vos', 'sexto', 'fazeis', 'tanto', 'usa', 'ora', 'então', 'bastante', 'estivestes', 'mesmo', 'meu', 'obrigada', 'através', 'devem', 'nossa', 'geral', 'é', 'além', 'vão', 'pontos', 'essa', 'são', 'conhecida', 'põe', 'assim', 'fazia', 'fez', 'lado', 'me', 'des', 'essas', 'ontem', 'sua', 'vêm', 'quando', 'algo', 'tivemos', 'cuja', 'podia', 'novos', 'nível', 'lugar', 'povo', 'seu', 'cedo', 'pode', 'nuns', 'poderá'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqa-7WXBAw8q"
      },
      "source": [
        "## Instalação do BERT da Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCdqJCtQN52l"
      },
      "source": [
        "Instala a interface pytorch para o BERT by Hugging Face. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RfUN_KolV-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a35f1e6b-2c55-4970-c07d-e84a530d4024"
      },
      "source": [
        "# Instala a última versão da biblioteca\n",
        "#!pip install transformers\n",
        "\n",
        "# Instala uma versão específica da biblioteca\n",
        "!pip install -U transformers==4.5.1"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.5.1\n",
            "  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2022.6.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 39.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.11.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (3.7.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.64.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 46.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.1) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.1) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.1) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=29753ecae5b059430becd45e0c06215b09f4844cc7408ee21debe204b3fb6cd3\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQj2wmKDpkrH"
      },
      "source": [
        "# 2 - Download do arquivo do PyTorch Checkpoint\n",
        "\n",
        "Lista de modelos da comunidade:\n",
        "* https://huggingface.co/models\n",
        "\n",
        "Português(https://github.com/neuralmind-ai/portuguese-bert):  \n",
        "* **\"neuralmind/bert-base-portuguese-cased\"**\n",
        "* **\"neuralmind/bert-large-portuguese-cased\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajrTjZzapkrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0761d7f7-5c4d-41cf-82e4-f97ee335c878"
      },
      "source": [
        "# Importando as bibliotecas\n",
        "import os\n",
        "\n",
        "# Variável para setar o arquivo\n",
        "URL_MODELO = None\n",
        "\n",
        "# Comente uma das urls para carregar modelos de tamanhos diferentes(base/large)\n",
        "# URL_MODELO do arquivo do modelo tensorflow\n",
        "# arquivo menor(base) 1.1 Gbytes\n",
        "#URL_MODELO = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip\"\n",
        "\n",
        "# arquivo grande(large) 3.5 Gbytes\n",
        "#URL_MODELO = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-large-portuguese-cased/bert-large-portuguese-cased_pytorch_checkpoint.zip\"\n",
        "\n",
        "# Se a variável foi setada\n",
        "if URL_MODELO:\n",
        "\n",
        "    # Diretório descompactação\n",
        "    DIRETORIO_MODELO = \"/content/modelo\"\n",
        "\n",
        "    # Recupera o nome do arquivo do modelo da URL_MODELO\n",
        "    arquivo = URL_MODELO.split(\"/\")[-1]\n",
        "\n",
        "    # Nome do arquivo do vocabulário\n",
        "    arquivo_vocab = \"vocab.txt\"\n",
        "\n",
        "    # Caminho do arquivo na URL_MODELO\n",
        "    caminho = URL_MODELO[0:len(URL_MODELO)-len(arquivo)]\n",
        "\n",
        "    # Verifica se a pasta de descompactação existe na pasta corrente\n",
        "    if os.path.exists(DIRETORIO_MODELO):\n",
        "      print(\"Apagando diretório existente do modelo!\")\n",
        "      # Apaga a pasta e os arquivos existentes\n",
        "      !rm -rf $DIRETORIO_MODELO      \n",
        "    \n",
        "    # Baixa o arquivo do modelo\n",
        "    !wget $URL_MODELO\n",
        "    # Descompacta o arquivo na pasta de descompactação\n",
        "    !unzip -o $arquivo -d $DIRETORIO_MODELO\n",
        "\n",
        "    # Baixa o arquivo do vocabulário\n",
        "    # O vocabulário não está no arquivo compactado acima, mesma url mas arquivo diferente\n",
        "    URL_MODELO_VOCAB = caminho + arquivo_vocab\n",
        "    !wget $URL_MODELO_VOCAB\n",
        "    \n",
        "    # Coloca o arquivo do vocabulário no diretório de descompactação\n",
        "    !mv $arquivo_vocab $DIRETORIO_MODELO\n",
        "            \n",
        "    # Move o arquivo para pasta de descompactação\n",
        "    !mv $arquivo $DIRETORIO_MODELO\n",
        "       \n",
        "    print(\"Pasta do \" + DIRETORIO_MODELO + \" pronta!\")\n",
        "    \n",
        "    # Lista a pasta corrente\n",
        "    !ls -la $DIRETORIO_MODELO\n",
        "else:\n",
        "    DIRETORIO_MODELO = None\n",
        "    print(\"Variável URL_MODELO não setada!\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variável URL_MODELO não setada!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcpd9t9PpkrX"
      },
      "source": [
        "# 3 - Carregando o Tokenizador BERT\n",
        "\n",
        "O tokenizador utiliza WordPiece, veja em [artigo original](https://arxiv.org/pdf/1609.08144.pdf).\n",
        "\n",
        "Carregando o tokenizador da pasta \"/content/modelo/\" do diretório padrão se variável `URL_MODELO` setada.\n",
        "\n",
        "**Caso contrário carrega da comunidade**\n",
        "\n",
        "Por default(`do_lower_case=True`) todas as letras são colocadas para minúsculas. Para ignorar a conversão para minúsculo use o parâmetro `do_lower_case=False`. Esta opção também considera as letras acentuadas(ãçéí...), que são necessárias a língua portuguesa.\n",
        "\n",
        "O parâmetro `do_lower_case` interfere na quantidade tokens a ser gerado a partir de um documento. Quando igual a `False` reduz a quantidade de tokens gerados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8cKVs4fpkrY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "bda2281604e6463fbe119bee63c050f3",
            "96722210d376441c9602f08b6ebd6082",
            "8817f969a93542b3ab283283097f9a67",
            "009fe7b2bfc64d5faccf81bb4d03ec54",
            "5d76cd69079047f09423792fe6de26b6",
            "115e94047d314d8c87f7a7c94a0ffb86",
            "83ee1b3d7e164a78a2c5a3ea99e8e45b",
            "8e30204f7f2c425fa3413404c00ec5a3",
            "35864dc58419441283a785091941e2b1",
            "4657b381ad0c4a01b7443762e1e1a3e3",
            "e9773b7ebe0843699cd55ddea89c4550",
            "075904972429442ba696edc066937595",
            "1b5eb14fa68a4c588537ff24c49b17b8",
            "53e9064e6cf9411691a7ffb90da587d8",
            "97e7488ab4d847b99e900db07dc3b536",
            "81b147c1866a408a8d2ec09a697c4c76",
            "48c23043de004e33b58f84af877971e7",
            "80172e6e3d984e148f11874370b3b5dc",
            "d9554271b938496795b6c405f53b2bfb",
            "8e711cded8ab4facafec8a708a8b0c6a",
            "82bf51769809487a848c74c764f5dd7d",
            "4002c8498ff148d9802a7e591b4ca3a5",
            "5197ade8f4e24efaabbd2686b3852591",
            "2f88f8cf95c5408c8f14645428afb6ac",
            "528250300cd84b29a263b641eb826522",
            "21d46b37a039404d9deefbe9d2f814bc",
            "bb7414ce48ae45f99bab63b3b05eea03",
            "19e152645c444b67ac6b310659e83572",
            "847a847872c9445596b9d58a630f7fe4",
            "1a305a291b22437d8bc9ec84a224e7f0",
            "de9bf5ec621e4e31a01c10407702575c",
            "ed62361b45244e8c93db79d327a968ad",
            "dc977178191f4688a68a3a80fc1664a9",
            "c2b6156ceead4cf5aa114f6c728fc34d",
            "9091ffde3fd34212800e72156f7e8c42",
            "5a143172235a4580a8d44457d95a2f8a",
            "898a0ab743d44657bb1a3a36bdbc8a13",
            "c21e98bce6614ba68b1007fe2b8efea3",
            "3637f8380b754c24821babe6519bf803",
            "f9fc3bda2e8a4ff1a60f562574f3a68a",
            "60d1ce817ce540cc984bd7557ebc3710",
            "bb84a9aa696c4998b1805c8769cee5b5",
            "63424cff679b45bebc901f6cf6c70333",
            "16f965444caf458a905341f931fb3773"
          ]
        },
        "outputId": "d926cc22-cbc9-400e-bb46-125305f2ad85"
      },
      "source": [
        "# Importando as bibliotecas do tokenizador\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Se a variável URL_MODELO foi setada\n",
        "if DIRETORIO_MODELO:\n",
        "    # Carregando o Tokenizador\n",
        "    print(\"Carrgando o tokenizador BERT do diretório \" + DIRETORIO_MODELO + \"...\")\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(DIRETORIO_MODELO, \n",
        "                                              do_lower_case=False)    \n",
        "else:\n",
        "    # Carregando o Tokenizador da comunidade\n",
        "    print(\"Carregando o tokenizador da comunidade...\")\n",
        "    \n",
        "    #tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", do_lower_case=False)\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-large-portuguese-cased\", do_lower_case=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando o tokenizador da comunidade...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/210k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bda2281604e6463fbe119bee63c050f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "075904972429442ba696edc066937595"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5197ade8f4e24efaabbd2686b3852591"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/155 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2b6156ceead4cf5aa114f6c728fc34d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m__On2g1a--K"
      },
      "source": [
        "# 4 - Carregando o Modelo BERT(BertModel)\n",
        "\n",
        "Se a variável `URL_MODELO` estiver setada carrega o modelo do diretório `content/modelo`.\n",
        "\n",
        "Caso contrário carrega da comunidade.\n",
        "\n",
        "Carregando o modelo da pasta \"/content/modelo/\" do diretório padrão.\n",
        "\n",
        "A implementação do huggingface pytorch inclui um conjunto de interfaces projetadas para uma variedade de tarefas de PNL. Embora essas interfaces sejam todas construídas sobre um modelo treinado de BERT, cada uma possui diferentes camadas superiores e tipos de saída projetados para acomodar suas tarefas específicas de PNL.\n",
        "\n",
        "A documentação para estas pode ser encontrada em [aqui](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html).\n",
        "\n",
        "Por default o modelo está em modo avaliação ou seja `model.eval()`.\n",
        "\n",
        "-----------------------\n",
        "\n",
        "Durante a avaliação do modelo, este retorna um número de diferentes objetos com base em como é configurado na chamada do método `from_pretrained`. \n",
        "\n",
        "Quando definimos `output_hidden_states = True` na chamada do método `from_pretrained`, retorno do modelo possui no terceiro item os estados ocultos(**hidden_states**) de todas as camadas.  Veja a documentação para mais detalhes: https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "\n",
        "Quando **`output_hidden_states = True`** model retorna:\n",
        "- outputs[0] = last_hidden_state;\n",
        "- outputs[1] = pooler_output; \n",
        "- outputs[2] = hidden_states.\n",
        "\n",
        "Quando **`output_hidden_states = False`** ou não especificado model retorna:\n",
        "- outputs[0] = last_hidden_state;\n",
        "- outputs[1] = pooler_output.\n",
        "\n",
        "\n",
        "**ATENÇÃO**: O parâmetro ´**output_hidden_states = True**´ habilita gerar as camadas ocultas do modelo. Caso contrário somente a última camada é mantida. Este parâmetro otimiza a memória mas não os resultados.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRV6l_I-qg9s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "2c87de27311847098f7587174b3295a2",
            "b2a66e65c611407c924127f4b9882c50",
            "d16ec84b5b5542c390501622d0adbd48",
            "c27e7653d7b74cf09a6b3a526e7b8b12",
            "aabb859bd1c14bb7b851bf1419f3affe",
            "686c900f43844af6ab00052717a8a874",
            "6168e842211b499fb22bc941db6f62f2",
            "d11451168d344fc58eb453b657311b2b",
            "e8875ae7dfbf4e279a296b37f0af7a0b",
            "919854e8d54c47948ac52db4ca5f7fa7",
            "9914f02641874582902154f218324865",
            "02e1ade7333d490aa1eb4a36a19cbec6",
            "0fc1506a384a4d4abf58150e2f91954d",
            "fca1d7b1d8e643868a6fffbde54f3ed5",
            "88879927300b47eface5be4069a50150",
            "66e436f4e06f48cab11c691bcc463ee2",
            "3b54a91fa4934e87a785f4c9a9670afe",
            "72d2e412b7624636a80c1038971caeb8",
            "e71f7da0c0d14f1a80960c977af135c1",
            "e854cb384da541d786cb815dda9de4c0",
            "1d077133fd4542bd8676afd231825e07",
            "ddda468942cf449d8ba80638f29b14cf"
          ]
        },
        "outputId": "4727008a-29bb-4e1e-8ee5-5f98044b31e7"
      },
      "source": [
        "# Importando as bibliotecas do Modelo\n",
        "from transformers import BertModel\n",
        "\n",
        "# Se a variável URL_MODELO1 foi setada\n",
        "if URL_MODELO:\n",
        "    # Carregando o Tokenizador\n",
        "    print(\"Carregando o modelo BERT do diretório \" + DIRETORIO_MODELO + \"...\")\n",
        "\n",
        "    model = BertModel.from_pretrained(DIRETORIO_MODELO, \n",
        "                                      output_attentions = False,\n",
        "                                      output_hidden_states = True)    \n",
        "else:\n",
        "    # Carregando o Tokenizador da comunidade\n",
        "    print(\"Carregando o modelo BERT da comunidade ...\")\n",
        "\n",
        "    model = BertModel.from_pretrained(\"neuralmind/bert-large-portuguese-cased\", \n",
        "                                      output_attentions = False,\n",
        "                                      output_hidden_states = True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando o modelo BERT da comunidade ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/648 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c87de27311847098f7587174b3295a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02e1ade7333d490aa1eb4a36a19cbec6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU3wHzNUmmBP"
      },
      "source": [
        "# 5 - Funções auxiliares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWqMsrb-ew5T"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm98RoojJcqP"
      },
      "source": [
        "# Import das bibliotecas\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xaeX0oTVQ5t"
      },
      "source": [
        "##removeStopWords\n",
        "\n",
        "Remove as stopwords de um documento ou senteça."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIaQ9bzBVQ5t"
      },
      "source": [
        "def removeStopWord(documento, stopwords):\n",
        "    \"\"\"\n",
        "    Remove as stopwords de um documento.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `documento` - Um documento com stopwords.\n",
        "    `stopwords` - Uma lista com as stopwords.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remoção das stop words do documento\n",
        "    documentoSemStopwords = [palavra for palavra in documento.split() if palavra.lower() not in stopwords]\n",
        "\n",
        "    # Concatena o documento sem os stopwords\n",
        "    documentoLimpo = \" \".join(documentoSemStopwords)\n",
        "\n",
        "    # Retorna o documento\n",
        "    return documentoLimpo"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7NAe8ogCf1y"
      },
      "source": [
        "## retornaRelevante\n",
        "\n",
        "Retorna somente os palavras do documento ou sentença do tipo especificado."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g7-8PVNWgNQh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNNfykypChn-"
      },
      "source": [
        "def retornaPalavraRelevante(documento, nlp, classe_palavra_relevante=\"NOUN\"):\n",
        "    \"\"\"\n",
        "    Retorna somente os palavras do documento ou sentença do tipo especificado.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `documento` - Um documento com todas as palavras.\n",
        "    `nlp` - Processador de linguagem natural.\n",
        "    `classe_palavra_relevante` - Classe morfossintática da palavra relevante a ser selecionada.\n",
        "    \n",
        "    Retorno:\n",
        "    `documentoComRelevantesConcatenado` - Documento somente com as palavras relevantes.\n",
        "    \"\"\"\n",
        "  \n",
        "    # Realiza o parsing no documento usando spacy\n",
        "    doc = nlp(documento)\n",
        "\n",
        "    # Retorna a lista das palavras relevantes de um tipo\n",
        "    documentoComRelevantes = [token.text for token in doc if token.pos_ == classe_palavra_relevante]\n",
        "\n",
        "    # Concatena o documento com as palavras relevantes\n",
        "    documentoComRelevantesConcatenado = \" \".join(documentoComRelevantes)\n",
        "\n",
        "    # Retorna o documento\n",
        "    return documentoComRelevantesConcatenado"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s42mgtmSZ8MR"
      },
      "source": [
        "## getEmbeddingsCamadas\n",
        "\n",
        "Funções que recuperam os embeddings das camadas:\n",
        "- Primeira camada;\n",
        "- Penúltima camada;\n",
        "- Ùltima camada;\n",
        "- Soma das 4 últimas camadas;\n",
        "- Concatenação das 4 últimas camadas;\n",
        "- Soma de todas as camadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgo3EBTRZ9-3"
      },
      "source": [
        "def getEmbeddingPrimeiraCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][0]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingPenultimaCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][-2]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingUltimaCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "     \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][-1]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado    \n",
        "\n",
        "def getEmbeddingSoma4UltimasCamadas(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  embeddingCamadas = output[2][-4:]\n",
        "  # Saída: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "\n",
        "  # Usa o método `stack` para criar uma nova dimensão no tensor \n",
        "  # com a concateção dos tensores dos embeddings.        \n",
        "  #Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultadoStack = torch.stack(embeddingCamadas, dim=0)\n",
        "  # Saída: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  \n",
        "  # Realiza a soma dos embeddings de todos os tokens para as camadas\n",
        "  # Entrada: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  resultado = torch.sum(resultadoStack, dim=0)\n",
        "  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingConcat4UltimasCamadas(output):  \n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Cria uma lista com os tensores a serem concatenados\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> x <768 ou 1024>)  \n",
        "  # Lista com os tensores a serem concatenados\n",
        "  listaConcat = []\n",
        "  # Percorre os 4 últimos\n",
        "  for i in [-1,-2,-3,-4]:\n",
        "      # Concatena da lista\n",
        "      listaConcat.append(output[2][i])\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> x <768 ou 1024>)  \n",
        "  \n",
        "  # Realiza a concatenação dos embeddings de todos as camadas\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> x <768 ou 1024>)  \n",
        "  resultado = torch.cat(listaConcat, dim=-1)\n",
        "  # Saída: Entrada: (<1(lote)> x <qtde_tokens> x <3072 ou 4096>)  \n",
        "    \n",
        "  return resultado   \n",
        "\n",
        "def getEmbeddingSomaTodasAsCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "   \n",
        "  # Retorna todas as camadas descontando a primeira(0)\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  embeddingCamadas = output[2][1:]\n",
        "  # Saída: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  # Usa o método `stack` para criar uma nova dimensão no tensor \n",
        "  # com a concateção dos tensores dos embeddings.        \n",
        "  #Entrada: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultadoStack = torch.stack(embeddingCamadas, dim=0)\n",
        "  # Saída: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "    \n",
        "  # Realiza a soma dos embeddings de todos os tokens para as camadas\n",
        "  # Entrada: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  resultado = torch.sum(resultadoStack, dim=0)\n",
        "  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "    \n",
        "  return resultado"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7nx_eZ8hSlr"
      },
      "source": [
        "## getEmbeddingsVisual\n",
        "\n",
        "Função para gerar as coordenadas de plotagem a partir das sentenças de embeddings.\n",
        "\n",
        "Existe uma função para os tipos de camadas utilizadas:\n",
        "- Ùltima camada;\n",
        "- Soma das 4 últimas camadas;\n",
        "- Concatenação das 4 últimas camadas;\n",
        "- Soma de todas as camadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLdbOT8-g43V"
      },
      "source": [
        "def getEmbeddingsVisualUltimaCamada(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingUltimaCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, documento_tokenizado"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAf9lJJ2hZbt"
      },
      "source": [
        "def getEmbeddingsVisualSoma4UltimasCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSoma4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, documento_tokenizado"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XpwSN1ghpnz"
      },
      "source": [
        "def getEmbeddingsVisualConcat4UltimasCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingConcat4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, documento_tokenizado"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3KU1EFrnSPK"
      },
      "source": [
        "def getEmbeddingsVisualSomaTodasAsCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSomaTodasAsCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, documento_tokenizado"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8MjE0utzlZT"
      },
      "source": [
        "## getEmbeddings\n",
        "\n",
        "Função para gerar os embeddings de sentenças.\n",
        "\n",
        "Existe uma função para os tipos de camadas utilizadas:\n",
        "- Ùltima camada;\n",
        "- Soma das 4 últimas camadas;\n",
        "- Concatenação das 4 últimas camadas;\n",
        "- Soma de todas as camadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QcqOuwS067Q"
      },
      "source": [
        "def getEmbeddingsUltimaCamada(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingUltimaCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        " \n",
        "    return token_embeddings, documento_tokenizado"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK1wDGBl067Y"
      },
      "source": [
        "def getEmbeddingsSoma4UltimasCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSoma4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "   \n",
        "    return token_embeddings, documento_tokenizado"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hym19Hxr067Y"
      },
      "source": [
        "def getEmbeddingsConcat4UltimasCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingConcat4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    return token_embeddings, documento_tokenizado"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-PLZiUR067Z"
      },
      "source": [
        "def getEmbeddingsSomaTodasAsCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSomaTodasAsCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    return token_embeddings, documento_tokenizado"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFd1rse11DpZ"
      },
      "source": [
        "## getDocumentoTokenizado \n",
        "Retorna o documento tokenizado"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getDocumentoTokenizado(documento, tokenizador):\n",
        "\n",
        "    \"\"\"\n",
        "    Retorna um documento tokenizado e concatenado com tokens especiais \"[CLS]\" no início e o token \"[SEP]\" no fim para ser submetido ao BERT.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `documento` - Um documento a ser tokenizado para o BERT.\n",
        "    `tokenizador` - Tokenizador BERT.\n",
        "    \n",
        "    Retorno:\n",
        "    `documentoTokenizado` - Documento tokenizado.\n",
        "    \"\"\"\n",
        "\n",
        "    # Adiciona os tokens especiais.\n",
        "    documentoMarcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Documento tokenizado\n",
        "    documentoTokenizado = tokenizador.tokenize(documentoMarcado)\n",
        "\n",
        "    return documentoTokenizado"
      ],
      "metadata": {
        "id": "lHSnquWmuDCm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wvgXwN81RCz"
      },
      "source": [
        "## encontrarIndiceSubLista \n",
        "\n",
        "Retorna os índices de início e fim da sublista na lista"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abS44M4yvFxf"
      },
      "source": [
        "# Localiza os índices de início e fim de uma sublista em uma lista\n",
        "def encontrarIndiceSubLista(lista, sublista):\n",
        "\n",
        "    \"\"\"\n",
        "      Localiza os índices de início e fim de uma sublista em uma lista.\n",
        "    \n",
        "      Parâmetros:\n",
        "      `lista` - Uma lista.\n",
        "      `sublista` - Uma sublista a ser localizada na lista.\n",
        "    \"\"\"    \n",
        "    # https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore%E2%80%93Horspool_algorithm\n",
        "\n",
        "    # Recupera o tamanho da lista \n",
        "    h = len(lista)\n",
        "    # Recupera o tamanho da sublista\n",
        "    n = len(sublista)    \n",
        "    skip = {sublista[i]: n - i - 1 for i in range(n - 1)}\n",
        "    i = n - 1\n",
        "    while i < h:\n",
        "        for j in range(n):\n",
        "            if lista[i - j] != sublista[-j - 1]:\n",
        "                i += skip.get(lista[i], n)\n",
        "                break\n",
        "        else:\n",
        "            indiceInicio = i - n + 1\n",
        "            indiceFim = indiceInicio + len(sublista)-1\n",
        "            return indiceInicio, indiceFim\n",
        "    return -1, -1"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_pnIh1h1Z_J"
      },
      "source": [
        "## getEmbeddingSentencaEmbeddingDocumentoComTodasPalavras\n",
        "\n",
        "Retorna os embeddings de uma sentença com todas as palavras a partir dos embeddings do documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSQs3O5QpJSj"
      },
      "source": [
        "def getEmbeddingSentencaEmbeddingDocumentoComTodasPalavras(embeddingDocumento, documento, sentenca, tokenizador):\n",
        "  \n",
        "  # Tokeniza o documento\n",
        "  documentoTokenizado = getDocumentoTokenizado(documento, tokenizador)  \n",
        "  #print(documentoTokenizado)\n",
        "\n",
        "  # Tokeniza a sentença\n",
        "  sentencaTokenizada = getDocumentoTokenizado(sentenca, tokenizador)\n",
        "  \n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentencaTokenizada.remove(\"[CLS]\")\n",
        "  sentencaTokenizada.remove(\"[SEP]\")  \n",
        "  #print(sentencaTokenizada)\n",
        "  #print(len(sentencaTokenizada))\n",
        "  \n",
        "  # Localiza os índices dos tokens da sentença no documento\n",
        "  inicio, fim = encontrarIndiceSubLista(documentoTokenizado,sentencaTokenizada)\n",
        "  #print(\"Sentença inicia em:\", inicio, \"até\", fim) \n",
        " \n",
        "  # Recupera os embeddings dos tokens da sentença a partir dos embeddings do documento\n",
        "  embeddingSentenca = embeddingDocumento[inicio:fim+1]\n",
        "  #print(\"embeddingSentenca=\", embeddingSentenca.shape)\n",
        "  \n",
        "  # Retorna o embedding da sentença do documento\n",
        "  return embeddingSentenca"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd9xmB9jwZZN"
      },
      "source": [
        "## getEmbeddingSentencaEmbeddingDocumentoSemStopWord\n",
        "\n",
        "Retorna os embeddings de uma sentença sem stopwords a partir dos embeddings do documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5XVsCsdwZZP"
      },
      "source": [
        "def getEmbeddingSentencaEmbeddingDocumentoSemStopWord(embeddingDocumento, documento, sentenca, tokenizador):\n",
        "  \n",
        "  # Tokeniza o documento\n",
        "  documentoTokenizado = getDocumentoTokenizado(documento, tokenizador)  \n",
        "  #print(documentoTokenizado)\n",
        "  \n",
        "  # Remove as stopword da sentença\n",
        "  sentencaSemStopWord = removeStopWord(sentenca, getStopwords(nlp))\n",
        "\n",
        "  # Tokeniza a sentença sem stopword\n",
        "  sentencaTokenizadaSemStopWord = getDocumentoTokenizado(sentencaSemStopWord, tokenizador)\n",
        "\n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentencaTokenizadaSemStopWord.remove(\"[CLS]\")\n",
        "  sentencaTokenizadaSemStopWord.remove(\"[SEP]\")  \n",
        "  #print(sentencaTokenizadaSemStopWord)\n",
        "  #print(len(sentencaTokenizadaSemStopWord))\n",
        "\n",
        "  # Tokeniza a sentença\n",
        "  sentencaTokenizada = getDocumentoTokenizado(sentenca, tokenizador)\n",
        "  \n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentencaTokenizada.remove(\"[CLS]\")\n",
        "  sentencaTokenizada.remove(\"[SEP]\")  \n",
        "  #print(sentencaTokenizada)\n",
        "  #print(len(sentencaTokenizada))\n",
        "\n",
        "  # Localiza os índices dos tokens da sentença no documento\n",
        "  inicio, fim = encontrarIndiceSubLista(documentoTokenizado,sentencaTokenizada)\n",
        "  #print(\"Sentença inicia em:\", inicio, \"até\", fim) \n",
        "   \n",
        "  # Recupera os embeddings dos tokens da sentença a partir dos embeddings do documento\n",
        "  embeddingSentenca = embeddingDocumento[inicio:fim+1]\n",
        "  #print(\"embeddingSentenca=\", embeddingSentenca.shape)\n",
        "\n",
        "  # Lista com os tensores selecionados\n",
        "  listaTokensSelecionados = []\n",
        "  # Localizar os embeddings dos tokens da sentença tokenizada sem stop word na sentença \n",
        "  # Procura somente no intervalo da sentença\n",
        "  for i, tokenSentenca in enumerate(sentencaTokenizada):\n",
        "    for tokenSentencaSemStopWord in sentencaTokenizadaSemStopWord: \n",
        "      if tokenSentenca == tokenSentencaSemStopWord:        \n",
        "        listaTokensSelecionados.append(embeddingSentenca[i:i+1])\n",
        "  \n",
        "  # Concatena os vetores da lista pela dimensão 0\n",
        "  embeddingSentencaSemStopWord = torch.cat(listaTokensSelecionados, dim=0)\n",
        "  #print(\"embeddingSentencaSemStopWord:\",embeddingSentencaSemStopWord.shape)\n",
        "\n",
        "  # Retorna o embedding da sentença do documento\n",
        "  return embeddingSentencaSemStopWord"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgW4gfEzh34J"
      },
      "source": [
        "## getEmbeddingSentencaEmbeddingDocumentoSomenteRelevante\n",
        "\n",
        "Retorna os embeddings de uma sentença somente com as palavras relevantes a partir dos embeddings do documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHbQJhzSh34T"
      },
      "source": [
        "def getEmbeddingSentencaEmbeddingDocumentoSomenteRelevante(embeddingDocumento, documento, sentenca, tokenizador, classeRelevante=\"NOUN\"):\n",
        "  \n",
        "  # Tokeniza o documento\n",
        "  documentoTokenizado = getDocumentoTokenizado(documento, tokenizador)  \n",
        "  #print(documentoTokenizado)\n",
        "  \n",
        "  # Retorna as palavras relevantes da sentença da classe especificada\n",
        "  sentencaSomenteRelevante = retornaPalavraRelevante(sentenca, nlp, classeRelevante)\n",
        "\n",
        "  # Tokeniza a sentença \n",
        "  sentencaTokenizadaSomenteRelevante = getDocumentoTokenizado(sentencaSomenteRelevante, tokenizador)\n",
        "\n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentencaTokenizadaSomenteRelevante.remove(\"[CLS]\")\n",
        "  sentencaTokenizadaSomenteRelevante.remove(\"[SEP]\")  \n",
        "  #print(sentencaTokenizadaSomenteRelevante)\n",
        "  #print(len(sentencaTokenizadaSomenteRelevante))\n",
        "\n",
        "  # Tokeniza a sentença\n",
        "  sentencaTokenizada = getDocumentoTokenizado(sentenca, tokenizador)\n",
        "  \n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentencaTokenizada.remove(\"[CLS]\")\n",
        "  sentencaTokenizada.remove(\"[SEP]\")  \n",
        "  #print(sentencaTokenizada)\n",
        "  #print(len(sentencaTokenizada))\n",
        "\n",
        "  # Localiza os índices dos tokens da sentença no documento\n",
        "  inicio, fim = encontrarIndiceSubLista(documentoTokenizado,sentencaTokenizada)\n",
        "  #print(\"Sentença inicia em:\", inicio, \"até\", fim) \n",
        "   \n",
        "  # Recupera os embeddings dos tokens da sentença a partir dos embeddings do documento\n",
        "  embeddingSentenca = embeddingDocumento[inicio:fim+1]\n",
        "  #print(\"embeddingSentenca=\", embeddingSentenca.shape)\n",
        "\n",
        "  # Lista com os tensores selecionados\n",
        "  listaTokensSelecionados = []\n",
        "  # Localizar os embeddings dos tokens da sentença tokenizada sem stop word na sentença \n",
        "  # Procura somente no intervalo da sentença\n",
        "  for i, tokenSentenca in enumerate(sentencaTokenizada):\n",
        "    for tokenSentencaSomenteRelevante in sentencaTokenizadaSomenteRelevante: \n",
        "      if tokenSentenca == tokenSentencaSomenteRelevante:        \n",
        "        listaTokensSelecionados.append(embeddingSentenca[i:i+1])\n",
        "  \n",
        "  # Concatena os vetores da lista pela dimensão 0\n",
        "  embeddingSentencaRelevante = torch.cat(listaTokensSelecionados, dim=0)\n",
        "  #print(\"embeddingSentencaRelevante:\",embeddingSentencaRelevante.shape)\n",
        "\n",
        "  # Retorna o embedding da sentença do documento\n",
        "  return embeddingSentencaRelevante"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jccxPKRSbBoK"
      },
      "source": [
        "## getEmbeddingSentencaEmbeddingDocumento\n",
        "\n",
        "Retorna os embeddings de uma sentença com ou sem stopwords a partir dos embeddings do documento sem os StopWords.\n",
        "\n",
        "Filtros:\n",
        "- ALL - Sentença com todas as palavras\n",
        "- CLEAN - Sentença sem as stopwords\n",
        "- NOUN - Sentença somente com substantivos\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRPeALoFbCAx"
      },
      "source": [
        "def getEmbeddingSentencaEmbeddingDocumento(embeddingDocumento, documento, sentenca, tokenizador, filtro=\"ALL\"):\n",
        "  if filtro == \"ALL\":\n",
        "    return getEmbeddingSentencaEmbeddingDocumentoComTodasPalavras(embeddingDocumento, documento, sentenca, tokenizador)\n",
        "  else:\n",
        "    if filtro == \"CLEAN\":\n",
        "        return getEmbeddingSentencaEmbeddingDocumentoSemStopWord(embeddingDocumento, documento, sentenca, tokenizador)\n",
        "    else:\n",
        "      if filtro == \"NOUN\":\n",
        "        return getEmbeddingSentencaEmbeddingDocumentoSomenteRelevante(embeddingDocumento, documento, sentenca, tokenizador, classeRelevante=\"NOUN\")\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-3QzDMwmfiJ"
      },
      "source": [
        "# 6 - Exemplo sentenças de documento original e permutado utilizando embedding da última camada do BERT usando a estratégia MEAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTmrN_IRmfiO"
      },
      "source": [
        "## Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYKIVpzTmfiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6575cd58-fa2f-4d76-b2da-adf05b67e823"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_original = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documentoOriginalConcatenado = \" \".join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + documentoOriginalConcatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLsHMNz9mfiQ"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pXg2A6zmfiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f7bb80f-34a6-4c18-efe4-6d655927e03f"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM5GUtVNmfiR"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ePiuflemfiS"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qhHUUoAmfiS"
      },
      "source": [
        "Gera os embeddings para o documentoginal. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o970gzwhmfiS"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnXcn_dvmfiT"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSiGmlpjmfiT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7191b10e-b952-4425-d329-6b7399fd3d64"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFDtOmN_mfiV"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM4Aosw4mfiV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4623528-3330-4697-ecf2-f888ad0059d6"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoOriginal = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", embeddingDocumentoOriginal.size())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([26, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T68Aje2tmfiW"
      },
      "source": [
        "Confirmando vetores dependentes do condocumento\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeNaW--hmfiW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb272186-36ac-4988-f486-00023d59dd29"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVDBdrHWmfiX"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkZrVaVFmfiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5065a34f-1f85-4b09-b7b7-e2b8c8f69361"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "embeddingSentenca1Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca1Original, tokenizer)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca2Original, tokenizer)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca3Original, tokenizer)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca4Original, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento original.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1Original,\"-\", str(embeddingSentenca1Original[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1Original,\"-\", str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2Original,\"-\", str(embeddingSentenca2Original[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2Original,\"-\", str(torch.sum(embeddingSentenca2Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3Original,\"-\", str(embeddingSentenca3Original[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3Original,\"-\", str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4Original,\"-\", str(embeddingSentenca4Original[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4Original,\"-\", str(torch.sum(embeddingSentenca4Original[:4])))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: Bom Dia, professor. - tensor([[-0.5098, -0.2877,  0.0873,  ...,  0.5436, -0.9302,  0.4668],\n",
            "        [ 0.6078, -0.8869,  0.3736,  ..., -0.3517, -1.2140, -0.3077],\n",
            "        [ 0.9075, -1.1233, -0.0093,  ...,  0.4433, -0.4633, -0.0113],\n",
            "        [ 0.2499, -0.4717, -0.1217,  ...,  0.7079, -0.2300,  0.4911]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-113.4554)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.3987, -0.9450,  0.1785,  ...,  0.7189, -0.6772, -0.1452],\n",
            "        [ 0.2067, -0.2705,  0.7145,  ...,  0.3047, -0.2718,  0.7577],\n",
            "        [ 0.2355,  0.2686,  0.5669,  ...,  1.0817,  0.5614,  0.3750],\n",
            "        [ 0.5264, -0.4600,  0.4810,  ..., -0.5559, -0.2941,  0.0378]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-115.8800)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[ 0.5178,  0.0863,  0.7394,  ..., -0.5765, -0.6208, -0.2230],\n",
            "        [ 0.1617,  1.1516, -0.0350,  ...,  0.1730,  0.2104, -0.0207],\n",
            "        [ 0.9251,  0.5806,  0.2491,  ...,  0.1687,  0.0772, -0.1173],\n",
            "        [ 0.5782,  1.3571, -0.5161,  ..., -0.0942,  0.0404, -0.0390]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-110.5299)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[ 0.4339, -0.7168,  0.1173,  ...,  0.0768, -1.2036,  0.3863],\n",
            "        [-0.6806, -1.0194, -0.0765,  ..., -0.1768, -0.6326,  0.2895],\n",
            "        [ 1.1384, -0.8541,  0.8992,  ...,  0.2798, -0.0381,  0.5061],\n",
            "        [ 0.9206, -0.9862,  0.1347,  ...,  0.4182, -1.0164,  0.4713]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-116.1903)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exqsxerrmfiY"
      },
      "source": [
        "Examinando os embeddings do documentoginal\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5py_A7lVmfiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ec3f7a-1209-4c01-e07b-0e3f46cfe40b"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaOriginal = tokenizer.tokenize(sentenca1Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1TokenizadaOriginal)\n",
        "embeddingSentenca1Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca1Original, tokenizer)\n",
        "print(\"\\nSentença 1 Original=\\\"\", sentenca1Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaOriginal = tokenizer.tokenize(sentenca2Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2TokenizadaOriginal)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca2Original, tokenizer)\n",
        "print(\"\\nSentença 2 Original=\\\"\", sentenca2Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca2Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca2Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaOriginal = tokenizer.tokenize(sentenca3Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3TokenizadaOriginal)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca3Original, tokenizer)\n",
        "print(\"\\nSentença 3 Original=\\\"\", sentenca3Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca3Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca3Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaOriginal = tokenizer.tokenize(sentenca4Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4TokenizadaOriginal)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca4Original, tokenizer)\n",
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Original))\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 1024])\n",
            "    Soma embeddings:  -141.45\n",
            "\n",
            "Sentença 2 Original=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -173.58\n",
            "\n",
            "Sentença 3 Original=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -167.58\n",
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -200.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hg9eKyjEfE5"
      },
      "source": [
        "## Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqHzON3PCa49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3445705e-5c29-4f4d-e414-4282f14ac0a1"
      },
      "source": [
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_permutado = [documento_original[3],   # \"Aguardo uma resposta, João.\",\n",
        "             documento_original[1],             # \"Qual o conteúdo da prova?\",              \n",
        "             documento_original[0],             # \"Vai cair tudo na prova?\",\n",
        "             documento_original[2]]             # \"Bom Dia, professor.\"]     \n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documentoPermutadoConcatenado = \" \".join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + documentoPermutadoConcatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 Bom           8,399\n",
            " 15 Dia           3,616\n",
            " 16 ,               117\n",
            " 17 professor     2,917\n",
            " 18 .               119\n",
            " 19 Vai          20,805\n",
            " 20 cair          9,322\n",
            " 21 tudo          2,745\n",
            " 22 na              229\n",
            " 23 prova         2,310\n",
            " 24 ?               136\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Snv8-ACy47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0de7420d-6c55-461a-b075-50983621e9c8"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLv52fBItM3I"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMFR5tSiCy48"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDFnt2yntIgn"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je2zyykXCy49"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZSIxolutQSp"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z09FmGtnCy49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4f9933c-f5e7-414f-ee94-4588a1718a5b"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aetb3LVYtXnI"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkk3Ix9kC93C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f96fed8-9c8a-4922-cf6c-c09e378f4a50"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoPermutado = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", embeddingDocumentoPermutado.size())"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([26, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIsMSKxNIUg9"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkHr7wEFIUhA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7bbc320-62ee-4562-f895-266cb92ce298"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca1Permutado, tokenizer)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca2Permutado, tokenizer)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca3Permutado, tokenizer)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca4Permutado, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento permutado.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1Permutado,\"-\", str(embeddingSentenca1Permutado[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1Original,\"-\", str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2Permutado,\"-\", str(embeddingSentenca2Permutado[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2Permutado,\"-\", str(torch.sum(embeddingSentenca2Permutado[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3Permutado,\"-\", str(embeddingSentenca3Permutado[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3Permutado,\"-\", str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4Permutado,\"-\", str(embeddingSentenca4Permutado[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4Permutado,\"-\", str(torch.sum(embeddingSentenca4Permutado[:4])))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[ 0.4152, -0.7838, -0.0364,  ...,  0.2686, -1.3504,  0.4707],\n",
            "        [-0.6012, -1.1043, -0.2660,  ..., -0.1943, -0.8112,  0.2180],\n",
            "        [ 1.2319, -1.1194,  0.9353,  ...,  0.2953, -0.1973,  0.5069],\n",
            "        [ 0.8861, -0.9666,  0.1831,  ...,  0.4595, -1.1045,  0.5615]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-113.4554)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.2957, -1.0942, -0.0024,  ...,  0.5875, -0.7164,  0.0104],\n",
            "        [ 0.2193, -0.4914,  0.7233,  ...,  0.3551, -0.3698,  0.8577],\n",
            "        [ 0.1377,  0.3856,  0.6060,  ...,  1.2662,  0.4965,  0.5211],\n",
            "        [ 0.5271, -0.4796,  0.4135,  ..., -0.5303, -0.3934,  0.1571]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-115.7949)\n",
            "\n",
            "Sentença 3: Bom Dia, professor. - tensor([[-0.2387, -0.2844,  0.3606,  ...,  0.6976, -0.9028,  0.3216],\n",
            "        [ 0.8188, -0.5643,  0.6965,  ..., -0.1440, -0.7018, -0.1933],\n",
            "        [ 0.9349, -1.0496,  0.1645,  ...,  0.4782, -0.3837, -0.2850],\n",
            "        [ 0.0104, -0.4793,  0.0501,  ...,  0.7341, -0.3687,  0.4562]])\n",
            "Soma embedding Sentença3: Bom Dia, professor. - tensor(-110.5299)\n",
            "\n",
            "Sentença 4: Vai cair tudo na prova? - tensor([[ 7.8763e-01, -1.1689e-01,  7.6317e-01,  ..., -4.3840e-01,\n",
            "         -6.5442e-01, -6.2304e-02],\n",
            "        [ 2.1062e-01,  1.0353e+00,  2.5935e-02,  ...,  7.1822e-02,\n",
            "          1.2368e-01,  1.1607e-03],\n",
            "        [ 7.5943e-01,  7.9969e-01,  2.4871e-01,  ...,  1.8793e-01,\n",
            "          9.4661e-02, -9.1650e-02],\n",
            "        [ 6.7001e-01,  1.3208e+00, -5.6431e-01,  ..., -6.3590e-03,\n",
            "          1.6016e-01, -1.4379e-01]])\n",
            "Soma embedding Sentença4: Vai cair tudo na prova? - tensor(-110.7603)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MINDqF2LDA9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8564c973-c4b7-47eb-df7d-83b0007dea0a"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaPermutado = tokenizer.tokenize(sentenca1Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1TokenizadaPermutado)\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca1Permutado, tokenizer)\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaPermutado = tokenizer.tokenize(sentenca2Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2TokenizadaPermutado)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca2Permutado, tokenizer)\n",
        "print(\"\\nSentença 2 Permutada=\\\"\", sentenca2Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca2Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca2Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaPermutado = tokenizer.tokenize(sentenca3Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3TokenizadaPermutado)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca3Permutado, tokenizer)\n",
        "print(\"\\nSentença 3 Permutada=\\\"\", sentenca3Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca3Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca3Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaPermutado = tokenizer.tokenize(sentenca4Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4TokenizadaPermutado)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca4Permutado, tokenizer)\n",
        "print(\"\\nSentença 4 Permutada=\\\"\", sentenca4Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Permutado))\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -199.90\n",
            "\n",
            "Sentença 2 Permutada=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -173.06\n",
            "\n",
            "Sentença 3 Permutada=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 14 e término em 18\n",
            "    Formato modelo : torch.Size([5, 1024])\n",
            "    Soma embeddings:  -141.81\n",
            "\n",
            "Sentença 4 Permutada=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 19 e término em 24\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -167.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UILLnj7KvHi"
      },
      "source": [
        "## Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eyEbV-7Kz6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df0d959-2dff-42f5-bc03-e6d098ab6787"
      },
      "source": [
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaOriginal)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Original))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embeddingSentenca4Original[:4]))\n",
        "\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaPermutado)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Permutado))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embeddingSentenca1Permutado[:4]))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -200.72\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.4339, -0.7168,  0.1173,  ...,  0.0768, -1.2036,  0.3863],\n",
            "        [-0.6806, -1.0194, -0.0765,  ..., -0.1768, -0.6326,  0.2895],\n",
            "        [ 1.1384, -0.8541,  0.8992,  ...,  0.2798, -0.0381,  0.5061],\n",
            "        [ 0.9206, -0.9862,  0.1347,  ...,  0.4182, -1.0164,  0.4713]])\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -199.90\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.4152, -0.7838, -0.0364,  ...,  0.2686, -1.3504,  0.4707],\n",
            "        [-0.6012, -1.1043, -0.2660,  ..., -0.1943, -0.8112,  0.2180],\n",
            "        [ 1.2319, -1.1194,  0.9353,  ...,  0.2953, -0.1973,  0.5069],\n",
            "        [ 0.8861, -0.9666,  0.1831,  ...,  0.4595, -1.1045,  0.5615]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JplTToZvDLiX"
      },
      "source": [
        "## Similaridade de cosseno entre os embeddings das sentenças\n",
        "\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html#scipy.spatial.distance.cosine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eERVKqh2uk6S"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCosseno(embeddings1, embeddings2):\n",
        "    \"\"\"\n",
        "    Similaridade do cosseno dos embeddgins dos documentos.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `embeddings1` - Um embedding a ser medido.\n",
        "    `embeddings2` - Um embedding a ser medido.\n",
        "    \"\"\"\n",
        "    \n",
        "    similaridade = cosine(embeddings1, embeddings2)\n",
        "    \n",
        "    return similaridade"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av6tZHt6DLiY"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOQ9vWuADLiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da338b48-5d5c-450e-8728-99ec4dc66a11"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosOriginal = float(somaScos)/float(n-1)\n",
        "print(\"Ccos Original:\", CcosOriginal)  \n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.2171152432759603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmTaSFZNDLiY"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYIO7AXCDLiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fbb7405-df3e-4f86-95e7-37b2f0bedff4"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "   # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosPermutado = float(somaScos)/float(np-1)\n",
        "print(\"Ccos Original:\", CcosPermutado)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.2673523624738057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiJ_9-KPDLiY"
      },
      "source": [
        "### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 1.\n",
        "- Documentos com sentenças diferenntes resulta uma medida menor que 1.\n",
        "- Documento com sentenças muito diferentes apresentam valores menores que 1.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de similaridade.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ1pRGiEDLiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b08beb-8fd4-4351-b540-70e08f8f6510"
      },
      "source": [
        "print(\"Ccos Original :\", CcosOriginal)\n",
        "print(\"Ccos Permutado:\", CcosPermutado)\n",
        "\n",
        "if (CcosOriginal > CcosPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as sentenças!\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos Original : 0.2171152432759603\n",
            "Ccos Permutado: 0.2673523624738057\n",
            "Documento Permutado tem menor similaridade de cosseno entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tLfL6BFDLiZ"
      },
      "source": [
        "## Distância euclidiana entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância L2 ou norma L2.\n",
        "\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html#scipy.spatial.distance.euclidean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKrR5hMNDLiZ"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distanciaEuclidiana(embeddings1, embeddings2):\n",
        "    \"\"\"\n",
        "    Distância euclidiana entre os embeddings dos documentos.\n",
        "    Possui outros nomes como distância L2 ou norma L2.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `embeddings1` - Um embedding a ser medido.\n",
        "    `embeddings2` - Um embedding a ser medido.\n",
        "    \"\"\"\n",
        "    \n",
        "    distancia = euclidean(embeddings1, embeddings2)\n",
        "    \n",
        "    return distancia"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scio7VcxDLiZ"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qPGyX3WDLiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21bbf6ac-acb8-4925-f446-0eff81ca1a46"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucOriginal = float(somaSeuc)/float(n-1)\n",
        "print(\"Ceuc Original:\", CeucOriginal)  \n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 11.104767481486002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eiJkQ9tDLiZ"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCa8HJAjDLiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "440af6d8-7168-4092-b734-e0c02c9725d4"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucPermutado = float(somaSeuc)/float(np-1)\n",
        "print(\"Ceuc Original:\", CeucPermutado)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 12.478007634480795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz5eaOkEDLiZ"
      },
      "source": [
        "### Compara as médias da distância euclidiana dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhelRMqGDLia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f652b8a0-62ca-43bf-9814-10b53ad579dd"
      },
      "source": [
        "print(\"Ceuc Original :\", CeucOriginal)\n",
        "print(\"Ceuc Permutado:\", CeucPermutado)\n",
        "\n",
        "if (CeucOriginal < CeucPermutado):\n",
        "    print(\"Documento original tem menor distância euclidiana entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância euclidiana entre as sentenças!\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ceuc Original : 11.104767481486002\n",
            "Ceuc Permutado: 12.478007634480795\n",
            "Documento original tem menor distância euclidiana entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqPCQJ24DLia"
      },
      "source": [
        "## Distância Manhattan entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "\n",
        "Igual a distância de subtração absoluta.\n",
        "\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cityblock.html#scipy.spatial.distance.cityblock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCjpuWTRDLib"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def distanciaManhattan(embeddings1, embeddings2):\n",
        "    \"\"\"\n",
        "    Distância Manhattan entre os embeddings dos documentos \n",
        "    Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `embeddings1` - Um embedding a ser medido.\n",
        "    `embeddings2` - Um embedding a ser medido.\n",
        "    \"\"\"\n",
        "    \n",
        "    distancia = cityblock(embeddings1, embeddings2)\n",
        "\n",
        "    return distancia\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKSaAqoZDLib"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFLah0Q9DLib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31eba1ed-a629-413e-fdb3-97f5f25cace7"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanOriginal = float(somaSman)/float(n-1)\n",
        "print(\"Cman Original:\", CmanOriginal)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Cman Original: 274.52039591471356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43BxjteRDLib"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E6e9k1YDLic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b60d7993-b7ba-4196-8091-fa730de47eb0"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanPermutado = float(somaSman)/float(n-1)\n",
        "print(\"Cman Permutado:\", CmanPermutado)  \n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Cman Permutado: 308.22056070963544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ-427FYDLic"
      },
      "source": [
        "### Compara as médias da distância de manhattan dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3snpqLtIDLic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eab6899-19e8-4e74-bff3-ddf5012eea1b"
      },
      "source": [
        "print(\"Cman Original :\", CmanOriginal)\n",
        "print(\"Cman Permutado:\", CmanPermutado)\n",
        "\n",
        "if (CmanOriginal < CmanPermutado):\n",
        "    print(\"Documento original tem menor distância de manhattan entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância de manhattan entre as sentenças!\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cman Original : 274.52039591471356\n",
            "Cman Permutado: 308.22056070963544\n",
            "Documento original tem menor distância de manhattan entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJRDXLHua9ce"
      },
      "source": [
        "## Resumo\n",
        "\n",
        "Resultado das medidas utilizando a última camada do BERT.\n",
        "\n",
        "Base(MEAN):\n",
        "- Ccos       :   0.62004846          0.54099079\n",
        "- Ceuc       :   7.11709849          7.99353107\n",
        "- Cman       :   153.63694255          171.98441060\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4jEPcWRa9ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2093ebec-e0ae-45ce-b9d8-993f0ac5fd4d"
      },
      "source": [
        "print(\"Resultado das medidas utilizando a última camada do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print(\"Ccos       :   {:.8f}          {:.8f}\".format(CcosOriginal,CcosPermutado))\n",
        "print(\"Ceuc       :   {:.8f}          {:.8f}\".format(CeucOriginal,CeucPermutado))\n",
        "print(\"Cman       :   {:.8f}          {:.8f}\".format(CmanOriginal,CmanPermutado))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado das medidas utilizando a última camada do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Ccos       :   0.21711524          0.26735236\n",
            "Ceuc       :   11.10476748          12.47800763\n",
            "Cman       :   274.52039591          308.22056071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5NHv8JQ2Om8"
      },
      "source": [
        "# 7 - Exemplo sentenças de documento original e permutado utilizando embedding da última camada do BERT usando estratégia MAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuFaynIX2OnA"
      },
      "source": [
        "## Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ird39LBl2OnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3979976-d898-4bfb-dd18-3f9893879071"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_original = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documentoOriginalConcatenado = \" \".join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + documentoOriginalConcatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCazJyHf2OnB"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkzJFTWX2OnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "481b4d93-51f8-4f06-d5fd-067f95c837e6"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H624EpGv2OnB"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBfLtHz92OnB"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM5nK5Zr2OnB"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0SPckuI2OnC"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceWI29Ij2OnC"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ReZJfwR2OnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe39a2ff-00f7-491f-8139-57fa21172e8e"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysFToAty2OnC"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp8ImZM52OnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce2884e-0c81-4b11-86bd-f601e6ccb687"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoOriginal = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", embeddingDocumentoOriginal.size())"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([26, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhiX25CW2OnC"
      },
      "source": [
        "Confirmando vetores dependentes do condocumento\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7C8abOy2OnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a503e0c4-0bd9-4da4-f229-618a35bdd450"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZCfusGL2OnD"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaOvIge52OnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9378204a-6b1d-430c-d27e-37b3a83bec4e"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "embeddingSentenca1Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca1Original, tokenizer)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca2Original, tokenizer)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca3Original, tokenizer)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca4Original, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento original.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1Original,\"-\", str(embeddingSentenca1Original[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1Original,\"-\", str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2Original,\"-\", str(embeddingSentenca2Original[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2Original,\"-\", str(torch.sum(embeddingSentenca2Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3Original,\"-\", str(embeddingSentenca3Original[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3Original,\"-\", str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4Original,\"-\", str(embeddingSentenca4Original[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4Original,\"-\", str(torch.sum(embeddingSentenca4Original[:4])))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: Bom Dia, professor. - tensor([[-0.5098, -0.2877,  0.0873,  ...,  0.5436, -0.9302,  0.4668],\n",
            "        [ 0.6078, -0.8869,  0.3736,  ..., -0.3517, -1.2140, -0.3077],\n",
            "        [ 0.9075, -1.1233, -0.0093,  ...,  0.4433, -0.4633, -0.0113],\n",
            "        [ 0.2499, -0.4717, -0.1217,  ...,  0.7079, -0.2300,  0.4911]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-113.4554)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.3987, -0.9450,  0.1785,  ...,  0.7189, -0.6772, -0.1452],\n",
            "        [ 0.2067, -0.2705,  0.7145,  ...,  0.3047, -0.2718,  0.7577],\n",
            "        [ 0.2355,  0.2686,  0.5669,  ...,  1.0817,  0.5614,  0.3750],\n",
            "        [ 0.5264, -0.4600,  0.4810,  ..., -0.5559, -0.2941,  0.0378]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-115.8800)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[ 0.5178,  0.0863,  0.7394,  ..., -0.5765, -0.6208, -0.2230],\n",
            "        [ 0.1617,  1.1516, -0.0350,  ...,  0.1730,  0.2104, -0.0207],\n",
            "        [ 0.9251,  0.5806,  0.2491,  ...,  0.1687,  0.0772, -0.1173],\n",
            "        [ 0.5782,  1.3571, -0.5161,  ..., -0.0942,  0.0404, -0.0390]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-110.5299)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[ 0.4339, -0.7168,  0.1173,  ...,  0.0768, -1.2036,  0.3863],\n",
            "        [-0.6806, -1.0194, -0.0765,  ..., -0.1768, -0.6326,  0.2895],\n",
            "        [ 1.1384, -0.8541,  0.8992,  ...,  0.2798, -0.0381,  0.5061],\n",
            "        [ 0.9206, -0.9862,  0.1347,  ...,  0.4182, -1.0164,  0.4713]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-116.1903)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbGSdUzw2OnD"
      },
      "source": [
        "Examinando os embeddings do documento original\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okFUjfDG2OnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95af7349-eebb-4757-bd95-936fa51641b0"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaOriginal = tokenizer.tokenize(sentenca1Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1TokenizadaOriginal)\n",
        "embeddingSentenca1Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca1Original, tokenizer)\n",
        "print(\"\\nSentença 1 Original=\\\"\", sentenca1Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaOriginal = tokenizer.tokenize(sentenca2Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2TokenizadaOriginal)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca2Original, tokenizer)\n",
        "print(\"\\nSentença 2 Original=\\\"\", sentenca2Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca2Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca2Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaOriginal = tokenizer.tokenize(sentenca3Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3TokenizadaOriginal)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoPermutadoConcatenado, sentenca3Original, tokenizer)\n",
        "print(\"\\nSentença 3 Original=\\\"\", sentenca3Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca3Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca3Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaOriginal = tokenizer.tokenize(sentenca4Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4TokenizadaOriginal)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca4Original, tokenizer)\n",
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Original))\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 1024])\n",
            "    Soma embeddings:  -141.45\n",
            "\n",
            "Sentença 2 Original=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -173.58\n",
            "\n",
            "Sentença 3 Original=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -172.02\n",
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -200.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVL1lBnB2OnD"
      },
      "source": [
        "## Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uMC8h3F2OnE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d47f9384-010d-4a86-9cfe-4531265d2551"
      },
      "source": [
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_permutado = [documento_original[3],   # \"Aguardo uma resposta, João.\",\n",
        "             documento_original[1],             # \"Qual o conteúdo da prova?\",              \n",
        "             documento_original[0],             # \"Vai cair tudo na prova?\",\n",
        "             documento_original[2]]             # \"Bom Dia, professor.\"]     \n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documentoPermutadoConcatenado = \" \".join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + documentoPermutadoConcatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 Bom           8,399\n",
            " 15 Dia           3,616\n",
            " 16 ,               117\n",
            " 17 professor     2,917\n",
            " 18 .               119\n",
            " 19 Vai          20,805\n",
            " 20 cair          9,322\n",
            " 21 tudo          2,745\n",
            " 22 na              229\n",
            " 23 prova         2,310\n",
            " 24 ?               136\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPjSfaUR2OnE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c6edc5f-78cc-4659-9bd3-23d4ceef8072"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXp0WqZa2OnE"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuwXz5Ye2OnE"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUmYOo7s2OnE"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gLUe6tT2OnE"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS0h6i4t2OnE"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1kcPJU82OnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9033b43-3031-4814-c05d-fdfca4db59c3"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Xu6swe2OnF"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOljOpq52OnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17ff4926-a957-4826-dbc1-3253011386f7"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoPermutado = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", embeddingDocumentoPermutado.size())"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([26, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BlaBKoE2OnF"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWAJMgs72OnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab7feeda-566e-40a3-b2fe-ee94b282f7a8"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca1Permutado, tokenizer)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca2Permutado, tokenizer)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca3Permutado, tokenizer)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca4Permutado, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento permutado.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1Permutado,\"-\", str(embeddingSentenca1Permutado[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1Original,\"-\", str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2Permutado,\"-\", str(embeddingSentenca2Permutado[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2Permutado,\"-\", str(torch.sum(embeddingSentenca2Permutado[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3Permutado,\"-\", str(embeddingSentenca3Permutado[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3Permutado,\"-\", str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4Permutado,\"-\", str(embeddingSentenca4Permutado[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4Permutado,\"-\", str(torch.sum(embeddingSentenca4Permutado[:4])))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[ 0.4152, -0.7838, -0.0364,  ...,  0.2686, -1.3504,  0.4707],\n",
            "        [-0.6012, -1.1043, -0.2660,  ..., -0.1943, -0.8112,  0.2180],\n",
            "        [ 1.2319, -1.1194,  0.9353,  ...,  0.2953, -0.1973,  0.5069],\n",
            "        [ 0.8861, -0.9666,  0.1831,  ...,  0.4595, -1.1045,  0.5615]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-113.4554)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.2957, -1.0942, -0.0024,  ...,  0.5875, -0.7164,  0.0104],\n",
            "        [ 0.2193, -0.4914,  0.7233,  ...,  0.3551, -0.3698,  0.8577],\n",
            "        [ 0.1377,  0.3856,  0.6060,  ...,  1.2662,  0.4965,  0.5211],\n",
            "        [ 0.5271, -0.4796,  0.4135,  ..., -0.5303, -0.3934,  0.1571]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-115.7949)\n",
            "\n",
            "Sentença 3: Bom Dia, professor. - tensor([[-0.2387, -0.2844,  0.3606,  ...,  0.6976, -0.9028,  0.3216],\n",
            "        [ 0.8188, -0.5643,  0.6965,  ..., -0.1440, -0.7018, -0.1933],\n",
            "        [ 0.9349, -1.0496,  0.1645,  ...,  0.4782, -0.3837, -0.2850],\n",
            "        [ 0.0104, -0.4793,  0.0501,  ...,  0.7341, -0.3687,  0.4562]])\n",
            "Soma embedding Sentença3: Bom Dia, professor. - tensor(-115.7974)\n",
            "\n",
            "Sentença 4: Vai cair tudo na prova? - tensor([[ 7.8763e-01, -1.1689e-01,  7.6317e-01,  ..., -4.3840e-01,\n",
            "         -6.5442e-01, -6.2304e-02],\n",
            "        [ 2.1062e-01,  1.0353e+00,  2.5935e-02,  ...,  7.1822e-02,\n",
            "          1.2368e-01,  1.1607e-03],\n",
            "        [ 7.5943e-01,  7.9969e-01,  2.4871e-01,  ...,  1.8793e-01,\n",
            "          9.4661e-02, -9.1650e-02],\n",
            "        [ 6.7001e-01,  1.3208e+00, -5.6431e-01,  ..., -6.3590e-03,\n",
            "          1.6016e-01, -1.4379e-01]])\n",
            "Soma embedding Sentença4: Vai cair tudo na prova? - tensor(-110.7603)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jktC1hfL2OnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28c5f8ee-ba55-48ad-8402-5e04f72343cd"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaPermutado = tokenizer.tokenize(sentenca1Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1TokenizadaPermutado)\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca1Permutado, tokenizer)\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaPermutado = tokenizer.tokenize(sentenca2Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2TokenizadaPermutado)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca2Permutado, tokenizer)\n",
        "print(\"\\nSentença 2 Permutada=\\\"\", sentenca2Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca2Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca2Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaPermutado = tokenizer.tokenize(sentenca3Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3TokenizadaPermutado)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca3Permutado, tokenizer)\n",
        "print(\"\\nSentença 3 Permutada=\\\"\", sentenca3Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca3Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca3Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaPermutado = tokenizer.tokenize(sentenca4Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4TokenizadaPermutado)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca4Permutado, tokenizer)\n",
        "print(\"\\nSentença 4 Permutada=\\\"\", sentenca4Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Permutado))\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -199.90\n",
            "\n",
            "Sentença 2 Permutada=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -173.06\n",
            "\n",
            "Sentença 3 Permutada=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 14 e término em 18\n",
            "    Formato modelo : torch.Size([5, 1024])\n",
            "    Soma embeddings:  -141.81\n",
            "\n",
            "Sentença 4 Permutada=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 19 e término em 24\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -167.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45HsR7ey2OnF"
      },
      "source": [
        "## Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9pbUlC72OnG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abdc4d57-3c31-4ca8-a67e-cdd9517153ce"
      },
      "source": [
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaOriginal)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Original))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embeddingSentenca4Original[:4]))\n",
        "\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaPermutado)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Permutado))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embeddingSentenca1Permutado[:4]))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -200.72\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.4339, -0.7168,  0.1173,  ...,  0.0768, -1.2036,  0.3863],\n",
            "        [-0.6806, -1.0194, -0.0765,  ..., -0.1768, -0.6326,  0.2895],\n",
            "        [ 1.1384, -0.8541,  0.8992,  ...,  0.2798, -0.0381,  0.5061],\n",
            "        [ 0.9206, -0.9862,  0.1347,  ...,  0.4182, -1.0164,  0.4713]])\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -199.90\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.4152, -0.7838, -0.0364,  ...,  0.2686, -1.3504,  0.4707],\n",
            "        [-0.6012, -1.1043, -0.2660,  ..., -0.1943, -0.8112,  0.2180],\n",
            "        [ 1.2319, -1.1194,  0.9353,  ...,  0.2953, -0.1973,  0.5069],\n",
            "        [ 0.8861, -0.9666,  0.1831,  ...,  0.4595, -1.1045,  0.5615]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd8YKgnyJuUv"
      },
      "source": [
        "## Similaridade de cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cREd7N1JuUv"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCosseno(sentenca1, sentenca2):\n",
        "  similaridade = cosine(sentenca1, sentenca2)\n",
        "  return similaridade"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auu-XqHOJuUv"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bNpbR0QJuUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd3e5f7-a38d-4bc9-ec37-85fd355a7697"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosOriginal = float(somaScos)/float(n-1)\n",
        "print(\"Ccos Original:\", CcosOriginal)  \n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.18421759208043417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWNj-6i5JuUv"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQEsVKJ1JuUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07bb4f6c-4ff4-4a6a-9a34-0a171aeed768"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosPermutado = float(somaScos)/float(np-1)\n",
        "print(\"Ccos Original:\", CcosPermutado)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.2132158875465393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ecHZJrCJuUw"
      },
      "source": [
        "### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 1.\n",
        "- Documentos com sentenças diferenntes resulta uma medida menor que 1.\n",
        "- Documento com sentenças muito diferentes apresentam valores menores que 1.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de similaridade.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTxA53uUJuUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e28d891-c960-42ec-9549-bea8a9bde959"
      },
      "source": [
        "print(\"Ccos Original :\", CcosOriginal)\n",
        "print(\"Ccos Permutado:\", CcosPermutado)\n",
        "\n",
        "if (CcosOriginal > CcosPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as sentenças!\")"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos Original : 0.18421759208043417\n",
            "Ccos Permutado: 0.2132158875465393\n",
            "Documento Permutado tem menor similaridade de cosseno entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPQwuHf5JuUw"
      },
      "source": [
        "## Distância euclidiana entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância L2 ou norma L2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RmL_qXCJuUw"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distanciaEuclidiana(sentenca1, sentenca2):\n",
        "  distancia = euclidean(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a61ckdmoJuUw"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yu-uTlhJuUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3abb95fb-ac46-450b-9661-e2d88acee594"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucOriginal = float(somaSeuc)/float(n-1)\n",
        "print(\"Ceuc Original:\", CeucOriginal)  \n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 15.606139183044434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBRvVkI4JuUw"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNgY1epXJuUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f945716-6346-4d91-bdf1-4d727df40ff0"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucPermutado = float(somaSeuc)/float(np-1)\n",
        "print(\"Ceuc Original:\", CeucPermutado)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 16.949792861938477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI4qwrbWJuUx"
      },
      "source": [
        "### Compara as médias da distância euclidiana dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeJFDd5mJuUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b215e9f-28c5-4797-83a2-0b2963200176"
      },
      "source": [
        "print(\"Ceuc Original :\", CeucOriginal)\n",
        "print(\"Ceuc Permutado:\", CeucPermutado)\n",
        "\n",
        "if (CeucOriginal < CeucPermutado):\n",
        "    print(\"Documento original tem menor distância euclidiana entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância euclidiana entre as sentenças!\")"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ceuc Original : 15.606139183044434\n",
            "Ceuc Permutado: 16.949792861938477\n",
            "Documento original tem menor distância euclidiana entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EUoPQNyJuUx"
      },
      "source": [
        "## Distância Manhattan entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "\n",
        "Igual a subtração absoluta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IydTlzptJuUx"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def distanciaManhattan(sentenca1, sentenca2):\n",
        "  distancia = cityblock(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nuu93cfvJuUx"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1DavITCJuUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a3fca6-5e92-4428-ce41-4be595bff736"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj)) \n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanOriginal = float(somaSman)/float(n-1)\n",
        "print(\"Cman Original:\", CmanOriginal)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Cman Original: 375.63950602213544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcWmFXmQJuUx"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6mpG5x0JuUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ed3e06-89c4-4917-8667-7e02308635de"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj)) \n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanPermutado = float(somaSman)/float(np-1)\n",
        "print(\"Ceuc Original:\", CmanPermutado)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 415.24462890625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa8RFKDVJuUy"
      },
      "source": [
        "### Compara as médias da distância de manhattan dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB_oWMJ0JuUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ffd679b-1ff4-4b5f-f9d5-9f5e547a6dcf"
      },
      "source": [
        "print(\"Cman Original :\", CmanOriginal)\n",
        "print(\"Cman Permutado:\", CmanPermutado)\n",
        "\n",
        "if (CmanOriginal < CmanPermutado):\n",
        "    print(\"Documento original tem menor distância de manhattan entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância de manhattan entre as sentenças!\")"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cman Original : 375.63950602213544\n",
            "Cman Permutado: 415.24462890625\n",
            "Documento original tem menor distância de manhattan entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRdr2bqZ2OnQ"
      },
      "source": [
        "## Resumo\n",
        "\n",
        "Resultado das medidas utilizando a última camada do BERT.\n",
        "\n",
        "Base(MEAN):\n",
        "- Ccos       :   0.62004846          0.54099079\n",
        "- Ceuc       :   7.11709849          7.99353107\n",
        "- Cman       :   153.63694255          171.98441060\n",
        "\n",
        "Base(MAX):\n",
        "- Ccos       :   0.79404344          0.76070702\n",
        "- Ceuc       :   10.07102553          10.70145098\n",
        "- Cman       :   213.39687602          231.29823303"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW0cKGq52OnQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de48f8d7-bc4a-4f61-b399-c6da6ff2c278"
      },
      "source": [
        "print(\"Resultado das medidas utilizando a última camada do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print(\"Ccos       :   {:.8f}          {:.8f}\".format(CcosOriginal,CcosPermutado))\n",
        "print(\"Ceuc       :   {:.8f}          {:.8f}\".format(CeucOriginal,CeucPermutado))\n",
        "print(\"Cman       :   {:.8f}          {:.8f}\".format(CmanOriginal,CmanPermutado))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado das medidas utilizando a última camada do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Ccos       :   0.18421759          0.21321589\n",
            "Ceuc       :   15.60613918          16.94979286\n",
            "Cman       :   375.63950602          415.24462891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru02mC0Estsb"
      },
      "source": [
        "# 8 - Exemplo sentenças de documento original e permutado utilizando embedding a concatenação das 4 últimas camadas do BERT usando estratégia a MEAN\n",
        "\n",
        "Como estamos utilizando os embeddings concatenado das 4 últimas camadas onde ocorre 768 entenda-se 3072 que é o resultado de 768 por 4 que é a dimensão do MCL BERT de tamanho base. E onde ocorre 1024 entenda-se 4096 que é o resultado de 1024 por 4 que é a dimensão do MCL BERT de tamanho large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1vDmYJTstsg"
      },
      "source": [
        "## Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJOUyEpistsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "863377c8-98c0-4568-d08d-11d04c621228"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_original = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documentoOriginalConcatenado = \" \".join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + documentoOriginalConcatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4HrZqBfstsh"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0tDxh3Mstsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc6b51ea-5bb3-43d3-9827-e1d73104fb03"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAQf9nM8stsh"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFqFcnx2stsh"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJbHPnoAstsi"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda todas as camadas da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51R6f4Mistsi"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx2_AvR8tbnZ"
      },
      "source": [
        "Recupera a saída e concatena as 4 últimas camada do BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C0KcRUHstsj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "724f7b7a-9af2-4dcf-cb0a-13ef1d443ca9"
      },
      "source": [
        "# Cria uma lista com os tensores a serem concatenados\n",
        "# Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "# Lista com os tensores a serem concatenados\n",
        "listaConcat = []\n",
        "# Percorre os 4 últimos\n",
        "for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "# Realiza a concatenação dos embeddings de todos as camadas\n",
        "# Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "# Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "\n",
        "print (\"O vetor da  concatenação das 4 últimas camadas oculta tem o formato:\", concat4_hidden_states.size())"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da  concatenação das 4 últimas camadas oculta tem o formato: torch.Size([1, 26, 4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM4luftrstsk"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f18k_o-stsk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c1ccd5a-f463-43b3-c53b-3a194fdcebc5"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoOriginal = torch.squeeze(concat4_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", embeddingDocumentoOriginal.size())"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([26, 4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g0E9s2Rstsk"
      },
      "source": [
        "Confirmando vetores dependentes do condocumento\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQGOQF-kstsk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c08cd1f-6831-40ac-faa7-e5152a8ef134"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veMZsnAsstsl"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9RSPe2Hstsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e954effa-19dd-4673-bad0-f30092b16dda"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "embeddingSentenca1Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca1Original, tokenizer)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca2Original, tokenizer)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca3Original, tokenizer)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca4Original, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento original.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1Original,\"-\", str(embeddingSentenca1Original[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1Original,\"-\", str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2Original,\"-\", str(embeddingSentenca2Original[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2Original,\"-\", str(torch.sum(embeddingSentenca2Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3Original,\"-\", str(embeddingSentenca3Original[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3Original,\"-\", str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4Original,\"-\", str(embeddingSentenca4Original[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4Original,\"-\", str(torch.sum(embeddingSentenca4Original[:4])))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: Bom Dia, professor. - tensor([[-0.5098, -0.2877,  0.0873,  ...,  0.9288, -0.0796,  0.3684],\n",
            "        [ 0.6078, -0.8869,  0.3736,  ...,  0.0124,  0.0094, -0.1719],\n",
            "        [ 0.9075, -1.1233, -0.0093,  ...,  0.1359, -0.3067, -0.4078],\n",
            "        [ 0.2499, -0.4717, -0.1217,  ...,  0.4604,  0.0075,  0.1555]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-176.3423)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.3987, -0.9450,  0.1785,  ...,  0.8476, -0.0572,  0.0365],\n",
            "        [ 0.2067, -0.2705,  0.7145,  ...,  0.2113,  0.2899,  0.5135],\n",
            "        [ 0.2355,  0.2686,  0.5669,  ...,  0.7205,  0.2510,  0.0924],\n",
            "        [ 0.5264, -0.4600,  0.4810,  ...,  0.1881,  0.0931,  0.1731]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-165.1487)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[ 0.5178,  0.0863,  0.7394,  ..., -0.3301, -0.0115,  0.1168],\n",
            "        [ 0.1617,  1.1516, -0.0350,  ..., -0.2189,  0.1096, -0.3660],\n",
            "        [ 0.9251,  0.5806,  0.2491,  ...,  0.0027,  0.2467, -0.1535],\n",
            "        [ 0.5782,  1.3571, -0.5161,  ...,  0.5285,  0.2553, -0.0197]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-177.0387)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[ 0.4339, -0.7168,  0.1173,  ...,  0.3840, -0.0936,  0.1355],\n",
            "        [-0.6806, -1.0194, -0.0765,  ...,  0.2201,  0.2731, -0.0384],\n",
            "        [ 1.1384, -0.8541,  0.8992,  ...,  0.5247,  0.2278,  0.0653],\n",
            "        [ 0.9206, -0.9862,  0.1347,  ...,  0.4393, -0.4792,  0.1743]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-169.8891)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGUkprWbstsl"
      },
      "source": [
        "Examinando os embeddings do documento original\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL-c0aqKstsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a56c5d-0bbd-46d3-efde-e87d860d0216"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaOriginal = tokenizer.tokenize(sentenca1Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1TokenizadaOriginal)\n",
        "embeddingSentenca1Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca1Original, tokenizer)\n",
        "print(\"\\nSentença 1 Original=\\\"\", sentenca1Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaOriginal = tokenizer.tokenize(sentenca2Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2TokenizadaOriginal)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca2Original, tokenizer)\n",
        "print(\"\\nSentença 2 Original=\\\"\", sentenca2Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca2Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca2Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaOriginal = tokenizer.tokenize(sentenca3Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3TokenizadaOriginal)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca3Original, tokenizer)\n",
        "print(\"\\nSentença 3 Original=\\\"\", sentenca3Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca3Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca3Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaOriginal = tokenizer.tokenize(sentenca4Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4TokenizadaOriginal)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca4Original, tokenizer)\n",
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Original))\n"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 4096])\n",
            "    Soma embeddings:  -225.64\n",
            "\n",
            "Sentença 2 Original=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 4096])\n",
            "    Soma embeddings:  -252.06\n",
            "\n",
            "Sentença 3 Original=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 4096])\n",
            "    Soma embeddings:  -260.65\n",
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 4096])\n",
            "    Soma embeddings:  -288.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRdBVlt9stsm"
      },
      "source": [
        "## Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xsxt0Jistsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21b8c5df-ed4d-47ba-98a4-62981d1cc761"
      },
      "source": [
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_permutado = [documento_original[3],   # \"Aguardo uma resposta, João.\",\n",
        "             documento_original[1],             # \"Qual o conteúdo da prova?\",              \n",
        "             documento_original[0],             # \"Vai cair tudo na prova?\",\n",
        "             documento_original[2]]             # \"Bom Dia, professor.\"]     \n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documentoPermutadoConcatenado = \" \".join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + documentoPermutadoConcatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 Bom           8,399\n",
            " 15 Dia           3,616\n",
            " 16 ,               117\n",
            " 17 professor     2,917\n",
            " 18 .               119\n",
            " 19 Vai          20,805\n",
            " 20 cair          9,322\n",
            " 21 tudo          2,745\n",
            " 22 na              229\n",
            " 23 prova         2,310\n",
            " 24 ?               136\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM5NtnGRuAsU"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GDGmFc_stsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "285b6f44-39a9-44a7-e73e-c3ecd4d9af80"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4WehJSxuDvo"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrvEtHp7stsm"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQDiqh5UuMnc"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda todas as camadas da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tGQsVMpstsm"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5CmoS1at0YA"
      },
      "source": [
        "Recupera a saída e concatena as 4 últimas camada do BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRDYmUq9stsn"
      },
      "source": [
        "# Cria uma lista com os tensores a serem concatenados\n",
        "# Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "# Lista com os tensores a serem concatenados\n",
        "listaConcat = []\n",
        "# Percorre os 4 últimos\n",
        "for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "# Realiza a concatenação dos embeddings de todos as camadas\n",
        "# Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "# Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  "
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm9EVbDauSiI"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXNqk-oQstsn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6274df4-c29b-44a9-dbce-3941414015b0"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoPermutado = torch.squeeze(concat4_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", embeddingDocumentoPermutado.size())"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([26, 4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbLgVumnstso"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1nUZ7OLstso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afb430f1-535e-4587-ffad-24bc3a5c2a57"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca1Permutado, tokenizer)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca2Permutado, tokenizer)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca3Permutado, tokenizer)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca4Permutado, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento permutado.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1Permutado,\"-\", str(embeddingSentenca1Permutado[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1Original,\"-\", str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2Permutado,\"-\", str(embeddingSentenca2Permutado[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2Permutado,\"-\", str(torch.sum(embeddingSentenca2Permutado[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3Permutado,\"-\", str(embeddingSentenca3Permutado[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3Permutado,\"-\", str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4Permutado,\"-\", str(embeddingSentenca4Permutado[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4Permutado,\"-\", str(torch.sum(embeddingSentenca4Permutado[:4])))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[ 0.4152, -0.7838, -0.0364,  ...,  0.4706, -0.1614,  0.1815],\n",
            "        [-0.6012, -1.1043, -0.2660,  ...,  0.1239,  0.2729, -0.0574],\n",
            "        [ 1.2319, -1.1194,  0.9353,  ...,  0.5062,  0.1518,  0.1078],\n",
            "        [ 0.8861, -0.9666,  0.1831,  ...,  0.5239, -0.4849,  0.1787]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-176.3423)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.2957, -1.0942, -0.0024,  ...,  0.6032, -0.1615,  0.4615],\n",
            "        [ 0.2193, -0.4914,  0.7233,  ...,  0.2540,  0.1627,  0.7631],\n",
            "        [ 0.1377,  0.3856,  0.6060,  ...,  0.7089,  0.2763,  0.2866],\n",
            "        [ 0.5271, -0.4796,  0.4135,  ...,  0.3287,  0.1340,  0.4361]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-165.4919)\n",
            "\n",
            "Sentença 3: Bom Dia, professor. - tensor([[-0.2387, -0.2844,  0.3606,  ...,  0.9577, -0.2327,  0.2957],\n",
            "        [ 0.8188, -0.5643,  0.6965,  ..., -0.1356,  0.0236, -0.3090],\n",
            "        [ 0.9349, -1.0496,  0.1645,  ...,  0.1721, -0.3351, -0.5991],\n",
            "        [ 0.0104, -0.4793,  0.0501,  ...,  0.5897, -0.1399,  0.1081]])\n",
            "Soma embedding Sentença3: Bom Dia, professor. - tensor(-177.0387)\n",
            "\n",
            "Sentença 4: Vai cair tudo na prova? - tensor([[ 0.7876, -0.1169,  0.7632,  ..., -0.1344,  0.1030, -0.0097],\n",
            "        [ 0.2106,  1.0353,  0.0259,  ..., -0.2075, -0.0469, -0.3289],\n",
            "        [ 0.7594,  0.7997,  0.2487,  ...,  0.1881,  0.1629, -0.0855],\n",
            "        [ 0.6700,  1.3208, -0.5643,  ...,  0.6387,  0.3624, -0.0947]])\n",
            "Soma embedding Sentença4: Vai cair tudo na prova? - tensor(-176.5751)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E_hSMahstso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8910867-35af-4c77-92a7-de437344818d"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaPermutado = tokenizer.tokenize(sentenca1Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1TokenizadaPermutado)\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca1Permutado, tokenizer)\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaPermutado = tokenizer.tokenize(sentenca2Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2TokenizadaPermutado)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca2Permutado, tokenizer)\n",
        "print(\"\\nSentença 2 Permutada=\\\"\", sentenca2Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca2Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca2Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaPermutado = tokenizer.tokenize(sentenca3Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3TokenizadaPermutado)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca3Permutado, tokenizer)\n",
        "print(\"\\nSentença 3 Permutada=\\\"\", sentenca3Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca3Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca3Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaPermutado = tokenizer.tokenize(sentenca4Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4TokenizadaPermutado)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca4Permutado, tokenizer)\n",
        "print(\"\\nSentença 4 Permutada=\\\"\", sentenca4Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Permutado))\n"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 4096])\n",
            "    Soma embeddings:  -291.17\n",
            "\n",
            "Sentença 2 Permutada=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 4096])\n",
            "    Soma embeddings:  -249.21\n",
            "\n",
            "Sentença 3 Permutada=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 14 e término em 18\n",
            "    Formato modelo : torch.Size([5, 4096])\n",
            "    Soma embeddings:  -217.20\n",
            "\n",
            "Sentença 4 Permutada=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 19 e término em 24\n",
            "    Formato modelo : torch.Size([6, 4096])\n",
            "    Soma embeddings:  -265.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CsH6v4Dstso"
      },
      "source": [
        "## Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p5iSl9Nstso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07bd047f-abe7-426f-b991-95f55ebe80dc"
      },
      "source": [
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaOriginal)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Original))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embeddingSentenca4Original[:4]))\n",
        "\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaPermutado)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Permutado))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embeddingSentenca1Permutado[:4]))"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 4096])\n",
            "    Soma embeddings:  -288.69\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.4339, -0.7168,  0.1173,  ...,  0.3840, -0.0936,  0.1355],\n",
            "        [-0.6806, -1.0194, -0.0765,  ...,  0.2201,  0.2731, -0.0384],\n",
            "        [ 1.1384, -0.8541,  0.8992,  ...,  0.5247,  0.2278,  0.0653],\n",
            "        [ 0.9206, -0.9862,  0.1347,  ...,  0.4393, -0.4792,  0.1743]])\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 4096])\n",
            "    Soma embeddings:  -291.17\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.4152, -0.7838, -0.0364,  ...,  0.4706, -0.1614,  0.1815],\n",
            "        [-0.6012, -1.1043, -0.2660,  ...,  0.1239,  0.2729, -0.0574],\n",
            "        [ 1.2319, -1.1194,  0.9353,  ...,  0.5062,  0.1518,  0.1078],\n",
            "        [ 0.8861, -0.9666,  0.1831,  ...,  0.5239, -0.4849,  0.1787]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX-s-jyF9UFT"
      },
      "source": [
        "## Similaridade de cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSSZT6m9UFT"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCosseno(sentenca1, sentenca2):\n",
        "  similaridade = cosine(sentenca1, sentenca2)\n",
        "  return similaridade"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2xmrGvd9UFT"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbyuNamR9UFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0da67b3d-a8e6-4cba-94be-e8e3b62aafd2"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>) \n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosOriginal = float(somaScos)/float(n-1)\n",
        "print(\"Ccos Original:\", CcosOriginal)  \n"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.1821711262067159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WUjUZw_9UFT"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unc7fF7i9UFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e5f470-2c1f-4efc-81d3-0fe8fa02c8da"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosPermutado = float(somaScos)/float(np-1)\n",
        "print(\"Ccos Original:\", CcosPermutado)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.22398316860198975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPygSiX_9UFT"
      },
      "source": [
        "### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 1.\n",
        "- Documentos com sentenças diferenntes resulta uma medida menor que 1.\n",
        "- Documento com sentenças muito diferentes apresentam valores menores que 1.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de similaridade.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zD7FTPz9UFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f918963b-1d69-4f16-ab88-fc45c54a7768"
      },
      "source": [
        "print(\"Ccos Original :\", CcosOriginal)\n",
        "print(\"Ccos Permutado:\", CcosPermutado)\n",
        "\n",
        "if (CcosOriginal > CcosPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as sentenças!\")"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos Original : 0.1821711262067159\n",
            "Ccos Permutado: 0.22398316860198975\n",
            "Documento Permutado tem menor similaridade de cosseno entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVcRGoM09UFU"
      },
      "source": [
        "## Distância euclidiana entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância L2 ou norma L2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqK7DcTJ9UFU"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distanciaEuclidiana(sentenca1, sentenca2):\n",
        "  distancia = euclidean(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtzn3Ll69UFU"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McejBc0P9UFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1e614e8-c1e8-410c-a617-1a88acf333d1"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Distância euclidiana entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)  \n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucOriginal = float(somaSeuc)/float(n-1)\n",
        "print(\"Ceuc Original:\", CeucOriginal)  \n"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 19.390050888061523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7_eRCMV9UFU"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBEbKqiH9UFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6aeb812-2c05-4039-a186-f3b8f8211d40"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Distância euclidiana entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>) \n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucPermutado = float(somaSeuc)/float(np-1)\n",
        "print(\"Ceuc Original:\", CeucPermutado)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 21.618199666341145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5qzdOVy9UFU"
      },
      "source": [
        "### Compara as médias da distância euclidiana dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnBVeDrv9UFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6155512e-ba1a-4025-f468-a3dbb3107457"
      },
      "source": [
        "print(\"Ceuc Original :\", CeucOriginal)\n",
        "print(\"Ceuc Permutado:\", CeucPermutado)\n",
        "\n",
        "if (CeucOriginal < CeucPermutado):\n",
        "    print(\"Documento original tem menor distância euclidiana entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância euclidiana entre as sentenças!\")"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ceuc Original : 19.390050888061523\n",
            "Ceuc Permutado: 21.618199666341145\n",
            "Documento original tem menor distância euclidiana entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojaQA2C49UFV"
      },
      "source": [
        "## Distância Manhattan entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "\n",
        "Igual a subtração absoluta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4VBoLbH9UFV"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def distanciaManhattan(sentenca1, sentenca2):\n",
        "  distancia = cityblock(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN96DPuI9UFV"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyAvO6WV9UFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fd3df41-710f-492e-cc18-dcd117443057"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Distância de manhattan entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>) \n",
        "    Sman = distanciaManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanOriginal = float(somaSman)/float(n-1)\n",
        "print(\"Cman Original:\", CmanOriginal)  \n"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Cman Original: 939.0954182942709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oenwZkqO9UFV"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg81Wnik9UFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc57b96-1beb-4501-9286-e87df5c77e2e"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Distância de manhattan entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanPermutado = float(somaSman)/float(np-1)\n",
        "print(\"Ceuc Original:\", CmanPermutado)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 1060.0675455729167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcTxjUcc9UFV"
      },
      "source": [
        "### Compara as médias da distância de manhattan dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JzGE3he9UFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a17bc7b-500b-4d66-e14d-f4ce5b0c4d0d"
      },
      "source": [
        "print(\"Cman Original :\", CmanOriginal)\n",
        "print(\"Cman Permutado:\", CmanPermutado)\n",
        "\n",
        "if (CmanOriginal > CmanPermutado):\n",
        "    print(\"Documento original tem menor distância de manhattan entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância de manhattan entre as sentenças!\")"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cman Original : 939.0954182942709\n",
            "Cman Permutado: 1060.0675455729167\n",
            "Documento Permutado tem maior distância de manhattan entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaWJ6lU1a3RN"
      },
      "source": [
        "## Resumo\n",
        "\n",
        "Resultado das medidas utilizando as quatro últimas camadas do BERT.\n",
        "\n",
        "\n",
        "Base(MEAN):\n",
        "- Ccos       :   0.62004846          0.54099079\n",
        "- Ceuc       :   7.11709849          7.99353107\n",
        "- Cman       :   153.63694255          171.98441060\n",
        "\n",
        "Base(MAX):\n",
        "- Ccos       :   0.79404344          0.76070702\n",
        "- Ceuc       :   10.07102553          10.70145098\n",
        "- Cman       :   213.39687602          231.29823303\n",
        "\n",
        "Base(MEAN):\n",
        "- Ccos       :   0.80771943          0.77137093\n",
        "- Ceuc       :   20.71080144          23.16955884\n",
        "- Cman       :   883.51291911          983.28851318"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhAP-gfja3RN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe682a9-c8a4-4bdf-8939-97f53c34f49b"
      },
      "source": [
        "print(\"Resultado das medidas utilizando as quatro últimas camada do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print(\"Ccos       :   {:.8f}          {:.8f}\".format(CcosOriginal,CcosPermutado))\n",
        "print(\"Ceuc       :   {:.8f}          {:.8f}\".format(CeucOriginal,CeucPermutado))\n",
        "print(\"Cman       :   {:.8f}          {:.8f}\".format(CmanOriginal,CmanPermutado))"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado das medidas utilizando as quatro últimas camada do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Ccos       :   0.18217113          0.22398317\n",
            "Ceuc       :   19.39005089          21.61819967\n",
            "Cman       :   939.09541829          1060.06754557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJKcCTMPL341"
      },
      "source": [
        "# 9 - Exemplo sentenças de documento original e permutado utilizando embedding da concatenação das 4 últimas camadas do BERT usando estratégia MAX e todas as palavras.\n",
        "\n",
        "Como estamos utilizando os embeddings concatenado das 4 últimas camadas onde ocorre 768 entenda-se 3072 que é o resultado de 768 por 4 que é a dimensão do MCL BERT de tamanho base. E onde ocorre 1024 entenda-se 4096 que é o resultado de 1024 por 4 que é a dimensão do MCL BERT de tamanho large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC_lpJhbM0iL"
      },
      "source": [
        "## Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzLY--ZCM0iS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c627964-c31c-4a25-90c0-c3cbca5a21f4"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_original = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documentoOriginalConcatenado = \" \".join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + documentoOriginalConcatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWF6G0EqM0iT"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPokJjkHM0iT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fa24297-980c-4795-c34c-9a4378d1715a"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvGb4Lr9M0iT"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x23ik0ehM0iU"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpDIoOYgM0iU"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda todas as camadas da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mGvXQN2M0iU"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VajVWPxSM0iU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76f6862d-7396-4ab4-d9ff-b58b70829578"
      },
      "source": [
        "# Cria uma lista com os tensores a serem concatenados\n",
        "# Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "# Lista com os tensores a serem concatenados\n",
        "listaConcat = []\n",
        "# Percorre os 4 últimos\n",
        "for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "# Realiza a concatenação dos embeddings de todos as camadas\n",
        "# Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "# Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "\n",
        "print (\"O vetor da  concatenação das 4 últimas camadas oculta tem o formato:\", concat4_hidden_states.size())"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da  concatenação das 4 últimas camadas oculta tem o formato: torch.Size([1, 26, 4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f-UKTRuM0iU"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ue_2PX2M0iV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f643a9ee-ed09-47e9-c0de-5c1e9a508dc3"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoOriginal = torch.squeeze(concat4_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", embeddingDocumentoOriginal.size())"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([26, 4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCwadAdmM0iV"
      },
      "source": [
        "Confirmando vetores dependentes do condocumento\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb8Cm77zM0iV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "455b7a04-3cd0-4de0-f9e8-020ee8dba8bf"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGjE2y_pM0iV"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W28pu2G9M0iV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaaa02b4-0847-4f22-d5ff-2ae70a639bfc"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "embeddingSentenca1Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca1Original, tokenizer)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca2Original, tokenizer)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca3Original, tokenizer)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca4Original, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento original.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1Original,\"-\", str(embeddingSentenca1Original[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1Original,\"-\", str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2Original,\"-\", str(embeddingSentenca2Original[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2Original,\"-\", str(torch.sum(embeddingSentenca2Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3Original,\"-\", str(embeddingSentenca3Original[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3Original,\"-\", str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4Original,\"-\", str(embeddingSentenca4Original[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4Original,\"-\", str(torch.sum(embeddingSentenca4Original[:4])))"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: Bom Dia, professor. - tensor([[-0.5098, -0.2877,  0.0873,  ...,  0.9288, -0.0796,  0.3684],\n",
            "        [ 0.6078, -0.8869,  0.3736,  ...,  0.0124,  0.0094, -0.1719],\n",
            "        [ 0.9075, -1.1233, -0.0093,  ...,  0.1359, -0.3067, -0.4078],\n",
            "        [ 0.2499, -0.4717, -0.1217,  ...,  0.4604,  0.0075,  0.1555]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-176.3423)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.3987, -0.9450,  0.1785,  ...,  0.8476, -0.0572,  0.0365],\n",
            "        [ 0.2067, -0.2705,  0.7145,  ...,  0.2113,  0.2899,  0.5135],\n",
            "        [ 0.2355,  0.2686,  0.5669,  ...,  0.7205,  0.2510,  0.0924],\n",
            "        [ 0.5264, -0.4600,  0.4810,  ...,  0.1881,  0.0931,  0.1731]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-165.1487)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[ 0.5178,  0.0863,  0.7394,  ..., -0.3301, -0.0115,  0.1168],\n",
            "        [ 0.1617,  1.1516, -0.0350,  ..., -0.2189,  0.1096, -0.3660],\n",
            "        [ 0.9251,  0.5806,  0.2491,  ...,  0.0027,  0.2467, -0.1535],\n",
            "        [ 0.5782,  1.3571, -0.5161,  ...,  0.5285,  0.2553, -0.0197]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-177.0387)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[ 0.4339, -0.7168,  0.1173,  ...,  0.3840, -0.0936,  0.1355],\n",
            "        [-0.6806, -1.0194, -0.0765,  ...,  0.2201,  0.2731, -0.0384],\n",
            "        [ 1.1384, -0.8541,  0.8992,  ...,  0.5247,  0.2278,  0.0653],\n",
            "        [ 0.9206, -0.9862,  0.1347,  ...,  0.4393, -0.4792,  0.1743]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-169.8891)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDAG_xYHM0iV"
      },
      "source": [
        "Examinando os embeddings do documento original\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN1ZnWWLM0iW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e351cb8-44dc-4249-bb5e-c033d489b470"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaOriginal = tokenizer.tokenize(sentenca1Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1TokenizadaOriginal)\n",
        "embeddingSentenca1Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca1Original, tokenizer)\n",
        "print(\"\\nSentença 1 Original=\\\"\", sentenca1Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaOriginal = tokenizer.tokenize(sentenca2Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2TokenizadaOriginal)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca2Original, tokenizer)\n",
        "print(\"\\nSentença 2 Original=\\\"\", sentenca2Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca2Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca2Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaOriginal = tokenizer.tokenize(sentenca3Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3TokenizadaOriginal)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca3Original, tokenizer)\n",
        "print(\"\\nSentença 3 Original=\\\"\", sentenca3Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca3Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca3Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaOriginal = tokenizer.tokenize(sentenca4Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4TokenizadaOriginal)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca4Original, tokenizer)\n",
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Original))\n"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 4096])\n",
            "    Soma embeddings:  -225.64\n",
            "\n",
            "Sentença 2 Original=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 4096])\n",
            "    Soma embeddings:  -252.06\n",
            "\n",
            "Sentença 3 Original=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 4096])\n",
            "    Soma embeddings:  -260.65\n",
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 4096])\n",
            "    Soma embeddings:  -288.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWXVLJ1bM0iW"
      },
      "source": [
        "## Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPWjgUWeM0iW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84537b4-1d46-4c91-92b4-8759015ce969"
      },
      "source": [
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_permutado = [documento_original[3],   # \"Aguardo uma resposta, João.\",\n",
        "             documento_original[1],             # \"Qual o conteúdo da prova?\",              \n",
        "             documento_original[0],             # \"Vai cair tudo na prova?\",\n",
        "             documento_original[2]]             # \"Bom Dia, professor.\"]     \n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documentoPermutadoConcatenado = \" \".join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + documentoPermutadoConcatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 Bom           8,399\n",
            " 15 Dia           3,616\n",
            " 16 ,               117\n",
            " 17 professor     2,917\n",
            " 18 .               119\n",
            " 19 Vai          20,805\n",
            " 20 cair          9,322\n",
            " 21 tudo          2,745\n",
            " 22 na              229\n",
            " 23 prova         2,310\n",
            " 24 ?               136\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN2heM1aM0iW"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjakxc4xM0iW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c6dc14-cf61-4f9c-c5bc-77e1e39c37fa"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkJeVNRgM0iW"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67NUmOkIM0iX"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TIWhsT3M0iX"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda todas as camadas da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07xtO-rnM0iX"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvV4gue4M0iX"
      },
      "source": [
        "Recupera a saída e concatena as 4 últimas camada do BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svq_3keEM0iX"
      },
      "source": [
        "# Cria uma lista com os tensores a serem concatenados\n",
        "# Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "# Lista com os tensores a serem concatenados\n",
        "listaConcat = []\n",
        "# Percorre os 4 últimos\n",
        "for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "# Realiza a concatenação dos embeddings de todos as camadas\n",
        "# Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "# Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  "
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybtNemHbM0iX"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSav8HKIM0iX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a7cafa0-d28f-4c76-cd87-cdc187e3c479"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoPermutado = torch.squeeze(concat4_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", embeddingDocumentoPermutado.size())"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([26, 4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2EqEuq6M0iY"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_hCQhZnM0iY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f903b735-e3c8-43db-9b77-4d68204d6766"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca1Permutado, tokenizer)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca2Permutado, tokenizer)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca3Permutado, tokenizer)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca4Permutado, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento permutado.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1Permutado,\"-\", str(embeddingSentenca1Permutado[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1Original,\"-\", str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2Permutado,\"-\", str(embeddingSentenca2Permutado[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2Permutado,\"-\", str(torch.sum(embeddingSentenca2Permutado[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3Permutado,\"-\", str(embeddingSentenca3Permutado[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3Permutado,\"-\", str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4Permutado,\"-\", str(embeddingSentenca4Permutado[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4Permutado,\"-\", str(torch.sum(embeddingSentenca4Permutado[:4])))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[ 0.4152, -0.7838, -0.0364,  ...,  0.4706, -0.1614,  0.1815],\n",
            "        [-0.6012, -1.1043, -0.2660,  ...,  0.1239,  0.2729, -0.0574],\n",
            "        [ 1.2319, -1.1194,  0.9353,  ...,  0.5062,  0.1518,  0.1078],\n",
            "        [ 0.8861, -0.9666,  0.1831,  ...,  0.5239, -0.4849,  0.1787]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-176.3423)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.2957, -1.0942, -0.0024,  ...,  0.6032, -0.1615,  0.4615],\n",
            "        [ 0.2193, -0.4914,  0.7233,  ...,  0.2540,  0.1627,  0.7631],\n",
            "        [ 0.1377,  0.3856,  0.6060,  ...,  0.7089,  0.2763,  0.2866],\n",
            "        [ 0.5271, -0.4796,  0.4135,  ...,  0.3287,  0.1340,  0.4361]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-165.4919)\n",
            "\n",
            "Sentença 3: Bom Dia, professor. - tensor([[-0.2387, -0.2844,  0.3606,  ...,  0.9577, -0.2327,  0.2957],\n",
            "        [ 0.8188, -0.5643,  0.6965,  ..., -0.1356,  0.0236, -0.3090],\n",
            "        [ 0.9349, -1.0496,  0.1645,  ...,  0.1721, -0.3351, -0.5991],\n",
            "        [ 0.0104, -0.4793,  0.0501,  ...,  0.5897, -0.1399,  0.1081]])\n",
            "Soma embedding Sentença3: Bom Dia, professor. - tensor(-177.0387)\n",
            "\n",
            "Sentença 4: Vai cair tudo na prova? - tensor([[ 0.7876, -0.1169,  0.7632,  ..., -0.1344,  0.1030, -0.0097],\n",
            "        [ 0.2106,  1.0353,  0.0259,  ..., -0.2075, -0.0469, -0.3289],\n",
            "        [ 0.7594,  0.7997,  0.2487,  ...,  0.1881,  0.1629, -0.0855],\n",
            "        [ 0.6700,  1.3208, -0.5643,  ...,  0.6387,  0.3624, -0.0947]])\n",
            "Soma embedding Sentença4: Vai cair tudo na prova? - tensor(-176.5751)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXJIz3jPM0iY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e693a8e3-2c18-4d07-ba22-08b91183f4ea"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaPermutado = tokenizer.tokenize(sentenca1Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1TokenizadaPermutado)\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca1Permutado, tokenizer)\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaPermutado = tokenizer.tokenize(sentenca2Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2TokenizadaPermutado)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca2Permutado, tokenizer)\n",
        "print(\"\\nSentença 2 Permutada=\\\"\", sentenca2Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca2Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca2Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaPermutado = tokenizer.tokenize(sentenca3Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3TokenizadaPermutado)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca3Permutado, tokenizer)\n",
        "print(\"\\nSentença 3 Permutada=\\\"\", sentenca3Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca3Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca3Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaPermutado = tokenizer.tokenize(sentenca4Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4TokenizadaPermutado)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca4Permutado, tokenizer)\n",
        "print(\"\\nSentença 4 Permutada=\\\"\", sentenca4Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Permutado))\n"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 4096])\n",
            "    Soma embeddings:  -291.17\n",
            "\n",
            "Sentença 2 Permutada=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 4096])\n",
            "    Soma embeddings:  -249.21\n",
            "\n",
            "Sentença 3 Permutada=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 14 e término em 18\n",
            "    Formato modelo : torch.Size([5, 4096])\n",
            "    Soma embeddings:  -217.20\n",
            "\n",
            "Sentença 4 Permutada=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 19 e término em 24\n",
            "    Formato modelo : torch.Size([6, 4096])\n",
            "    Soma embeddings:  -265.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyOCD7COM0iY"
      },
      "source": [
        "## Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ucrFp95M0iY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fec4058-34b9-4664-e936-16db05938dd7"
      },
      "source": [
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaOriginal)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Original))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embeddingSentenca4Original[:4]))\n",
        "\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaPermutado)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Permutado))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embeddingSentenca1Permutado[:4]))"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 4096])\n",
            "    Soma embeddings:  -288.69\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.4339, -0.7168,  0.1173,  ...,  0.3840, -0.0936,  0.1355],\n",
            "        [-0.6806, -1.0194, -0.0765,  ...,  0.2201,  0.2731, -0.0384],\n",
            "        [ 1.1384, -0.8541,  0.8992,  ...,  0.5247,  0.2278,  0.0653],\n",
            "        [ 0.9206, -0.9862,  0.1347,  ...,  0.4393, -0.4792,  0.1743]])\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 4096])\n",
            "    Soma embeddings:  -291.17\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.4152, -0.7838, -0.0364,  ...,  0.4706, -0.1614,  0.1815],\n",
            "        [-0.6012, -1.1043, -0.2660,  ...,  0.1239,  0.2729, -0.0574],\n",
            "        [ 1.2319, -1.1194,  0.9353,  ...,  0.5062,  0.1518,  0.1078],\n",
            "        [ 0.8861, -0.9666,  0.1831,  ...,  0.5239, -0.4849,  0.1787]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJLbGeGl9rXA"
      },
      "source": [
        "## Similaridade de cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txsn241y9rXA"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCosseno(sentenca1, sentenca2):\n",
        "  similaridade = cosine(sentenca1, sentenca2)\n",
        "  return similaridade"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5MH4W1E9rXA"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q30qFwSb9rXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8f447ea-fb15-4af2-8424-0b1fc60e63bb"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosOriginal = float(somaScos)/float(n-1)\n",
        "print(\"Ccos Original:\", CcosOriginal)  \n"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.18527722358703613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZLT9K6r9rXA"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF5f2DSb9rXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcd33a10-7d94-4321-a2ef-973ea71a3357"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosPermutado = float(somaScos)/float(np-1)\n",
        "print(\"Ccos Original:\", CcosPermutado)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.20632920662562051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIKvfbWo9rXB"
      },
      "source": [
        "### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 1.\n",
        "- Documentos com sentenças diferenntes resulta uma medida menor que 1.\n",
        "- Documento com sentenças muito diferentes apresentam valores menores que 1.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de similaridade.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCITgUD59rXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b92648fe-737f-47ea-ed2b-ab85879f2ba3"
      },
      "source": [
        "print(\"Ccos Original :\", CcosOriginal)\n",
        "print(\"Ccos Permutado:\", CcosPermutado)\n",
        "\n",
        "if (CcosOriginal > CcosPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as sentenças!\")"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos Original : 0.18527722358703613\n",
            "Ccos Permutado: 0.20632920662562051\n",
            "Documento Permutado tem menor similaridade de cosseno entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poXedMfZ9rXB"
      },
      "source": [
        "## Distância euclidiana entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância L2 ou norma L2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgMLv7Xr9rXB"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distanciaEuclidiana(sentenca1, sentenca2):\n",
        "  distancia = euclidean(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFcLYcv_9rXB"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLPkoJU49rXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c35da630-59c7-4421-fb86-4f8ee01eb869"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucOriginal = float(somaSeuc)/float(n-1)\n",
        "print(\"Ceuc Original:\", CeucOriginal)  \n"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 27.702423095703125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYF1Bm-g9rXB"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1rSfTES9rXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04879e60-35f4-437d-a1df-ba0facc70d15"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucPermutado = float(somaSeuc)/float(np-1)\n",
        "print(\"Ceuc Original:\", CeucPermutado)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 29.813589731852215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_kqtJxd9rXC"
      },
      "source": [
        "### Compara as médias da distância euclidiana dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC4IlKjS9rXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee0f97a-4218-4c30-e7b8-68641f493bc0"
      },
      "source": [
        "print(\"Ceuc Original :\", CeucOriginal)\n",
        "print(\"Ceuc Permutado:\", CeucPermutado)\n",
        "\n",
        "if (CeucOriginal < CeucPermutado):\n",
        "    print(\"Documento original tem menor distância euclidiana entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância euclidiana entre as sentenças!\")"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ceuc Original : 27.702423095703125\n",
            "Ceuc Permutado: 29.813589731852215\n",
            "Documento original tem menor distância euclidiana entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ron62mX39rXC"
      },
      "source": [
        "## Distância Manhattan entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "\n",
        "\n",
        "Igual a subtração absoluta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU8WUSNJ9rXC"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def distanciaManhattan(sentenca1, sentenca2):\n",
        "  distancia = cityblock(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp7lYFkQ9rXC"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZsETQjN9rXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11773b6f-9179-4f36-9ff8-3c1621f08c7d"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj)) \n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanOriginal = float(somaSman)/float(n-1)\n",
        "print(\"Cman Original:\", CmanOriginal)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Cman Original: 1287.9195149739583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hraWMnO49rXC"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1lpFQpW9rXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799ae551-3f47-42ea-ab22-8c86d34f4207"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj)) \n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanPermutado = float(somaSman)/float(np-1)\n",
        "print(\"Ceuc Original:\", CmanPermutado)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 1404.3882649739583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW9oqevL9rXD"
      },
      "source": [
        "### Compara as médias da distância de manhattan dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E71-ic6l9rXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5d09088-0edd-4656-8937-ad5ab2ac6948"
      },
      "source": [
        "print(\"Cman Original :\", CmanOriginal)\n",
        "print(\"Cman Permutado:\", CmanPermutado)\n",
        "\n",
        "if (CmanOriginal < CmanPermutado):\n",
        "    print(\"Documento original tem menor distância de manhattan entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância de manhattan entre as sentenças!\")"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cman Original : 1287.9195149739583\n",
            "Cman Permutado: 1404.3882649739583\n",
            "Documento original tem menor distância de manhattan entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqVSa_XBL35H"
      },
      "source": [
        "## Resumo\n",
        "\n",
        "Resultado das medidas utilizando as quatro últimas camadas do BERT.\n",
        "\n",
        "Base(MEAN):\n",
        "- Ccos       :   0.62004846          0.54099079\n",
        "- Ceuc       :   7.11709849          7.99353107\n",
        "- Cman       :   153.63694255          171.98441060\n",
        "\n",
        "Base(MAX):\n",
        "- Ccos       :   0.79404344          0.76070702\n",
        "- Ceuc       :   10.07102553          10.70145098\n",
        "- Cman       :   213.39687602          231.29823303\n",
        "\n",
        "Base(MEAN):\n",
        "- Ccos       :   0.80771943          0.77137093\n",
        "- Ceuc       :   20.71080144          23.16955884\n",
        "- Cman       :   883.51291911          983.28851318\n",
        "\n",
        "Base(MAX):\n",
        "- Ccos       :   0.83094325          0.80360138\n",
        "- Ceuc       :   27.86794090          30.03283564\n",
        "- Cman       :   1175.89337158          1273.93892415"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yac6Etu7L35I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66856045-29a0-4fcb-8d75-b4c7f301826b"
      },
      "source": [
        "print(\"Resultado das medidas utilizando a última camada do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print(\"Ccos       :   {:.8f}          {:.8f}\".format(CcosOriginal,CcosPermutado))\n",
        "print(\"Ceuc       :   {:.8f}          {:.8f}\".format(CeucOriginal,CeucPermutado))\n",
        "print(\"Cman       :   {:.8f}          {:.8f}\".format(CmanOriginal,CmanPermutado))"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado das medidas utilizando a última camada do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Ccos       :   0.18527722          0.20632921\n",
            "Ceuc       :   27.70242310          29.81358973\n",
            "Cman       :   1287.91951497          1404.38826497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsBQPbuNsFmj"
      },
      "source": [
        "# 10 - Exemplo sentenças de documento original e permutado utilizando embedding da última camada do BERT usando a estratégia MEAN e palavras relevantes(CLEAN - Stopword)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19AATfMisFmo"
      },
      "source": [
        "## Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU9FaJm4sFmo",
        "outputId": "e8a3e823-02d0-4e2d-ca15-5207e9e7181e"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_original = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documentoOriginalConcatenado = \" \".join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + documentoOriginalConcatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGAC2fxisFmr"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRDaFwzosFmr",
        "outputId": "537be6cc-a1e0-4e90-a18a-640034e8f41f"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2Ky0UDDsFms"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p30SEmzsFms"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtpDq1ACsFmt"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX-Rty6PsFmt"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdxPC0A8sFmt"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVj5fIjjsFmu",
        "outputId": "da4b389d-6d38-4b3b-ad5c-9d6089c646b0"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJrBKm9DsFmu"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zauxa9psFmu",
        "outputId": "2d7359bc-cb7b-4534-e0b7-e6e6b8d7ab34"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoOriginal = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", embeddingDocumentoOriginal.size())"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([26, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBreB1FdsFmv"
      },
      "source": [
        "Confirmando vetores dependentes do condocumento\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp6eo0DCsFmv",
        "outputId": "27a4f05f-9597-46d4-f787-ce0c684ac11f"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMxG57QasFmw"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfWmF0eOsFmw",
        "outputId": "150d50dc-423e-4c0b-e11b-bb090575a354"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "embeddingSentenca1Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca1Original, tokenizer)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca2Original, tokenizer)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca3Original, tokenizer)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca4Original, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento original.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1Original,\"-\", str(embeddingSentenca1Original[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1Original,\"-\", str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2Original,\"-\", str(embeddingSentenca2Original[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2Original,\"-\", str(torch.sum(embeddingSentenca2Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3Original,\"-\", str(embeddingSentenca3Original[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3Original,\"-\", str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4Original,\"-\", str(embeddingSentenca4Original[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4Original,\"-\", str(torch.sum(embeddingSentenca4Original[:4])))"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: Bom Dia, professor. - tensor([[-0.5098, -0.2877,  0.0873,  ...,  0.5436, -0.9302,  0.4668],\n",
            "        [ 0.6078, -0.8869,  0.3736,  ..., -0.3517, -1.2140, -0.3077],\n",
            "        [ 0.9075, -1.1233, -0.0093,  ...,  0.4433, -0.4633, -0.0113],\n",
            "        [ 0.2499, -0.4717, -0.1217,  ...,  0.7079, -0.2300,  0.4911]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-113.4554)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.3987, -0.9450,  0.1785,  ...,  0.7189, -0.6772, -0.1452],\n",
            "        [ 0.2067, -0.2705,  0.7145,  ...,  0.3047, -0.2718,  0.7577],\n",
            "        [ 0.2355,  0.2686,  0.5669,  ...,  1.0817,  0.5614,  0.3750],\n",
            "        [ 0.5264, -0.4600,  0.4810,  ..., -0.5559, -0.2941,  0.0378]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-115.8800)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[ 0.5178,  0.0863,  0.7394,  ..., -0.5765, -0.6208, -0.2230],\n",
            "        [ 0.1617,  1.1516, -0.0350,  ...,  0.1730,  0.2104, -0.0207],\n",
            "        [ 0.9251,  0.5806,  0.2491,  ...,  0.1687,  0.0772, -0.1173],\n",
            "        [ 0.5782,  1.3571, -0.5161,  ..., -0.0942,  0.0404, -0.0390]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-110.5299)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[ 0.4339, -0.7168,  0.1173,  ...,  0.0768, -1.2036,  0.3863],\n",
            "        [-0.6806, -1.0194, -0.0765,  ..., -0.1768, -0.6326,  0.2895],\n",
            "        [ 1.1384, -0.8541,  0.8992,  ...,  0.2798, -0.0381,  0.5061],\n",
            "        [ 0.9206, -0.9862,  0.1347,  ...,  0.4182, -1.0164,  0.4713]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-116.1903)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AT4hmrosFmw"
      },
      "source": [
        "Examinando os embeddings do documento original\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlNrBzHFsFmx",
        "outputId": "904a8775-d33f-4c13-b82e-6adc859cb75b"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaOriginal = tokenizer.tokenize(sentenca1Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1TokenizadaOriginal)\n",
        "embeddingSentenca1Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca1Original, tokenizer)\n",
        "print(\"\\nSentença 1 Original=\\\"\", sentenca1Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaOriginal = tokenizer.tokenize(sentenca2Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2TokenizadaOriginal)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca2Original, tokenizer)\n",
        "print(\"\\nSentença 2 Original=\\\"\", sentenca2Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca2Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca2Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaOriginal = tokenizer.tokenize(sentenca3Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3TokenizadaOriginal)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca3Original, tokenizer)\n",
        "print(\"\\nSentença 3 Original=\\\"\", sentenca3Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca3Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca3Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaOriginal = tokenizer.tokenize(sentenca4Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4TokenizadaOriginal)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca4Original, tokenizer)\n",
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Original))\n"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 1024])\n",
            "    Soma embeddings:  -141.45\n",
            "\n",
            "Sentença 2 Original=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -173.58\n",
            "\n",
            "Sentença 3 Original=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -167.58\n",
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -200.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWsYpv_bsFmx"
      },
      "source": [
        "## Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e-102FSsFmx",
        "outputId": "9a2e0d58-2fb2-43d9-8602-c962d6bd9f75"
      },
      "source": [
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_permutado = [documento_original[3],   # \"Aguardo uma resposta, João.\",\n",
        "             documento_original[1],             # \"Qual o conteúdo da prova?\",              \n",
        "             documento_original[0],             # \"Vai cair tudo na prova?\",\n",
        "             documento_original[2]]             # \"Bom Dia, professor.\"]     \n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documentoPermutadoConcatenado = \" \".join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + documentoPermutadoConcatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 Bom           8,399\n",
            " 15 Dia           3,616\n",
            " 16 ,               117\n",
            " 17 professor     2,917\n",
            " 18 .               119\n",
            " 19 Vai          20,805\n",
            " 20 cair          9,322\n",
            " 21 tudo          2,745\n",
            " 22 na              229\n",
            " 23 prova         2,310\n",
            " 24 ?               136\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoWi9iDGsFmy",
        "outputId": "8c2bb1f4-0a62-45b4-de4b-852439ce64f9"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUf--8eJsFmy"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPXl21tpsFmz"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx2aMCAcsFmz"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLkXV0DvsFmz"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eRSCaP4sFm0"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUMEhXZjsFm0",
        "outputId": "6721467a-4ca3-4440-fd03-f1cf2c6cf8cd"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAOnH5jBsFm1"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzMYCJr5sFm1",
        "outputId": "7d41764c-4f7e-471e-a4bf-47e751fde905"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoPermutado = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", embeddingDocumentoPermutado.size())"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([26, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E_mRhwVsFm1"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QEGPIF7sFm2",
        "outputId": "7aa24eea-a829-43fc-eb42-53afaac75fa6"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca1Permutado, tokenizer)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca2Permutado, tokenizer)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca3Permutado, tokenizer)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca4Permutado, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento permutado.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1Permutado,\"-\", str(embeddingSentenca1Permutado[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1Original,\"-\", str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2Permutado,\"-\", str(embeddingSentenca2Permutado[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2Permutado,\"-\", str(torch.sum(embeddingSentenca2Permutado[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3Permutado,\"-\", str(embeddingSentenca3Permutado[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3Permutado,\"-\", str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4Permutado,\"-\", str(embeddingSentenca4Permutado[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4Permutado,\"-\", str(torch.sum(embeddingSentenca4Permutado[:4])))"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[ 0.4152, -0.7838, -0.0364,  ...,  0.2686, -1.3504,  0.4707],\n",
            "        [-0.6012, -1.1043, -0.2660,  ..., -0.1943, -0.8112,  0.2180],\n",
            "        [ 1.2319, -1.1194,  0.9353,  ...,  0.2953, -0.1973,  0.5069],\n",
            "        [ 0.8861, -0.9666,  0.1831,  ...,  0.4595, -1.1045,  0.5615]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-113.4554)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.2957, -1.0942, -0.0024,  ...,  0.5875, -0.7164,  0.0104],\n",
            "        [ 0.2193, -0.4914,  0.7233,  ...,  0.3551, -0.3698,  0.8577],\n",
            "        [ 0.1377,  0.3856,  0.6060,  ...,  1.2662,  0.4965,  0.5211],\n",
            "        [ 0.5271, -0.4796,  0.4135,  ..., -0.5303, -0.3934,  0.1571]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-115.7949)\n",
            "\n",
            "Sentença 3: Bom Dia, professor. - tensor([[-0.2387, -0.2844,  0.3606,  ...,  0.6976, -0.9028,  0.3216],\n",
            "        [ 0.8188, -0.5643,  0.6965,  ..., -0.1440, -0.7018, -0.1933],\n",
            "        [ 0.9349, -1.0496,  0.1645,  ...,  0.4782, -0.3837, -0.2850],\n",
            "        [ 0.0104, -0.4793,  0.0501,  ...,  0.7341, -0.3687,  0.4562]])\n",
            "Soma embedding Sentença3: Bom Dia, professor. - tensor(-110.5299)\n",
            "\n",
            "Sentença 4: Vai cair tudo na prova? - tensor([[ 7.8763e-01, -1.1689e-01,  7.6317e-01,  ..., -4.3840e-01,\n",
            "         -6.5442e-01, -6.2304e-02],\n",
            "        [ 2.1062e-01,  1.0353e+00,  2.5935e-02,  ...,  7.1822e-02,\n",
            "          1.2368e-01,  1.1607e-03],\n",
            "        [ 7.5943e-01,  7.9969e-01,  2.4871e-01,  ...,  1.8793e-01,\n",
            "          9.4661e-02, -9.1650e-02],\n",
            "        [ 6.7001e-01,  1.3208e+00, -5.6431e-01,  ..., -6.3590e-03,\n",
            "          1.6016e-01, -1.4379e-01]])\n",
            "Soma embedding Sentença4: Vai cair tudo na prova? - tensor(-110.7603)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kehz0vDBsFm2",
        "outputId": "5bc3e4f4-872a-4991-ef5c-253cefa73f22"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaPermutado = tokenizer.tokenize(sentenca1Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1TokenizadaPermutado)\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca1Permutado, tokenizer)\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaPermutado = tokenizer.tokenize(sentenca2Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2TokenizadaPermutado)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca2Permutado, tokenizer)\n",
        "print(\"\\nSentença 2 Permutada=\\\"\", sentenca2Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca2Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca2Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaPermutado = tokenizer.tokenize(sentenca3Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3TokenizadaPermutado)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca3Permutado, tokenizer)\n",
        "print(\"\\nSentença 3 Permutada=\\\"\", sentenca3Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca3Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca3Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaPermutado = tokenizer.tokenize(sentenca4Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4TokenizadaPermutado)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca4Permutado, tokenizer)\n",
        "print(\"\\nSentença 4 Permutada=\\\"\", sentenca4Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Permutado))\n"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -199.90\n",
            "\n",
            "Sentença 2 Permutada=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -173.06\n",
            "\n",
            "Sentença 3 Permutada=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 14 e término em 18\n",
            "    Formato modelo : torch.Size([5, 1024])\n",
            "    Soma embeddings:  -141.81\n",
            "\n",
            "Sentença 4 Permutada=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 19 e término em 24\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -167.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvUnrHFysFm2"
      },
      "source": [
        "## Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRXHN3O-sFm2",
        "outputId": "a99bc46f-66ce-40d5-a2e0-f557125889dd"
      },
      "source": [
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaOriginal)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Original))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embeddingSentenca4Original[:4]))\n",
        "\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaPermutado)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Permutado))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embeddingSentenca1Permutado[:4]))"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -200.72\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.4339, -0.7168,  0.1173,  ...,  0.0768, -1.2036,  0.3863],\n",
            "        [-0.6806, -1.0194, -0.0765,  ..., -0.1768, -0.6326,  0.2895],\n",
            "        [ 1.1384, -0.8541,  0.8992,  ...,  0.2798, -0.0381,  0.5061],\n",
            "        [ 0.9206, -0.9862,  0.1347,  ...,  0.4182, -1.0164,  0.4713]])\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -199.90\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.4152, -0.7838, -0.0364,  ...,  0.2686, -1.3504,  0.4707],\n",
            "        [-0.6012, -1.1043, -0.2660,  ..., -0.1943, -0.8112,  0.2180],\n",
            "        [ 1.2319, -1.1194,  0.9353,  ...,  0.2953, -0.1973,  0.5069],\n",
            "        [ 0.8861, -0.9666,  0.1831,  ...,  0.4595, -1.1045,  0.5615]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvXgM6N1sFm3"
      },
      "source": [
        "## Similaridade de cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKzuxFWJsFm3"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCosseno(sentenca1, sentenca2):\n",
        "  similaridade = cosine(sentenca1, sentenca2)\n",
        "  return similaridade"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDib4hLGsFm3"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHXaYmvGsFm3",
        "outputId": "dd96527b-b030-42b0-9cb5-3ed913c363f3"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer, filtro=\"CLEAN\")        \n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosOriginal = float(somaScos)/float(n-1)\n",
        "print(\"Ccos Original:\", CcosOriginal)  \n"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.22540746132532755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeDBXHS7sFm4"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEUtq3GXsFm4",
        "outputId": "c41ad147-14a3-401b-d846-ae76ad69d14f"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "   # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosPermutado = float(somaScos)/float(np-1)\n",
        "print(\"Ccos Original:\", CcosPermutado)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.28482739130655926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj7h64hJsFm4"
      },
      "source": [
        "### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 1.\n",
        "- Documentos com sentenças diferenntes resulta uma medida menor que 1.\n",
        "- Documento com sentenças muito diferentes apresentam valores menores que 1.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de similaridade.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQLa6sp9sFm4",
        "outputId": "131653d3-8fd7-440a-ae41-9386f5403af9"
      },
      "source": [
        "print(\"Ccos Original :\", CcosOriginal)\n",
        "print(\"Ccos Permutado:\", CcosPermutado)\n",
        "\n",
        "if (CcosOriginal > CcosPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as sentenças!\")"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos Original : 0.22540746132532755\n",
            "Ccos Permutado: 0.28482739130655926\n",
            "Documento Permutado tem menor similaridade de cosseno entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjvPrHXZsFm5"
      },
      "source": [
        "## Distância euclidiana entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância L2 ou norma L2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL_xEvs-sFm5"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distanciaEuclidiana(sentenca1, sentenca2):\n",
        "  distancia = euclidean(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqFVlwDBsFm5"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3uj0JMvsFm5",
        "outputId": "fd04bf88-a9e2-4088-b0d0-5063a2f462c9"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer, filtro=\"CLEAN\")        \n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucOriginal = float(somaSeuc)/float(n-1)\n",
        "print(\"Ceuc Original:\", CeucOriginal)  \n"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 11.496897061665853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv5VUtoMsFm5"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QsCz48XsFm6",
        "outputId": "5d5775cb-a0d8-4d66-de8d-bae7c90e2db6"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucPermutado = float(somaSeuc)/float(np-1)\n",
        "print(\"Ceuc Original:\", CeucPermutado)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 13.097944895426432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWtkCP0dsFm6"
      },
      "source": [
        "### Compara as médias da distância euclidiana dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SISdwB4RsFm6",
        "outputId": "28016de7-cf6e-4afd-89e3-677ceb6d2c7e"
      },
      "source": [
        "print(\"Ceuc Original :\", CeucOriginal)\n",
        "print(\"Ceuc Permutado:\", CeucPermutado)\n",
        "\n",
        "if (CeucOriginal < CeucPermutado):\n",
        "    print(\"Documento original tem menor distância euclidiana entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância euclidiana entre as sentenças!\")"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ceuc Original : 11.496897061665853\n",
            "Ceuc Permutado: 13.097944895426432\n",
            "Documento original tem menor distância euclidiana entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln6IuKWfsFm6"
      },
      "source": [
        "## Distância Manhattan entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "\n",
        "Igual subtração absoluta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJSt1k0esFm7"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def distanciaManhattan(sentenca1, sentenca2):\n",
        "  distancia = cityblock(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBO2UR1IsFm7"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxrMfVszsFm7",
        "outputId": "0db23c8b-32a7-430c-cef9-d29cd04ca498"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer, filtro=\"CLEAN\")        \n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanOriginal = float(somaSman)/float(n-1)\n",
        "print(\"Cman Original:\", CmanOriginal)  \n"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Cman Original: 282.4735107421875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2POGH7msFm7"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ji9F1JOsFm7",
        "outputId": "af175a46-2e4b-4e46-f03b-5f720f2d7901"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanPermutado = float(somaSman)/float(np-1)\n",
        "print(\"Ceuc Original:\", CmanPermutado)"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 319.11614990234375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF2QrnIVsFm8"
      },
      "source": [
        "### Compara as médias da distância de manhattan dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gCvmLXqsFm8",
        "outputId": "c13bce85-8152-440c-c4b0-e131df5311b2"
      },
      "source": [
        "print(\"Cman Original :\", CmanOriginal)\n",
        "print(\"Cman Permutado:\", CmanPermutado)\n",
        "\n",
        "if (CmanOriginal < CmanPermutado):\n",
        "    print(\"Documento original tem menor distância de manhattan entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância de manhattan entre as sentenças!\")"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cman Original : 282.4735107421875\n",
            "Cman Permutado: 319.11614990234375\n",
            "Documento original tem menor distância de manhattan entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5rbxNyXsFm8"
      },
      "source": [
        "## Resumo\n",
        "\n",
        "Resultado das medidas utilizando a última camada do BERT.\n",
        "\n",
        "Base(MEAN):\n",
        "- Ccos       :   0.67999101          0.69653825\n",
        "- Ceuc       :   6.34085274          6.15486606\n",
        "- Cman       :   136.53579712          132.14489237\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh4_TmjNsFm8",
        "outputId": "209d3174-df53-42bb-accd-07c5601cd79f"
      },
      "source": [
        "print(\"Resultado das medidas utilizando a última camada do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print(\"Ccos       :   {:.8f}          {:.8f}\".format(CcosOriginal,CcosPermutado))\n",
        "print(\"Ceuc       :   {:.8f}          {:.8f}\".format(CeucOriginal,CeucPermutado))\n",
        "print(\"Cman       :   {:.8f}          {:.8f}\".format(CmanOriginal,CmanPermutado))"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado das medidas utilizando a última camada do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Ccos       :   0.22540746          0.28482739\n",
            "Ceuc       :   11.49689706          13.09794490\n",
            "Cman       :   282.47351074          319.11614990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQSCarFgl9j5"
      },
      "source": [
        "# 11 - Exemplo sentenças de documento original e permutado utilizando embedding da última camada do BERT usando a estratégia MEAN com palavras relavantes(NOUN-Substantivos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFVHojw1l9j9"
      },
      "source": [
        "## Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qeR3y4Cl9j9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f5b0ad-772b-4721-d285-873255882342"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_original = [\"O que é Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documentoOriginalConcatenado = \" \".join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + documentoOriginalConcatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 O               231\n",
            "  2 que             179\n",
            "  3 é               253\n",
            "  4 Bom           8,399\n",
            "  5 Dia           3,616\n",
            "  6 ,               117\n",
            "  7 professor     2,917\n",
            "  8 .               119\n",
            "  9 Qual         13,082\n",
            " 10 o               146\n",
            " 11 conteúdo      5,015\n",
            " 12 da              180\n",
            " 13 prova         2,310\n",
            " 14 ?               136\n",
            " 15 Vai          20,805\n",
            " 16 cair          9,322\n",
            " 17 tudo          2,745\n",
            " 18 na              229\n",
            " 19 prova         2,310\n",
            " 20 ?               136\n",
            " 21 Agu           8,125\n",
            " 22 ##ardo        2,222\n",
            " 23 uma             230\n",
            " 24 resposta      4,299\n",
            " 25 ,               117\n",
            " 26 João          1,453\n",
            " 27 .               119\n",
            " 28 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_HEZhCOl9j-"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMvf5JR9l9j-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461f3a73-3b46-4d11-94ba-611e446387b3"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpYRgyDKl9j-"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSkH4nStl9j_"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGIWW2frl9j_"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4UA1jqxl9j_"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rpP0syul9j_"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EumV200sl9kA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1e0626b-ba92-4da5-a5c1-b67a74b470a9"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 29, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8UQ-CIIl9kA"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWdHs89nl9kA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2542df10-f1dc-4329-a356-4b5322754179"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoOriginal = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", embeddingDocumentoOriginal.size())"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([29, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4Zflk6Fl9kA"
      },
      "source": [
        "Confirmando vetores dependentes do condocumento\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkjJEjmll9kA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc211eb-ab20-4dfc-e8d3-381670b6c2fe"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [CLS]\n",
            "1 O\n",
            "2 que\n",
            "3 é\n",
            "4 Bom\n",
            "5 Dia\n",
            "6 ,\n",
            "7 professor\n",
            "8 .\n",
            "9 Qual\n",
            "10 o\n",
            "11 conteúdo\n",
            "12 da\n",
            "13 prova\n",
            "14 ?\n",
            "15 Vai\n",
            "16 cair\n",
            "17 tudo\n",
            "18 na\n",
            "19 prova\n",
            "20 ?\n",
            "21 Agu\n",
            "22 ##ardo\n",
            "23 uma\n",
            "24 resposta\n",
            "25 ,\n",
            "26 João\n",
            "27 .\n",
            "28 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku6SY2qBl9kB"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0X1HZeil9kB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e322ec12-b8d7-4176-ef13-fde57415d6b4"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "embeddingSentenca1Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca1Original, tokenizer)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca2Original, tokenizer)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca3Original, tokenizer)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca4Original, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento original.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1Original,\"-\", str(embeddingSentenca1Original[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1Original,\"-\", str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2Original,\"-\", str(embeddingSentenca2Original[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2Original,\"-\", str(torch.sum(embeddingSentenca2Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3Original,\"-\", str(embeddingSentenca3Original[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3Original,\"-\", str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4Original,\"-\", str(embeddingSentenca4Original[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4Original,\"-\", str(torch.sum(embeddingSentenca4Original[:4])))"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: O que é Bom Dia, professor. - tensor([[ 0.0959, -0.2270, -0.1118,  ...,  0.2721, -0.2466, -0.0210],\n",
            "        [ 1.2365, -0.5517,  0.6621,  ..., -0.1528, -0.0894, -0.2182],\n",
            "        [ 0.9527, -0.2890, -0.0094,  ...,  0.1436, -0.2898, -0.0972],\n",
            "        [ 0.2908,  0.3894,  0.1951,  ...,  0.7773, -0.4434,  0.2160]])\n",
            "Soma embedding Sentença1: O que é Bom Dia, professor. - tensor(-112.8717)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.2835, -1.0831, -0.1362,  ...,  0.5261, -0.5203, -0.2719],\n",
            "        [ 0.1944, -0.3802,  0.6350,  ...,  0.1018, -0.2393,  0.5174],\n",
            "        [ 0.2776,  0.2497,  0.5277,  ...,  1.0712,  0.4269,  0.1689],\n",
            "        [ 0.6002, -0.4322,  0.3809,  ..., -0.7250, -0.4373, -0.1360]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-116.6605)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[ 0.4871,  0.2863,  0.7100,  ..., -0.5872, -0.5978, -0.3000],\n",
            "        [ 0.1895,  1.2702,  0.0154,  ...,  0.1025,  0.1821, -0.0459],\n",
            "        [ 0.8765,  0.7753,  0.2558,  ...,  0.0639,  0.0988, -0.2345],\n",
            "        [ 0.5971,  1.4270, -0.5191,  ..., -0.1405,  0.0417, -0.0900]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-110.7973)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[ 0.3572, -0.7941,  0.2145,  ...,  0.2267, -1.3031,  0.5656],\n",
            "        [-0.7110, -0.9771, -0.1139,  ..., -0.0588, -0.6391,  0.4080],\n",
            "        [ 1.1714, -0.8507,  0.8752,  ...,  0.0221, -0.2051,  0.5171],\n",
            "        [ 0.9509, -0.8061,  0.0883,  ...,  0.3120, -1.1729,  0.4734]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-116.4401)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uat3GKd1l9kB"
      },
      "source": [
        "Examinando os embeddings do documento original\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w88VPPW8l9kB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "482dd793-9be4-42f3-a297-c35029bf9888"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaOriginal = tokenizer.tokenize(sentenca1Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1TokenizadaOriginal)\n",
        "embeddingSentenca1Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca1Original, tokenizer)\n",
        "print(\"\\nSentença 1 Original=\\\"\", sentenca1Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaOriginal = tokenizer.tokenize(sentenca2Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2TokenizadaOriginal)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca2Original, tokenizer)\n",
        "print(\"\\nSentença 2 Original=\\\"\", sentenca2Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca2Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca2Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaOriginal = tokenizer.tokenize(sentenca3Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3TokenizadaOriginal)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca3Original, tokenizer)\n",
        "print(\"\\nSentença 3 Original=\\\"\", sentenca3Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca3Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca3Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaOriginal = tokenizer.tokenize(sentenca4Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4TokenizadaOriginal)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca4Original, tokenizer)\n",
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Original))\n"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original: ['O que é Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=\" O que é Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['O', 'que', 'é', 'Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 8\n",
            "    Formato modelo : torch.Size([8, 1024])\n",
            "    Soma embeddings:  -224.26\n",
            "\n",
            "Sentença 2 Original=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 9 e término em 14\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -174.23\n",
            "\n",
            "Sentença 3 Original=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 15 e término em 20\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -168.12\n",
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 21 e término em 27\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -200.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyIawL-gl9kB"
      },
      "source": [
        "## Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m30dkANxl9kB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31db70b9-9466-4a2a-b7d0-90750766b034"
      },
      "source": [
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_permutado = [documento_original[3],   # \"Aguardo uma resposta, João.\",\n",
        "             documento_original[1],             # \"Qual o conteúdo da prova?\",              \n",
        "             documento_original[0],             # \"Vai cair tudo na prova?\",\n",
        "             documento_original[2]]             # \"Bom Dia, professor.\"]     \n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documentoPermutadoConcatenado = \" \".join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + documentoPermutadoConcatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 O               231\n",
            " 15 que             179\n",
            " 16 é               253\n",
            " 17 Bom           8,399\n",
            " 18 Dia           3,616\n",
            " 19 ,               117\n",
            " 20 professor     2,917\n",
            " 21 .               119\n",
            " 22 Vai          20,805\n",
            " 23 cair          9,322\n",
            " 24 tudo          2,745\n",
            " 25 na              229\n",
            " 26 prova         2,310\n",
            " 27 ?               136\n",
            " 28 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Muz4w-9ol9kC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f4a859-4520-4eeb-a608-0f94634462c7"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laIJK9b3l9kC"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu5tQwZMl9kC"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14LzqtKbl9kC"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhLV2Leel9kC"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwA1d4C6l9kC"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjWjud21l9kD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7839e1cf-fa3d-4821-fcf2-b4ef13689616"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 29, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiiNFe8ul9kD"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GeLFhGJl9kD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea3f2a7-1ac7-4b63-d20a-c2e9483bd2a6"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoPermutado = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", embeddingDocumentoPermutado.size())"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([29, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0d5gJKEl9kD"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfdswxTHl9kD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e62e58e7-91ef-46e1-c8e2-572667955e04"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca1Permutado, tokenizer)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca2Permutado, tokenizer)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca3Permutado, tokenizer)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca4Permutado, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento permutado.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1Permutado,\"-\", str(embeddingSentenca1Permutado[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1Original,\"-\", str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2Permutado,\"-\", str(embeddingSentenca2Permutado[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2Permutado,\"-\", str(torch.sum(embeddingSentenca2Permutado[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3Permutado,\"-\", str(embeddingSentenca3Permutado[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3Permutado,\"-\", str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4Permutado,\"-\", str(embeddingSentenca4Permutado[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4Permutado,\"-\", str(torch.sum(embeddingSentenca4Permutado[:4])))"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[ 0.3008, -0.8352,  0.0859,  ...,  0.3506, -1.4184,  0.6198],\n",
            "        [-0.7313, -1.0660, -0.1632,  ..., -0.1582, -0.8945,  0.4686],\n",
            "        [ 1.2001, -0.9558,  0.9364,  ...,  0.1588, -0.1159,  0.4672],\n",
            "        [ 0.9480, -0.8754,  0.2510,  ...,  0.3583, -1.0402,  0.5724]])\n",
            "Soma embedding Sentença1: O que é Bom Dia, professor. - tensor(-112.8717)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.1748, -1.2124, -0.0957,  ...,  0.8515, -0.7926, -0.1893],\n",
            "        [ 0.3634, -0.6917,  0.5889,  ...,  0.1342, -0.3014,  0.5190],\n",
            "        [ 0.3582,  0.3534,  0.3681,  ...,  1.3254,  0.2636,  0.1875],\n",
            "        [ 0.6097, -0.5928,  0.4144,  ..., -0.5881, -0.4897,  0.0071]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-115.8209)\n",
            "\n",
            "Sentença 3: O que é Bom Dia, professor. - tensor([[ 0.4201, -0.0428,  0.0921,  ..., -0.0947, -0.0346, -0.0147],\n",
            "        [ 1.2881, -0.0552,  0.9424,  ..., -0.4693,  0.0064, -0.2187],\n",
            "        [ 0.9519,  0.2317,  0.0453,  ..., -0.0338, -0.2452, -0.2208],\n",
            "        [ 0.3083,  1.0520,  0.2322,  ...,  0.8689, -0.3859,  0.2730]])\n",
            "Soma embedding Sentença3: O que é Bom Dia, professor. - tensor(-110.7973)\n",
            "\n",
            "Sentença 4: Vai cair tudo na prova? - tensor([[ 0.8320,  0.0908,  0.6936,  ..., -0.4201, -0.6960, -0.0789],\n",
            "        [ 0.2403,  1.1361,  0.0807,  ...,  0.1224,  0.1228,  0.0684],\n",
            "        [ 0.7999,  0.8793,  0.2468,  ...,  0.1440,  0.0347, -0.0522],\n",
            "        [ 0.6384,  1.5777, -0.5727,  ..., -0.0267,  0.1563, -0.1752]])\n",
            "Soma embedding Sentença4: Vai cair tudo na prova? - tensor(-110.9498)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufyOj-3Ql9kD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff1f4e7-37e1-467c-da3b-e6612c11c125"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaPermutado = tokenizer.tokenize(sentenca1Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1TokenizadaPermutado)\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca1Permutado, tokenizer)\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaPermutado = tokenizer.tokenize(sentenca2Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2TokenizadaPermutado)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca2Permutado, tokenizer)\n",
        "print(\"\\nSentença 2 Permutada=\\\"\", sentenca2Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca2Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca2Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaPermutado = tokenizer.tokenize(sentenca3Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3TokenizadaPermutado)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca3Permutado, tokenizer)\n",
        "print(\"\\nSentença 3 Permutada=\\\"\", sentenca3Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca3Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca3Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaPermutado = tokenizer.tokenize(sentenca4Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4TokenizadaPermutado)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, sentenca4Permutado, tokenizer)\n",
        "print(\"\\nSentença 4 Permutada=\\\"\", sentenca4Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaPermutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Permutado))\n"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'O que é Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -199.95\n",
            "\n",
            "Sentença 2 Permutada=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -173.07\n",
            "\n",
            "Sentença 3 Permutada=\" O que é Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['O', 'que', 'é', 'Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 14 e término em 21\n",
            "    Formato modelo : torch.Size([8, 1024])\n",
            "    Soma embeddings:  -224.24\n",
            "\n",
            "Sentença 4 Permutada=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 22 e término em 27\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -167.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dceUXyTOl9kE"
      },
      "source": [
        "## Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL9sCL3vl9kE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "836390cc-032d-4b9a-8737-641e7ff3b894"
      },
      "source": [
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaOriginal)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Original))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embeddingSentenca4Original[:4]))\n",
        "\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1Permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaPermutado)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Permutado))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embeddingSentenca1Permutado[:4]))"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -200.64\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.3572, -0.7941,  0.2145,  ...,  0.2267, -1.3031,  0.5656],\n",
            "        [-0.7110, -0.9771, -0.1139,  ..., -0.0588, -0.6391,  0.4080],\n",
            "        [ 1.1714, -0.8507,  0.8752,  ...,  0.0221, -0.2051,  0.5171],\n",
            "        [ 0.9509, -0.8061,  0.0883,  ...,  0.3120, -1.1729,  0.4734]])\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -199.95\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.3008, -0.8352,  0.0859,  ...,  0.3506, -1.4184,  0.6198],\n",
            "        [-0.7313, -1.0660, -0.1632,  ..., -0.1582, -0.8945,  0.4686],\n",
            "        [ 1.2001, -0.9558,  0.9364,  ...,  0.1588, -0.1159,  0.4672],\n",
            "        [ 0.9480, -0.8754,  0.2510,  ...,  0.3583, -1.0402,  0.5724]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jESgdoQ8l9kM"
      },
      "source": [
        "## Similaridade de cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cf1Y9arl9kM"
      },
      "source": [
        "def similaridadeCosseno(documento1, documento2):\n",
        "    \"\"\"\n",
        "    Similaridade do cosseno dos embeddgins dos documentos.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `documento1` - Um documento a ser medido.           \n",
        "    `documento2` - Um documento a ser medido.                 \n",
        "    \"\"\"\n",
        "    \n",
        "    similaridade = cosine(documento1, documento2)\n",
        "    \n",
        "    return similaridade"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NVHFoysl9kM"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbUGxtIHl9kM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f377980-6bfd-4c04-b448-554bb487049a"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosOriginal = float(somaScos)/float(n-1)\n",
        "print(\"Ccos Original:\", CcosOriginal)  \n"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['O que é Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.3446566164493561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh3HGUMIl9kM"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ7qnwBRl9kM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d042ce7c-7c39-48e4-8aa1-75da8b50edd6"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "   # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosPermutado = float(somaScos)/float(np-1)\n",
        "print(\"Ccos Original:\", CcosPermutado)"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'O que é Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.43422337373097736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYHLZrgIl9kN"
      },
      "source": [
        "### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 1.\n",
        "- Documentos com sentenças diferenntes resulta uma medida menor que 1.\n",
        "- Documento com sentenças muito diferentes apresentam valores menores que 1.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de similaridade.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeIb45RQl9kN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8037f0-50b9-43c2-adc4-511161bbac52"
      },
      "source": [
        "print(\"Ccos Original :\", CcosOriginal)\n",
        "print(\"Ccos Permutado:\", CcosPermutado)\n",
        "\n",
        "if (CcosOriginal > CcosPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as sentenças!\")"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos Original : 0.3446566164493561\n",
            "Ccos Permutado: 0.43422337373097736\n",
            "Documento Permutado tem menor similaridade de cosseno entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RdcWF9Tl9kN"
      },
      "source": [
        "## Distância euclidiana entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância L2 ou norma L2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL4gMjoll9kN"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distanciaEuclidiana(documento1, documento2):\n",
        "    \"\"\"\n",
        "    Distância euclidiana entre os embeddings dos documentos.\n",
        "    Possui outros nomes como distância L2 ou norma L2.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `documento1` - Um documento a ser medido.           \n",
        "    `documento2` - Um documento a ser medido.\n",
        "    \"\"\"\n",
        "    \n",
        "    distancia = euclidean(documento1, documento2)\n",
        "    \n",
        "    return distancia"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHA388-ql9kN"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr5jWF8Il9kN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93edefd9-6e21-4117-b664-550218fbf2e2"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer, filtro=\"NOUN\")        \n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucOriginal = float(somaSeuc)/float(n-1)\n",
        "print(\"Ceuc Original:\", CeucOriginal)  \n"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['O que é Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 17.477886199951172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpu9QRx4l9kO"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhLznobil9kO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a68426df-59eb-452a-8949-42a88156fe2a"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucPermutado = float(somaSeuc)/float(np-1)\n",
        "print(\"Ceuc Original:\", CeucPermutado)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'O que é Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 19.712294896443684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2G6XKN-l9kO"
      },
      "source": [
        "### Compara as médias da distância euclidiana dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4UFxZsVl9kO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026da52e-1e85-400c-c450-8d9bc25220e1"
      },
      "source": [
        "print(\"Ceuc Original :\", CeucOriginal)\n",
        "print(\"Ceuc Permutado:\", CeucPermutado)\n",
        "\n",
        "if (CeucOriginal < CeucPermutado):\n",
        "    print(\"Documento original tem menor distância euclidiana entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância euclidiana entre as sentenças!\")"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ceuc Original : 17.477886199951172\n",
            "Ceuc Permutado: 19.712294896443684\n",
            "Documento original tem menor distância euclidiana entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxAIaJell9kO"
      },
      "source": [
        "## Distância Manhattan entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "\n",
        "Igual subtração absoluta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtUNIARWl9kO"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def distanciaManhattan(embeddings1, embeddings2):\n",
        "    \"\"\"\n",
        "    Distância Manhattan entre os embeddings dos textos \n",
        "    Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `embeddings1` - Um embedding a ser medido.\n",
        "    `embeddings2` - Um embedding a ser medido.\n",
        "    \"\"\"\n",
        "    \n",
        "    distancia = cityblock(embeddings1, embeddings2)\n",
        "\n",
        "    return distancia"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKdd7j1wl9kO"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUjQCxv-l9kP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12aecc72-dd84-458f-9418-26e7f40551ca"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer, filtro=\"NOUN\")        \n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Sj, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanOriginal = float(somaSman)/float(n-1)\n",
        "print(\"Cman Original:\", CmanOriginal)  \n"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['O que é Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Cman Original: 429.46360270182294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jz_oV3yl9kP"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oUizCe-l9kP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "449f480e-2b25-4c1e-a932-80fcf4dd7fec"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Sj, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanPermutado = float(somaSman)/float(np-1)\n",
        "print(\"Ceuc Original:\", CmanPermutado)"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'O que é Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 480.89661661783856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3WYl6del9kP"
      },
      "source": [
        "### Compara as médias da distância de manhattan dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq5uNunkl9kP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c64a9f4d-e79c-4b9a-c5c7-91d5820c12ca"
      },
      "source": [
        "print(\"Cman Original :\", CmanOriginal)\n",
        "print(\"Cman Permutado:\", CmanPermutado)\n",
        "\n",
        "if (CmanOriginal < CmanPermutado):\n",
        "    print(\"Documento original tem menor distância de manhattan entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância de manhattan entre as sentenças!\")"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cman Original : 429.46360270182294\n",
            "Cman Permutado: 480.89661661783856\n",
            "Documento original tem menor distância de manhattan entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGqyDJIZl9kP"
      },
      "source": [
        "## Resumo\n",
        "\n",
        "Resultado das medidas utilizando a última camada do BERT.\n",
        "\n",
        "Base(MEAN):\n",
        "- Ccos       :   0.67999101          0.69653825\n",
        "- Ceuc       :   6.34085274          6.15486606\n",
        "- Cman       :   136.53579712          132.14489237\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEoyUyoUl9kQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ac99fb-13b3-4534-9b70-3b0b32c00417"
      },
      "source": [
        "print(\"Resultado das medidas utilizando a última camada do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print(\"Ccos       :   {:.8f}          {:.8f}\".format(CcosOriginal,CcosPermutado))\n",
        "print(\"Ceuc       :   {:.8f}          {:.8f}\".format(CeucOriginal,CeucPermutado))\n",
        "print(\"Cman       :   {:.8f}          {:.8f}\".format(CmanOriginal,CmanPermutado))"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado das medidas utilizando a última camada do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Ccos       :   0.34465662          0.43422337\n",
            "Ceuc       :   17.47788620          19.71229490\n",
            "Cman       :   429.46360270          480.89661662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKVnNNkQ1Fk4"
      },
      "source": [
        "# Comparando documentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVw9sDeX5Gdd"
      },
      "source": [
        "## Função para retornar a medida de coerência entre sentenças de um documento usando similaridade do coseno."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbA0L_PE1SS0"
      },
      "source": [
        "def getMedidaCoerenciaCosseno(documento, concat4=True, filtro=\"ALL\"):\n",
        "\n",
        "  # Concatena as sentenças do documento em uma string\n",
        "  stringDocumento = \" \".join(documento)\n",
        "\n",
        "  # Adiciona os tokens especiais\n",
        "  documento_marcado = \"[CLS] \" + stringDocumento + \" [SEP]\"\n",
        "\n",
        "  # Divide a sentença em tokens\n",
        "  documento_tokenizado = tokenizer.tokenize(documento_marcado)\n",
        "\n",
        "  # Mapeia os tokens em seus índices do vocabulário\n",
        "  documento_tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n",
        "\n",
        "  # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "  mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "  # Importa a bibliteca\n",
        "  import torch\n",
        "\n",
        "  # Converte as entradas de listas para tensores do torch\n",
        "  tokens_tensores = torch.as_tensor([documento_tokens_indexados])\n",
        "  mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "\n",
        "  # Prediz os atributos dos estados ocultos para cada camada\n",
        "  with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "  if concat4 == True:\n",
        "    # Cria uma lista com os tensores a serem concatenados\n",
        "    # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "    # Lista com os tensores a serem concatenados\n",
        "    listaConcat = []\n",
        "    # Percorre os 4 últimos\n",
        "    for i in [-1,-2,-3,-4]:\n",
        "        # Concatena da lista\n",
        "        listaConcat.append(outputs[2][i])\n",
        "        # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "        #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "    # Realiza a concatenação dos embeddings de todos as camadas\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "    concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "    # Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)\n",
        "\n",
        "    # Verifica se a primeira dimensão é igual 1 para remover a dimensão de lote \"batches\"\n",
        "    if concat4_hidden_states.shape[0] == 1:\n",
        "        # Usa o método \"squeeze\" para remover a primeira dimensão(0) pois possui tamanho 1\n",
        "        embeddingDocumento = torch.squeeze(concat4_hidden_states, dim=0)   \n",
        "\n",
        "  else:\n",
        "    # Recupera a última e única camada da saída\n",
        "    last_hidden_states = outputs[0]\n",
        "\n",
        "    # Verifica se a primeira dimensão é igual 1 para remover a dimensão de lote \"batches\"\n",
        "    if last_hidden_states.shape[0] == 1:\n",
        "        # Usa o método \"squeeze\" para remover a primeira dimensão(0) pois possui tamanho 1\n",
        "        embeddingDocumento = torch.squeeze(last_hidden_states, dim=0)   \n",
        "\n",
        "  # Quantidade de sentenças no documento\n",
        "  n = len(documento)\n",
        "\n",
        "  somaScos = 0\n",
        "\n",
        "  # Percorre as sentenças do documento\n",
        "  for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento[i]\n",
        "    Sj = documento[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumento, stringDocumento, Si, tokenizer, filtro=filtro)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumento, stringDocumento, Sj, tokenizer, filtro=filtro)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "  CcosOriginal = float(somaScos)/float(n-1)\n",
        "  return CcosOriginal"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCZF2v_H811O"
      },
      "source": [
        "## Exemplo 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp0FIK1V5LV2"
      },
      "source": [
        "## Comparando dois documentos distintos usando a medida de similaridade do cosseno de sentenças considerando todas(ALL) as palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_sJroEK1Me9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73bec4a0-03b0-4c3d-a3a5-961b668076cc"
      },
      "source": [
        "# Define um documento\n",
        "documento_1 = [\"Fui de carro.\",\n",
        "             \"O caminhão tombou.\",\n",
        "             \"O ônibus atrasou.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [\"O gato saiu correndo\",   \n",
        "             \"O cachorro latiu\",             \n",
        "             \"O passáro está no ninho.\"]             \n",
        "\n",
        "Ccos1 =  getMedidaCoerenciaCosseno(documento_1)\n",
        "Ccos2 =  getMedidaCoerenciaCosseno(documento_2)\n",
        "print(\"Ccos1:\", Ccos1)\n",
        "print(\"Ccos2:\", Ccos2)"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos1: 0.10913559794425964\n",
            "Ccos2: 0.14366477727890015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl4LrTEHcTk0"
      },
      "source": [
        "## Comparando dois documentos distintos usando a medida de similaridade do cosseno de sentenças e palavras relevantes (CLEAN - stopwords)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L98YJPQtcTk2",
        "outputId": "9b46a69e-4179-4711-8162-10f18e1a2744"
      },
      "source": [
        "# Define um documento\n",
        "documento_1 = [\"Fui de carro.\",\n",
        "             \"O caminhão tombou.\",              \n",
        "             \"O ônibus atrasou.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [\"O gato saiu correndo\",   \n",
        "             \"O cachorro latiu\",             \n",
        "             \"O passáro está no ninho.\"]             \n",
        "\n",
        "Ccos1 =  getMedidaCoerenciaCosseno(documento_1, filtro=\"CLEAN\")\n",
        "Ccos2 =  getMedidaCoerenciaCosseno(documento_2, filtro=\"CLEAN\")\n",
        "print(\"Ccos1:\", Ccos1)\n",
        "print(\"Ccos2:\", Ccos2)"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos1: 0.11006039381027222\n",
            "Ccos2: 0.15948572754859924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx1v76PJNQxz"
      },
      "source": [
        "## Comparando dois documentos distintos usando a medida de similaridade do cosseno de sentenças considerando palavras relevantes(NOUN - substantivos)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJhdhwZMNQxz",
        "outputId": "0efe45e8-d498-47be-d7ae-736121335286"
      },
      "source": [
        "# Define um documento\n",
        "documento_1 = [\"Fui de carro.\",\n",
        "             \"O caminhão tombou.\",              \n",
        "             \"O ônibus atrasou.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [\"O gato saiu correndo\",   \n",
        "             \"O cachorro latiu\",             \n",
        "             \"O passáro está no ninho.\"]             \n",
        "\n",
        "Ccos1 =  getMedidaCoerenciaCosseno(documento_1, filtro=\"NOUN\")\n",
        "Ccos2 =  getMedidaCoerenciaCosseno(documento_2, filtro=\"NOUN\")\n",
        "print(\"Ccos1:\", Ccos1)\n",
        "print(\"Ccos2:\", Ccos2)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos1: 0.2605067789554596\n",
            "Ccos2: 0.34332606196403503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9FVGMBU5aqK"
      },
      "source": [
        "## Comparando dois documentos distintos usando a medida de similaridade do cosseno de sentenças considerando todas(ALL) as palavras relevantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lFQfJmS4uCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1230a61-64da-48f8-c30e-245e4f8fee2f"
      },
      "source": [
        "# Define um documento\n",
        "documento_1 = [\"Fui de carro.\",\n",
        "             \"O caminhão tombou.\",              \n",
        "             \"O ônibus atrasou.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [\"O gato saiu correndo\",   \n",
        "             \"O cachorro latiu\",             \n",
        "             \"O jogo de futebol atrasou.\"]                         \n",
        "\n",
        "Ccos1 =  getMedidaCoerenciaCosseno(documento_1)\n",
        "Ccos2 =  getMedidaCoerenciaCosseno(documento_2)\n",
        "print(\"Ccos1:\", Ccos1)\n",
        "print(\"Ccos2:\", Ccos2)"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos1: 0.10913559794425964\n",
            "Ccos2: 0.12186715006828308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScpVYF5Kcwqa"
      },
      "source": [
        "## Comparando dois documentos distintos usando a medida de similaridade do cosseno de sentenças e palavras relevante (CLEAN-stopwords)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm-g4y8xcwqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e4580cc-fcd2-4c7b-83fd-71915228c787"
      },
      "source": [
        "# Define um documento\n",
        "documento_1 = [\"Fui de carro.\",\n",
        "             \"O caminhão tombou.\",              \n",
        "             \"O ônibus atrasou.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [\"O gato saiu correndo\",   \n",
        "             \"O cachorro latiu\",             \n",
        "             \"O jogo de futebol atrasou.\"]                         \n",
        "\n",
        "Ccos1 =  getMedidaCoerenciaCosseno(documento_1, filtro=\"CLEAN\")\n",
        "Ccos2 =  getMedidaCoerenciaCosseno(documento_2, filtro=\"CLEAN\")\n",
        "print(\"Ccos1:\", Ccos1)\n",
        "print(\"Ccos2:\", Ccos2)"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos1: 0.11006039381027222\n",
            "Ccos2: 0.12418749928474426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ2kKBHx88SI"
      },
      "source": [
        "## Exemplo 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBKnx0xW6OoR"
      },
      "source": [
        "## Comparando dois documentos com um contendo sentenças permutadas usando a medida de similaridade do cosseno de sentenças considerando todas(ALL) as palavras relevantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKooY6Gw5iUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b46b3b-4758-411a-d2d8-69d516da5578"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_1 = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [documento_1[3],       # \"Aguardo uma resposta, João.\",\n",
        "             documento_1[1],     # \"Qual o conteúdo da prova?\",              \n",
        "             documento_1[0],     # \"Bom Dia, professor.\",\n",
        "             documento_1[2]]     # \"Vai cair tudo na prova?\"]     \n",
        "\n",
        "Ccos1 =  getMedidaCoerenciaCosseno(documento_1)\n",
        "Ccos2 =  getMedidaCoerenciaCosseno(documento_2)\n",
        "print(\"Ccos1:\", Ccos1)\n",
        "print(\"Ccos2:\", Ccos2)"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos1: 0.1821711262067159\n",
            "Ccos2: 0.22398316860198975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4sUw9_gc8HM"
      },
      "source": [
        "## Comparando dois documentos com um contendo sentenças permutadas usando a medida de similaridade do cosseno de sentenças e palavras relevantes(CLEAN-stopwords)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhXGBZU2c8HN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a15f2a-4d57-4810-aa12-0df474bba25e"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_1 = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [documento_1[3],     # \"Aguardo uma resposta, João.\",\n",
        "           documento_1[1],     # \"Qual o conteúdo da prova?\",              \n",
        "           documento_1[0],     # \"Bom Dia, professor.\",\n",
        "           documento_1[2]]     # \"Vai cair tudo na prova?\"]     \n",
        "\n",
        "Ccos1 =  getMedidaCoerenciaCosseno(documento_1, filtro=\"CLEAN\")\n",
        "Ccos2 =  getMedidaCoerenciaCosseno(documento_2, filtro=\"CLEAN\")\n",
        "print(\"Ccos1:\", Ccos1)\n",
        "print(\"Ccos2:\", Ccos2)"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos1: 0.18990208705266318\n",
            "Ccos2: 0.2384093999862671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRunnvUyM5wn"
      },
      "source": [
        "## Comparando dois documentos com um contendo sentenças permutadas usando a medida de similaridade do cosseno de sentenças considerando as palavras relevantes(NOUN-substantivo)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHHszbUfM5wn",
        "outputId": "093880b3-3fe4-4f6b-e3ed-e5b7a4a6a67d"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_1 = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [documento_1[3],     # \"Aguardo uma resposta, João.\",\n",
        "           documento_1[1],     # \"Qual o conteúdo da prova?\",              \n",
        "           documento_1[0],     # \"Bom Dia, professor.\",\n",
        "           documento_1[2]]     # \"Vai cair tudo na prova?\"]   \n",
        "\n",
        "Ccos1 =  getMedidaCoerenciaCosseno(documento_1, filtro=\"NOUN\")\n",
        "Ccos2 =  getMedidaCoerenciaCosseno(documento_2, filtro=\"NOUN\")\n",
        "print(\"Ccos1:\", Ccos1)\n",
        "print(\"Ccos2:\", Ccos2)"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos1: 0.30666730801264447\n",
            "Ccos2: 0.38964641094207764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyLmnlBWZ6c5"
      },
      "source": [
        "# Comparando graficamente os embeddings gerados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDhlpbAajCGO"
      },
      "source": [
        "import matplotlib\n",
        "\n",
        "#TAMANHO_FONTE = 28\n",
        "#matplotlib.rc(\"font\", size=TAMANHO_FONTE)          # Controla o tamanho do do documento default\n",
        "#matplotlib.rc(\"axes\", titlesize=TAMANHO_FONTE)     # Tamanho da fonte do eixo do título\n",
        "#matplotlib.rc(\"axes\", labelsize=TAMANHO_FONTE)     # Tamanho da fonte dos rótulos do eixo x e y\n",
        "#matplotlib.rc(\"xtick\", labelsize=TAMANHO_FONTE)    # Tamanho da fonte das marcações do eixo y\n",
        "#matplotlib.rc(\"ytick\", labelsize=TAMANHO_FONTE)    # Tamanho da fonte dos marcações do eixo x\n",
        "#matplotlib.rc(\"legend\", fontsize=TAMANHO_FONTE-2)  # Tamanho da fonte da legenda\n",
        "#matplotlib.rc(\"figure\", titlesize=TAMANHO_FONTE + 4)   # Tamanho da fonte do título da figura"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHaf_DQceo3G"
      },
      "source": [
        "## Gerando embeddings das sentenças separadamente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOvAhXTjLXN_"
      },
      "source": [
        "### Calculando a similaridade com a primeira sentença"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsecc4RhSKPH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea4746d-c581-4db5-d4af-20fa863b4f20"
      },
      "source": [
        "# Import das biblioteca\n",
        "import pandas as pd\n",
        "\n",
        "# Define um documento com 4 sentenças\n",
        "documento_1 = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [documento_1[3],     # \"Aguardo uma resposta, João.\",\n",
        "           documento_1[1],     # \"Qual o conteúdo da prova?\",              \n",
        "           documento_1[0],     # \"Bom Dia, professor.\",\n",
        "           documento_1[2]]     # \"Vai cair tudo na prova?\"]   \n",
        "\n",
        "# Converte o documento em um dataframe\n",
        "df1 = pd.DataFrame(documento_1, columns = [\"sentenca\"])\n",
        "\n",
        "df2 = pd.DataFrame(documento_2, columns = [\"sentenca\"])"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-07-02 12:31:17,042 : INFO : NumExpr defaulting to 2 threads.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0N9Tf0ie4Vg"
      },
      "source": [
        "Gera os embeddings de cada sentença"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5fIzeEzUEOa"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Guarda os embeddings das sentenças\n",
        "matrix_embedding1 = []\n",
        "\n",
        "# Calcula a média dos embeddings das sentenças\n",
        "matrix_media_embedding1 = []\n",
        "\n",
        "for i,sentenca in enumerate(documento_1):\n",
        "    # Gera os embeddings da sentença utiliza a concatenação das 4 últimas camadas\n",
        "    embedding, tokens = getEmbeddingsConcat4UltimasCamadas(sentenca, model, tokenizer)    \n",
        "    # Guarda os embeddings da sentença\n",
        "    matrix_embedding1.append(embedding)\n",
        "    # Calcula a média dos embeddings dos tokens da sentença\n",
        "    media_embedding = torch.mean(embedding, dim=0)    \n",
        "    # Converte em um array numpy\n",
        "    media = media_embedding.numpy()\n",
        "    # Adiciona na lista\n",
        "    matrix_media_embedding1.append(media)"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsZ9OIyOe8HJ"
      },
      "source": [
        "Gera os embeddings de cada sentença"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB21PEwz7Ptg"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Guarda os embeddings das sentenças\n",
        "matrix_embedding2 = []\n",
        "\n",
        "# Calcula a média dos embeddings das sentenças\n",
        "matrix_media_embedding2 = []\n",
        "\n",
        "for i,sentenca in enumerate(documento_2):\n",
        "    # Gera os embeddings da sentença utiliza a concatenação das 4 últimas camadas\n",
        "    embedding, tokens = getEmbeddingsConcat4UltimasCamadas(sentenca, model, tokenizer)    \n",
        "    # Guarda os embeddings da sentença\n",
        "    matrix_embedding2.append(embedding)\n",
        "    # Calcula a média dos embeddings dos tokens da sentença\n",
        "    media_embedding = torch.mean(embedding, dim=0)    \n",
        "    # Converte em um array numpy\n",
        "    media = media_embedding.numpy()\n",
        "    # Adiciona na lista\n",
        "    matrix_media_embedding2.append(media)"
      ],
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcQ5yITSfBsG"
      },
      "source": [
        "Calcula a similaridade do cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_media_embedding1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvi8pFjrs562",
        "outputId": "e476d217-2257-4286-fb66-d09affbedc46"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([ 0.399776  , -0.3494937 ,  0.11349295, ...,  0.16037354,\n",
              "        -0.22572303, -0.08769296], dtype=float32),\n",
              " array([0.19280474, 0.16574818, 0.22165386, ..., 0.29237777, 0.0358055 ,\n",
              "        0.0914463 ], dtype=float32),\n",
              " array([ 0.52004486,  0.81783557,  0.08638554, ...,  0.20268512,\n",
              "         0.06256667, -0.11417088], dtype=float32),\n",
              " array([ 0.45807797, -0.3510454 ,  0.26197603, ...,  0.16521892,\n",
              "         0.06290239, -0.1561036 ], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "sOitleB8LXYI",
        "outputId": "b354cbcf-731e-45ff-9a8a-c73ca9a2150b"
      },
      "source": [
        "# Importa a biblioteca\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Coloca todos os embeddings de sentença em uma matriz\n",
        "embed_matrix1 = np.array([x for x in matrix_media_embedding1])\n",
        "embed_matrix2 = np.array([x for x in matrix_media_embedding2])\n",
        "\n",
        "# Calcula a similaridade do coseno entre as sentenças\n",
        "cos_matrix = cosine_similarity(embed_matrix1,embed_matrix2)\n",
        "\n",
        "# Coloca a similaridade para a primeira sentença\n",
        "df1[\"medida\"] = cos_matrix[0]\n",
        "\n",
        "df1"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      sentenca    medida\n",
              "0          Bom Dia, professor.  0.893253\n",
              "1    Qual o conteúdo da prova?  0.802693\n",
              "2      Vai cair tudo na prova?  1.000000\n",
              "3  Aguardo uma resposta, João.  0.804919"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95ce7a07-7084-45bc-bd9b-cbd2e64cd2bb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentenca</th>\n",
              "      <th>medida</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bom Dia, professor.</td>\n",
              "      <td>0.893253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Qual o conteúdo da prova?</td>\n",
              "      <td>0.802693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Vai cair tudo na prova?</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aguardo uma resposta, João.</td>\n",
              "      <td>0.804919</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95ce7a07-7084-45bc-bd9b-cbd2e64cd2bb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-95ce7a07-7084-45bc-bd9b-cbd2e64cd2bb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-95ce7a07-7084-45bc-bd9b-cbd2e64cd2bb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLL7ukdPosEn"
      },
      "source": [
        "### Mapa de calor calculado com a similaridade cosseno entre todas as sentenças gerados separadamente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "LxUCfzZFs9ls",
        "outputId": "1cdeb06e-8139-44b4-e2cd-370bb9c8ac19"
      },
      "source": [
        "# Cria o dataframe da lista com as sentenças como nome das colunas\n",
        "df1 = pd.DataFrame(cos_matrix,columns = documento_2)\n",
        "# Indexa pelas sentença do documento_1\n",
        "df1.index = documento_1\n",
        "df1"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             Aguardo uma resposta, João.  \\\n",
              "Bom Dia, professor.                             0.893253   \n",
              "Qual o conteúdo da prova?                       0.802668   \n",
              "Vai cair tudo na prova?                         0.807739   \n",
              "Aguardo uma resposta, João.                     1.000000   \n",
              "\n",
              "                             Qual o conteúdo da prova?  Bom Dia, professor.  \\\n",
              "Bom Dia, professor.                           0.802693             1.000000   \n",
              "Qual o conteúdo da prova?                     1.000000             0.802693   \n",
              "Vai cair tudo na prova?                       0.909448             0.804919   \n",
              "Aguardo uma resposta, João.                   0.802668             0.893253   \n",
              "\n",
              "                             Vai cair tudo na prova?  \n",
              "Bom Dia, professor.                         0.804919  \n",
              "Qual o conteúdo da prova?                   0.909448  \n",
              "Vai cair tudo na prova?                     1.000000  \n",
              "Aguardo uma resposta, João.                 0.807739  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c60bb8e-214d-4912-b846-ec7b80a9ef5a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Aguardo uma resposta, João.</th>\n",
              "      <th>Qual o conteúdo da prova?</th>\n",
              "      <th>Bom Dia, professor.</th>\n",
              "      <th>Vai cair tudo na prova?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Bom Dia, professor.</th>\n",
              "      <td>0.893253</td>\n",
              "      <td>0.802693</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.804919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Qual o conteúdo da prova?</th>\n",
              "      <td>0.802668</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.802693</td>\n",
              "      <td>0.909448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Vai cair tudo na prova?</th>\n",
              "      <td>0.807739</td>\n",
              "      <td>0.909448</td>\n",
              "      <td>0.804919</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Aguardo uma resposta, João.</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.802668</td>\n",
              "      <td>0.893253</td>\n",
              "      <td>0.807739</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c60bb8e-214d-4912-b846-ec7b80a9ef5a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3c60bb8e-214d-4912-b846-ec7b80a9ef5a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3c60bb8e-214d-4912-b846-ec7b80a9ef5a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "id": "havdK-ZliXAA",
        "outputId": "4f839afd-bf1b-4e37-8cdb-8e36d74758d9"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tamanho da figura\n",
        "fig, ax = plt.subplots(figsize=(18,12))\n",
        "\n",
        "# Cria o gráfico\n",
        "ax = sns.heatmap(cos_matrix, xticklabels=documento_2, yticklabels=documento_1, cbar_kws={\"label\": \"Medida de similaridade cosseno\"}, annot=True)\n",
        "\n",
        "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, horizontalalignment=\"right\")\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, horizontalalignment=\"right\")\n",
        "\n",
        "# Coloca o título da matriz\n",
        "ax.set_title(\"Similaridade do cosseno entre os embeddings das sentenças gerados separadamente\\n\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x864 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6n3U_D0aE6G"
      },
      "source": [
        "## Gerando embeddings das sentenças simultaneamente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMA19QFtZ4T0"
      },
      "source": [
        "### Calculando a similaridade com a primeira sentença"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCgWmOn9Z4T7"
      },
      "source": [
        "# Import das biblioteca\n",
        "import pandas as pd\n",
        "\n",
        "# Converte o documento em um dataframe\n",
        "df1 = pd.DataFrame(documento_1, columns = [\"sentenca\"])\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documentoOriginalConcatenado = \" \".join(documento_1)\n",
        "\n",
        "df2 = pd.DataFrame(documento_2, columns = [\"sentenca\"])\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documentoPermutadoConcatenado = \" \".join(documento_2)"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGGGPICufVVm"
      },
      "source": [
        "Gera os embeddings de cada sentença"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlzYvvUrfb-E"
      },
      "source": [
        "# Gerando embeddings dos documentos\n",
        "embeddingDocumentoOriginal, tokensOriginal = getEmbeddingsConcat4UltimasCamadas(documentoOriginalConcatenado, model, tokenizer)    \n",
        "\n",
        "embeddingDocumentoPermutado, tokensPermutado = getEmbeddingsConcat4UltimasCamadas(documentoPermutadoConcatenado, model, tokenizer)    "
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ri1po8efht3"
      },
      "source": [
        "Recupera os embeddings de cada sentença"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rsq_U2PFbAl4",
        "outputId": "5ed4411a-a35c-44f8-fcc3-e20030d0073b"
      },
      "source": [
        "print(\"Documento 1  :\", str(documento_1))\n",
        "print(\"Quantidade de sentenças:\",len(documento_1))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_1)\n",
        "\n",
        "# Guarda os embeddings das sentenças\n",
        "matrix_embedding1 = []\n",
        "\n",
        "# Calcula a média dos embeddings das sentenças\n",
        "matrix_media_embedding1 = []\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_1[i]\n",
        "  \n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)    \n",
        "\n",
        "    # Guarda os embeddings da sentença\n",
        "    matrix_embedding1.append(embeddingSi)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "\n",
        "    # Adiciona na lista\n",
        "    matrix_media_embedding1.append(mediaEmbeddingSi)"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento 1  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xh_-y4xfo6g"
      },
      "source": [
        "Recupera os embeddings de cada sentença"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xuOtDjGb11-",
        "outputId": "fde8fc50-acef-4e63-ebfc-e132a0daa2ed"
      },
      "source": [
        "print(\"Documento 2  :\", str(documento_2))\n",
        "print(\"Quantidade de sentenças:\",len(documento_2))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_2)\n",
        "\n",
        "# Guarda os embeddings das sentenças\n",
        "matrix_embedding2 = []\n",
        "\n",
        "# Calcula a média dos embeddings das sentenças\n",
        "matrix_media_embedding2 = []\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_2[i]\n",
        "  \n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoPermutado, documentoPermutadoConcatenado, Si, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)    \n",
        "  \n",
        "    # Guarda os embeddings da sentença\n",
        "    matrix_embedding2.append(embeddingSi)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "\n",
        "    # Adiciona na lista\n",
        "    matrix_media_embedding2.append(mediaEmbeddingSi)"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento 2  : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgrEAtYHfz3i"
      },
      "source": [
        "Calcula a similaridade do cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "vzNzT7gAZ4T8",
        "outputId": "b49607c8-1eaf-4512-9bc0-828d1c0a503c"
      },
      "source": [
        "# Importa a biblioteca\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Coloca todos os embeddings de sentença em uma matriz\n",
        "embed_matrix1 = np.array([x.numpy() for x in matrix_media_embedding1])\n",
        "embed_matrix2 = np.array([x.numpy() for x in matrix_media_embedding2])\n",
        "\n",
        "# Calcula a similaridade do coseno entre as sentenças\n",
        "cos_matrix = 1 - cosine_similarity(embed_matrix1,embed_matrix2)\n",
        "\n",
        "# Coloca a similaridade para a primeira sentença\n",
        "df1[\"medida\"] = cos_matrix[0]\n",
        "\n",
        "df1"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      sentenca    medida\n",
              "0          Bom Dia, professor.  0.438215\n",
              "1    Qual o conteúdo da prova?  0.332433\n",
              "2      Vai cair tudo na prova?  0.011505\n",
              "3  Aguardo uma resposta, João.  0.375243"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a72fa9d0-0e46-453d-85c9-93d1daf8a786\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentenca</th>\n",
              "      <th>medida</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bom Dia, professor.</td>\n",
              "      <td>0.438215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Qual o conteúdo da prova?</td>\n",
              "      <td>0.332433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Vai cair tudo na prova?</td>\n",
              "      <td>0.011505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aguardo uma resposta, João.</td>\n",
              "      <td>0.375243</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a72fa9d0-0e46-453d-85c9-93d1daf8a786')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a72fa9d0-0e46-453d-85c9-93d1daf8a786 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a72fa9d0-0e46-453d-85c9-93d1daf8a786');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw_hhpWEZ4T8"
      },
      "source": [
        "### Mapa de calor calculado com a similaridade cosseno entre todas as sentenças gerados separadamente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "RUGpvD-8Z4T8",
        "outputId": "df4d689a-e91b-4bf4-887a-ac8f4f28a10a"
      },
      "source": [
        "# Cria o dataframe da lista com as sentenças como nome das colunas\n",
        "df1 = pd.DataFrame(cos_matrix,columns = documento_2)\n",
        "# Indexa pela sentença\n",
        "df1.index = documento_1\n",
        "df1"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             Aguardo uma resposta, João.  \\\n",
              "Bom Dia, professor.                             0.438215   \n",
              "Qual o conteúdo da prova?                       0.436941   \n",
              "Vai cair tudo na prova?                         0.462695   \n",
              "Aguardo uma resposta, João.                     0.051127   \n",
              "\n",
              "                             Qual o conteúdo da prova?  Bom Dia, professor.  \\\n",
              "Bom Dia, professor.                           0.332433             0.011505   \n",
              "Qual o conteúdo da prova?                     0.011058             0.347841   \n",
              "Vai cair tudo na prova?                       0.143538             0.397618   \n",
              "Aguardo uma resposta, João.                   0.438635             0.462898   \n",
              "\n",
              "                             Vai cair tudo na prova?  \n",
              "Bom Dia, professor.                         0.375243  \n",
              "Qual o conteúdo da prova?                   0.134911  \n",
              "Vai cair tudo na prova?                     0.010414  \n",
              "Aguardo uma resposta, João.                 0.457150  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-43c6cb11-f563-4b05-857e-56c16318d6a7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Aguardo uma resposta, João.</th>\n",
              "      <th>Qual o conteúdo da prova?</th>\n",
              "      <th>Bom Dia, professor.</th>\n",
              "      <th>Vai cair tudo na prova?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Bom Dia, professor.</th>\n",
              "      <td>0.438215</td>\n",
              "      <td>0.332433</td>\n",
              "      <td>0.011505</td>\n",
              "      <td>0.375243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Qual o conteúdo da prova?</th>\n",
              "      <td>0.436941</td>\n",
              "      <td>0.011058</td>\n",
              "      <td>0.347841</td>\n",
              "      <td>0.134911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Vai cair tudo na prova?</th>\n",
              "      <td>0.462695</td>\n",
              "      <td>0.143538</td>\n",
              "      <td>0.397618</td>\n",
              "      <td>0.010414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Aguardo uma resposta, João.</th>\n",
              "      <td>0.051127</td>\n",
              "      <td>0.438635</td>\n",
              "      <td>0.462898</td>\n",
              "      <td>0.457150</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43c6cb11-f563-4b05-857e-56c16318d6a7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-43c6cb11-f563-4b05-857e-56c16318d6a7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-43c6cb11-f563-4b05-857e-56c16318d6a7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "rVqzhgg1Z4T8",
        "outputId": "de961c2f-287f-4d0e-be7d-e7b993e0afe1"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tamanho da figura\n",
        "fig, ax = plt.subplots(figsize=(18,12))\n",
        "\n",
        "# Cria o gráfico\n",
        "ax = sns.heatmap(cos_matrix, xticklabels=documento_2, yticklabels=documento_1, cbar_kws={\"label\": \"Medida de similaridade do cosseno\"}, annot=True)\n",
        "\n",
        "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, horizontalalignment=\"right\")\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, horizontalalignment=\"right\")\n",
        "\n",
        "# Coloca o título da matriz\n",
        "ax.set_title(\"Similaridade do cosseno entre os embeddings das sentenças gerados simultaneamente\\n\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x864 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}