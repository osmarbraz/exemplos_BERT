{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExemplosWordEmbeddingContextualBERT_pt_br_sentenca.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XWqMsrb-ew5T"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osmarbraz/exemplos_BERT/blob/main/ExemplosWordEmbeddingContextualBERT_pt_br_sentenca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78HE8FLsKN9Q"
      },
      "source": [
        "#Exemplo de Word Embeddings(pt-br) Contextual usando BERT Transformers by HuggingFace\n",
        "\n",
        "## **A execução pode ser feita através do menu Ambiente de Execução opção Executar tudo.**\n",
        "\n",
        "Exemplos de uso de **Word Embeddings Contextuais** para **desambiguação** de documentos originais e permutados utilizando suas sentenças. No final do notebook estão os exemplos com os documentos:\n",
        "\n",
        "*   documento original e permutado\n",
        "\n",
        "**Link biblioteca Huggingface:**\n",
        "https://github.com/huggingface/transformers\n",
        "\n",
        "\n",
        "**Artigo original BERT Jacob Devlin:**\n",
        "https://arxiv.org/pdf/1506.06724.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyxb5Px3p1-e"
      },
      "source": [
        "## 0 - Preparação do ambiente\n",
        "Preparação do ambiente para execução do exemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAPVtRXQqDim"
      },
      "source": [
        "###Tratamento de logs\n",
        "\n",
        "Método para tratamento dos logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcopxbGZqDip"
      },
      "source": [
        "# Biblioteca de logging\n",
        "import logging\n",
        "\n",
        "# Formatando a mensagem de logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GjYtXcMnSAe"
      },
      "source": [
        "### Identificando o ambiente Colab\n",
        "\n",
        "Cria uma variável para identificar que o notebook está sendo executado no Google Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMiH0E3OnRa1"
      },
      "source": [
        "# Se estiver executando no Google Colaboratory\n",
        "import sys\n",
        "\n",
        "# Retorna true ou false se estiver no Google Colaboratory\n",
        "IN_COLAB = 'google.colab' in sys.modules"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqa-7WXBAw8q"
      },
      "source": [
        "## 1 - Instalação BERT da Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCdqJCtQN52l"
      },
      "source": [
        "Instala a interface pytorch para o BERT by Hugging Face. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RfUN_KolV-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a2f9de-aebe-4448-e649-63b463e0285e"
      },
      "source": [
        "# Instala a última versão da biblioteca\n",
        "#!pip install transformers\n",
        "\n",
        "# Instala uma versão específica da biblioteca\n",
        "!pip install -U transformers==4.5.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: transformers==4.5.1 in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (0.0.45)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (0.10.2)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (8.0.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.1) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.1) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.1) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQj2wmKDpkrH"
      },
      "source": [
        "## 2 - Download do arquivo do PyTorch Checkpoint\n",
        "\n",
        "Lista de modelos da comunidade:\n",
        "* https://huggingface.co/models\n",
        "\n",
        "Português(https://github.com/neuralmind-ai/portuguese-bert):  \n",
        "* **'neuralmind/bert-base-portuguese-cased'**\n",
        "* **'neuralmind/bert-large-portuguese-cased'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajrTjZzapkrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a416a7d7-b30b-4c98-d2c1-0747e89e0fed"
      },
      "source": [
        "# Importando as bibliotecas\n",
        "import os\n",
        "\n",
        "# Variável para setar o arquivo\n",
        "URL_MODELO = None\n",
        "\n",
        "# Comente uma das urls para carregar modelos de tamanhos diferentes(base/large)\n",
        "# URL_MODELO do arquivo do modelo tensorflow\n",
        "# arquivo menor(base) 1.1 Gbytes\n",
        "# URL_MODELO = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip\"\n",
        "\n",
        "# arquivo grande(large) 3.5 Gbytes\n",
        "URL_MODELO = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-large-portuguese-cased/bert-large-portuguese-cased_pytorch_checkpoint.zip\"\n",
        "\n",
        "# Se a variável foi setada\n",
        "if URL_MODELO:\n",
        "\n",
        "    # Diretório descompactação\n",
        "    DIRETORIO_MODELO = '/content/modelo'\n",
        "\n",
        "    # Recupera o nome do arquivo do modelo da URL_MODELO\n",
        "    arquivo = URL_MODELO.split(\"/\")[-1]\n",
        "\n",
        "    # Nome do arquivo do vocabulário\n",
        "    arquivo_vocab = \"vocab.txt\"\n",
        "\n",
        "    # Caminho do arquivo na URL_MODELO\n",
        "    caminho = URL_MODELO[0:len(URL_MODELO)-len(arquivo)]\n",
        "\n",
        "    # Verifica se a pasta de descompactação existe no pasta corrente\n",
        "    if not os.path.exists(DIRETORIO_MODELO):\n",
        "   \n",
        "        # Baixa o arquivo do modelo\n",
        "        !wget $URL_MODELO\n",
        "    \n",
        "        # Descompacta o arquivo na pasta de descompactação\n",
        "        !unzip -o $arquivo -d $DIRETORIO_MODELO\n",
        "\n",
        "        # Baixa o arquivo do vocabulário\n",
        "        # O vocabulário não está no arquivo compactado acima, mesma url mas arquivo diferente\n",
        "        URL_MODELO_VOCAB = caminho + arquivo_vocab\n",
        "        !wget $URL_MODELO_VOCAB\n",
        "    \n",
        "        # Coloca o arquivo do vocabulário no diretório de descompactação\n",
        "        !mv $arquivo_vocab $DIRETORIO_MODELO\n",
        "            \n",
        "        # Move o arquivo para pasta de descompactação\n",
        "        !mv $arquivo $DIRETORIO_MODELO\n",
        "       \n",
        "        print('Pasta do ' + DIRETORIO_MODELO + ' pronta!')\n",
        "    else:\n",
        "      print('Pasta do ' + DIRETORIO_MODELO + ' já existe!')\n",
        "\n",
        "    # Lista a pasta corrente\n",
        "    !ls -la $DIRETORIO_MODELO\n",
        "else:\n",
        "    print('Variável URL_MODELO não setada!')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pasta do /content/modelo já existe!\n",
            "total 2525908\n",
            "drwxr-xr-x 2 root root       4096 May 20 17:23 .\n",
            "drwxr-xr-x 1 root root       4096 May 20 17:23 ..\n",
            "-rw-r--r-- 1 root root 1244275810 Jan 22  2020 bert-large-portuguese-cased_pytorch_checkpoint.zip\n",
            "-rw-rw-r-- 1 root root        874 Jan 12  2020 config.json\n",
            "-rw-rw-r-- 1 root root 1342014951 Jan 12  2020 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root     209528 Jan 21  2020 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcpd9t9PpkrX"
      },
      "source": [
        "## 3 - Carregando o Tokenizador BERT\n",
        "\n",
        "O tokenizador utiliza WordPiece, veja em [artigo original](https://arxiv.org/pdf/1609.08144.pdf).\n",
        "\n",
        "Carregando o tokenizador da pasta '/content/modelo/' do diretório padrão se variável `URL_MODELO` setada.\n",
        "\n",
        "**Caso contrário carrega da comunidade**\n",
        "\n",
        "Por default(`do_lower_case=True`) todas as letras são colocadas para minúsculas. Para ignorar a conversão para minúsculo use o parâmetro `do_lower_case=False`. Esta opção também considera as letras acentuadas(ãçéí...), que são necessárias a língua portuguesa.\n",
        "\n",
        "O parâmetro `do_lower_case` interfere na quantidade tokens a ser gerado apartir de um texto. Quando igual a `False` reduz a quantidade de tokens gerados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8cKVs4fpkrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b27f6b02-7bfe-4618-a9cd-892f7ca6f621"
      },
      "source": [
        "# Importando as bibliotecas do tokenizador\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Se a variável URL_MODELO foi setada\n",
        "if URL_MODELO:\n",
        "    # Carregando o Tokenizador\n",
        "    print('Carrgando o tokenizador BERT do diretório ' + DIRETORIO_MODELO + '...')\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(DIRETORIO_MODELO, \n",
        "                                              do_lower_case=False)    \n",
        "else:\n",
        "    # Carregando o Tokenizador da comunidade\n",
        "    print('Carregando o tokenizador da comunidade...')\n",
        "    \n",
        "    #tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)\n",
        "    tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased', \n",
        "                                              do_lower_case=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Carrgando o tokenizador BERT do diretório /content/modelo...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m__On2g1a--K"
      },
      "source": [
        "## 4 - Carregando o Modelo BERT(BertModel)\n",
        "\n",
        "Se a variável `URL_MODELO` estiver setada carrega o modelo do diretório `content/modelo`.\n",
        "\n",
        "Caso contrário carrega da comunidade.\n",
        "\n",
        "Carregando o modelo da pasta '/content/modelo/' do diretório padrão.\n",
        "\n",
        "A implementação do huggingface pytorch inclui um conjunto de interfaces projetadas para uma variedade de tarefas de PNL. Embora essas interfaces sejam todas construídas sobre um modelo treinado de BERT, cada uma possui diferentes camadas superiores e tipos de saída projetados para acomodar suas tarefas específicas de PNL.\n",
        "\n",
        "A documentação para estas pode ser encontrada em [aqui](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html).\n",
        "\n",
        "Por default o modelo está em modo avaliação ou seja `model.eval()`.\n",
        "\n",
        "-----------------------\n",
        "\n",
        "Durante a avaliação do modelo, este retorna um número de diferentes objetos com base em como é configurado na chamada do método `from_pretrained`. \n",
        "\n",
        "Quando definimos `output_hidden_states = True` na chamada do método `from_pretrained`, retorno do modelo possui no terceiro item os estados ocultos(**hidden_states**) de todas as camadas.  Veja a documentação para mais detalhes: https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "\n",
        "Quando **`output_hidden_states = True`** model retorna:\n",
        "- outputs[0] = last_hidden_state;\n",
        "- outputs[1] = pooler_output; \n",
        "- outputs[2] = hidden_states.\n",
        "\n",
        "Quando **`output_hidden_states = False`** ou não especificado model retorna:\n",
        "- outputs[0] = last_hidden_state;\n",
        "- outputs[1] = pooler_output.\n",
        "\n",
        "\n",
        "**ATENÇÃO**: O parâmetro ´**output_hidden_states = True**´ habilita gerar as camadas ocultas do modelo. Caso contrário somente a última camada é mantida. Este parâmetro otimiza a memória mas não os resultados.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRV6l_I-qg9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a3937ef-5975-4bba-d6a5-bb9c636bff77"
      },
      "source": [
        "# Importando as bibliotecas do Modelo\n",
        "from transformers import BertModel\n",
        "\n",
        "# Se a variável URL_MODELO1 foi setada\n",
        "if URL_MODELO:\n",
        "    # Carregando o Tokenizador\n",
        "    print('Carregando o modelo BERT do diretório ' + DIRETORIO_MODELO + '...')\n",
        "\n",
        "    model = BertModel.from_pretrained(DIRETORIO_MODELO, \n",
        "                                      output_attentions = True,\n",
        "                                      output_hidden_states = True)    \n",
        "else:\n",
        "    # Carregando o Tokenizador da comunidade\n",
        "    print('Carregando o modelo BERT da comunidade ...')\n",
        "\n",
        "    model = BertModel.from_pretrained('neuralmind/bert-large-portuguese-cased', \n",
        "                                      output_attentions = True,\n",
        "                                      output_hidden_states = True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Carregando o modelo BERT do diretório /content/modelo...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU3wHzNUmmBP"
      },
      "source": [
        "## 5 - Funções auxiliares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s42mgtmSZ8MR"
      },
      "source": [
        "### getEmbeddingsCamadas\n",
        "\n",
        "Funções que recuperam os embeddings das camadas:\n",
        "- Primeira camada;\n",
        "- Penúltima camada;\n",
        "- Ùltima camada;\n",
        "- Soma das 4 últimas camadas;\n",
        "- Concatenação das 4 últimas camadas;\n",
        "- Soma de todas as camadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgo3EBTRZ9-3"
      },
      "source": [
        "def getEmbeddingPrimeiraCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][0]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingPenultimaCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][-2]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingUltimaCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "     \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][-1]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado    \n",
        "\n",
        "def getEmbeddingSoma4UltimasCamadas(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  embeddingCamadas = output[2][-4:]\n",
        "  # Saída: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "\n",
        "  # Usa o método `stack` para criar uma nova dimensão no tensor \n",
        "  # com a concateção dos tensores dos embeddings.        \n",
        "  #Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultadoStack = torch.stack(embeddingCamadas, dim=0)\n",
        "  # Saída: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  \n",
        "  # Realiza a soma dos embeddings de todos os tokens para as camadas\n",
        "  # Entrada: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  resultado = torch.sum(resultadoStack, dim=0)\n",
        "  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingConcat4UltimasCamadas(output):  \n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Cria uma lista com os tensores a serem concatenados\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  # Lista com os tensores a serem concatenados\n",
        "  listaConcat = []\n",
        "  # Percorre os 4 últimos\n",
        "  for i in [-1,-2,-3,-4]:\n",
        "      # Concatena da lista\n",
        "      listaConcat.append(output[2][i])\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  # Realiza a concatenação dos embeddings de todos as camadas\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = torch.cat(listaConcat, dim=-1)\n",
        "  # Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "    \n",
        "  return resultado   \n",
        "\n",
        "def getEmbeddingSomaTodasAsCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "   \n",
        "  # Retorna todas as camadas descontando a primeira(0)\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  embeddingCamadas = output[2][1:]\n",
        "  # Saída: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  # Usa o método `stack` para criar uma nova dimensão no tensor \n",
        "  # com a concateção dos tensores dos embeddings.        \n",
        "  #Entrada: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultadoStack = torch.stack(embeddingCamadas, dim=0)\n",
        "  # Saída: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "    \n",
        "  # Realiza a soma dos embeddings de todos os tokens para as camadas\n",
        "  # Entrada: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  resultado = torch.sum(resultadoStack, dim=0)\n",
        "  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "    \n",
        "  return resultado"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWqMsrb-ew5T"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm98RoojJcqP"
      },
      "source": [
        "# Import das bibliotecas\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7nx_eZ8hSlr"
      },
      "source": [
        "### getEmbeddingsVisual\n",
        "\n",
        "Função para gerar as coordenadas de plotagem a partir das sentenças de embeddings.\n",
        "\n",
        "Existe uma função para os tipos de camadas utilizadas:\n",
        "- Ùltima camada;\n",
        "- Soma das 4 últimas camadas;\n",
        "- Concatenação das 4 últimas camadas;\n",
        "- Soma de todas as camadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLdbOT8-g43V"
      },
      "source": [
        "def getEmbeddingsVisualUltimaCamada(texto, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    texto_marcado = \"[CLS] \" + texto + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    texto_tokenizado = tokenizador.tokenize(texto_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(texto_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(texto_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingUltimaCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, texto_tokenizado"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAf9lJJ2hZbt"
      },
      "source": [
        "def getEmbeddingsVisualSoma4UltimasCamadas(texto, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    texto_marcado = \"[CLS] \" + texto + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    texto_tokenizado = tokenizador.tokenize(texto_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(texto_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(texto_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSoma4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, texto_tokenizado"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XpwSN1ghpnz"
      },
      "source": [
        "def getEmbeddingsVisualConcat4UltimasCamadas(texto, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    texto_marcado = \"[CLS] \" + texto + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    texto_tokenizado = tokenizador.tokenize(texto_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(texto_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(texto_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingConcat4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, texto_tokenizado"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3KU1EFrnSPK"
      },
      "source": [
        "def getEmbeddingsVisualSomaTodasAsCamadas(texto, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    texto_marcado = \"[CLS] \" + texto + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    texto_tokenizado = tokenizador.tokenize(texto_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(texto_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(texto_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSomaTodasAsCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, texto_tokenizado"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-3QzDMwmfiJ"
      },
      "source": [
        "## 6 - Exemplo sentenças de documento original e permutado utilizando embedding da última camada do BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlUOqzcYFinG"
      },
      "source": [
        "### Funções auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvWIBFTLJ7z9"
      },
      "source": [
        "def getDocumentoTokenizado(documento, tokenizador):\n",
        "\n",
        "    # Adiciona os tokens especiais.\n",
        "    documentoMarcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Documento tokenizado\n",
        "    documentoTokenizado = tokenizador.tokenize(documentoMarcado)\n",
        "\n",
        "    return documentoTokenizado"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abS44M4yvFxf"
      },
      "source": [
        "# Localiza os índices de início e fim de uma sublista em uma lista\n",
        "def encontrarIndiceSubLista(lista, sublista):\n",
        "    # Recupera o tamanho da lista \n",
        "    h = len(lista)\n",
        "    # Recupera o tamanho da sublista\n",
        "    n = len(sublista)    \n",
        "    skip = {sublista[i]: n - i - 1 for i in range(n - 1)}\n",
        "    i = n - 1\n",
        "    while i < h:\n",
        "        for j in range(n):\n",
        "            if lista[i - j] != sublista[-j - 1]:\n",
        "                i += skip.get(lista[i], n)\n",
        "                break\n",
        "        else:\n",
        "            indiceInicio = i - n + 1\n",
        "            indiceFim = indiceInicio + len(sublista)-1\n",
        "            return indiceInicio, indiceFim\n",
        "    return -1, -1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSQs3O5QpJSj"
      },
      "source": [
        "def getEmbeddingSentencaDocumentoEmbedding(embeddingDocumento, documento, sentenca, tokenizador):\n",
        "  # Tokeniza o documento\n",
        "  documentoTokenizado = getDocumentoTokenizado(documento, tokenizador)  \n",
        "  #print(documentoTokenizado)\n",
        "\n",
        "  # Tokeniza a sentença\n",
        "  sentençaTokenizada = getDocumentoTokenizado(sentenca, tokenizador)\n",
        "  \n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentençaTokenizada.remove('[CLS]')\n",
        "  sentençaTokenizada.remove('[SEP]')  \n",
        "  #print(sentençaTokenizada)\n",
        "  #print(len(sentençaTokenizada))\n",
        "  \n",
        "  # Localiza os índices dos tokens da sentença no documento\n",
        "  inicio, fim = encontrarIndiceSubLista(documentoTokenizado,sentençaTokenizada)\n",
        "  #print(inicio,fim) \n",
        " \n",
        "  # Recupera os embeddings dos tokens da sentença a partir dos embeddings do documento\n",
        "  embeddingSentenca = embeddingDocumento[inicio:fim+1]\n",
        "  #print(\"embeddingSentenca=\", embeddingSentenca.shape)\n",
        "  \n",
        "  # Retorna o embedding da sentença no documento\n",
        "  return embeddingSentenca"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTmrN_IRmfiO"
      },
      "source": [
        "### Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYKIVpzTmfiP",
        "outputId": "a8499544-011c-43db-b224-701db3ac7da0"
      },
      "source": [
        "# Define um sentença de exemplo com diversos significados da palavra  \"pilha\"\n",
        "documento_original = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento\n",
        "documento_texto_original = ' '.join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + documento_texto_original + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print('{:>3} {:<12} {:>6,}'.format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLsHMNz9mfiQ"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pXg2A6zmfiR",
        "outputId": "8e34f90b-7ba6-461b-c541-665ca8c4c2bb"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM5GUtVNmfiR"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ePiuflemfiS"
      },
      "source": [
        "# Importa a bibliteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qhHUUoAmfiS"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o970gzwhmfiS"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnXcn_dvmfiT"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSiGmlpjmfiT",
        "outputId": "099a40a9-7c9f-46a2-fc03-def2680b38aa"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFDtOmN_mfiV"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM4Aosw4mfiV",
        "outputId": "c5632ef9-ccca-48ba-88c4-c43a9b25e01e"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "token_embeddings_original = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", token_embeddings_original.size())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([26, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T68Aje2tmfiW"
      },
      "source": [
        "Confirmando vetores dependentes do contexto\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeNaW--hmfiW",
        "outputId": "24bdbab9-5715-49ee-b67f-92c52b2cbabc"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVDBdrHWmfiX"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkZrVaVFmfiX",
        "outputId": "7d66ebb9-7b55-41a4-e8f1-3d1ee61da8dd"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "embeddingSentenca1Original = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, sentenca1Original, tokenizer)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, sentenca2Original, tokenizer)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, sentenca3Original, tokenizer)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, sentenca4Original, tokenizer)\n",
        "\n",
        "print('Os primeiros 4 valores de cada sentença do documento original.')\n",
        "\n",
        "print('\\nSentença 1:', sentenca1Original,'-', str(embeddingSentenca1Original[:4]))\n",
        "print('Soma embedding Sentença1:', sentenca1Original,'-', str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print('\\nSentença 2:', sentenca2Original,'-', str(embeddingSentenca2Original[:4]))\n",
        "print('Soma embedding Sentença2:', sentenca2Original,'-', str(torch.sum(embeddingSentenca2Original[:4])))\n",
        "\n",
        "print('\\nSentença 3:', sentenca3Original,'-', str(embeddingSentenca3Original[:4]))\n",
        "print('Soma embedding Sentença3:', sentenca3Original,'-', str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print('\\nSentença 4:', sentenca4Original,'-', str(embeddingSentenca4Original[:4]))\n",
        "print('Soma embedding Sentença4:', sentenca4Original,'-', str(torch.sum(embeddingSentenca4Original[:4])))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: Bom Dia, professor. - tensor([[-0.5098, -0.2877,  0.0873,  ...,  0.5436, -0.9302,  0.4668],\n",
            "        [ 0.6078, -0.8869,  0.3736,  ..., -0.3517, -1.2140, -0.3077],\n",
            "        [ 0.9075, -1.1233, -0.0093,  ...,  0.4433, -0.4633, -0.0113],\n",
            "        [ 0.2499, -0.4717, -0.1217,  ...,  0.7079, -0.2300,  0.4911]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-113.4554)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.3987, -0.9450,  0.1785,  ...,  0.7189, -0.6772, -0.1452],\n",
            "        [ 0.2067, -0.2705,  0.7145,  ...,  0.3047, -0.2718,  0.7577],\n",
            "        [ 0.2355,  0.2686,  0.5669,  ...,  1.0817,  0.5614,  0.3750],\n",
            "        [ 0.5264, -0.4600,  0.4810,  ..., -0.5559, -0.2941,  0.0378]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-115.8800)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[ 0.5178,  0.0863,  0.7394,  ..., -0.5765, -0.6208, -0.2230],\n",
            "        [ 0.1617,  1.1516, -0.0350,  ...,  0.1730,  0.2104, -0.0207],\n",
            "        [ 0.9251,  0.5806,  0.2491,  ...,  0.1687,  0.0772, -0.1173],\n",
            "        [ 0.5782,  1.3571, -0.5161,  ..., -0.0942,  0.0404, -0.0390]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-110.5299)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[ 0.4339, -0.7168,  0.1173,  ...,  0.0768, -1.2036,  0.3863],\n",
            "        [-0.6806, -1.0194, -0.0765,  ..., -0.1768, -0.6326,  0.2895],\n",
            "        [ 1.1384, -0.8541,  0.8992,  ...,  0.2798, -0.0381,  0.5061],\n",
            "        [ 0.9206, -0.9862,  0.1347,  ...,  0.4182, -1.0164,  0.4713]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-116.1903)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exqsxerrmfiY"
      },
      "source": [
        "Examinando os embeddings do documento original\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5py_A7lVmfiY",
        "outputId": "85b7e041-c7fc-4116-d325-8807e1163a89"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaOriginal = tokenizer.tokenize(sentenca1Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1TokenizadaOriginal)\n",
        "embeddingSentenca1Original = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, sentenca1Original, tokenizer)\n",
        "print('\\nSentença 1 Original=\\'', sentenca1Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca1Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaOriginal = tokenizer.tokenize(sentenca2Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2TokenizadaOriginal)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, sentenca2Original, tokenizer)\n",
        "print('\\nSentença 2 Original=\\'', sentenca2Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca2TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca2Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca2Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaOriginal = tokenizer.tokenize(sentenca3Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3TokenizadaOriginal)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, sentenca3Original, tokenizer)\n",
        "print('\\nSentença 3 Original=\\'', sentenca3Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca3TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca3Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca3Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaOriginal = tokenizer.tokenize(sentenca4Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4TokenizadaOriginal)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, sentenca4Original, tokenizer)\n",
        "print('\\nSentença 4 Original=\\'', sentenca4Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca4Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Original))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=' Bom Dia, professor. '\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 1024])\n",
            "    Soma embeddings:  -141.45\n",
            "\n",
            "Sentença 2 Original=' Qual o conteúdo da prova? '\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -173.58\n",
            "\n",
            "Sentença 3 Original=' Vai cair tudo na prova? '\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -167.58\n",
            "\n",
            "Sentença 4 Original=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -200.72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hg9eKyjEfE5"
      },
      "source": [
        "### Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqHzON3PCa49",
        "outputId": "6242de3d-a703-496a-96ef-432c02d072a5"
      },
      "source": [
        "# Define um sentença de exemplo com diversos significados da palavra  \"pilha\"\n",
        "documento_permutado = [\"Aguardo uma resposta, João.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Bom Dia, professor.\"]\n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento\n",
        "documento_texto_permutado = ' '.join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + documento_texto_permutado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print('{:>3} {:<12} {:>6,}'.format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 Vai          20,805\n",
            " 15 cair          9,322\n",
            " 16 tudo          2,745\n",
            " 17 na              229\n",
            " 18 prova         2,310\n",
            " 19 ?               136\n",
            " 20 Bom           8,399\n",
            " 21 Dia           3,616\n",
            " 22 ,               117\n",
            " 23 professor     2,917\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5Snv8-ACy47",
        "outputId": "51b94b48-fa6d-4cc6-8429-77bdcefef6df"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLv52fBItM3I"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMFR5tSiCy48"
      },
      "source": [
        "# Importa a bibliteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDFnt2yntIgn"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je2zyykXCy49"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZSIxolutQSp"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z09FmGtnCy49",
        "outputId": "613c68a4-95dc-477b-ed47-3d064b45bac8"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aetb3LVYtXnI"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkk3Ix9kC93C",
        "outputId": "ca8a3573-beb6-4126-c73c-3079bf0d2f0b"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "token_embeddings_permutado = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", token_embeddings_permutado.size())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([26, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIsMSKxNIUg9"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkHr7wEFIUhA",
        "outputId": "4f6ce861-0f44-443f-ed6d-968675399dbc"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, sentenca1Permutado, tokenizer)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, sentenca2Permutado, tokenizer)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, sentenca3Permutado, tokenizer)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, sentenca4Permutado, tokenizer)\n",
        "\n",
        "print('Os primeiros 4 valores de cada sentença do documento permutado.')\n",
        "\n",
        "print('\\nSentença 1:', sentenca1Permutado,'-', str(embeddingSentenca1Permutado[:4]))\n",
        "print('Soma embedding Sentença1:', sentenca1Original,'-', str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print('\\nSentença 2:', sentenca2Permutado,'-', str(embeddingSentenca2Permutado[:4]))\n",
        "print('Soma embedding Sentença2:', sentenca2Permutado,'-', str(torch.sum(embeddingSentenca2Permutado[:4])))\n",
        "\n",
        "print('\\nSentença 3:', sentenca3Permutado,'-', str(embeddingSentenca3Permutado[:4]))\n",
        "print('Soma embedding Sentença3:', sentenca3Permutado,'-', str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print('\\nSentença 4:', sentenca4Permutado,'-', str(embeddingSentenca4Permutado[:4]))\n",
        "print('Soma embedding Sentença4:', sentenca4Permutado,'-', str(torch.sum(embeddingSentenca4Permutado[:4])))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[ 0.4472, -0.7700, -0.0164,  ...,  0.3660, -1.3074,  0.4681],\n",
            "        [-0.6242, -1.0259, -0.2210,  ..., -0.0656, -0.7453,  0.2866],\n",
            "        [ 1.3008, -1.0530,  0.9163,  ...,  0.3736, -0.1157,  0.4558],\n",
            "        [ 0.9188, -1.0255,  0.2020,  ...,  0.4574, -1.0556,  0.6117]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-113.4554)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.3900, -0.9698,  0.1223,  ...,  0.6340, -0.5703, -0.1435],\n",
            "        [ 0.1376, -0.3465,  0.7442,  ...,  0.2438, -0.2685,  0.7738],\n",
            "        [ 0.2753,  0.3924,  0.4863,  ...,  1.1585,  0.5529,  0.3655],\n",
            "        [ 0.5488, -0.5081,  0.4872,  ..., -0.5818, -0.3959,  0.0555]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-116.1011)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[ 0.4106,  0.0771,  0.6933,  ..., -0.5881, -0.6007, -0.2466],\n",
            "        [ 0.2369,  1.1819, -0.0527,  ...,  0.1136,  0.2569, -0.0286],\n",
            "        [ 0.9528,  0.6457,  0.2524,  ...,  0.1264,  0.0724, -0.0766],\n",
            "        [ 0.5796,  1.3671, -0.5035,  ..., -0.0884,  0.0721, -0.0096]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-110.5299)\n",
            "\n",
            "Sentença 4: Bom Dia, professor. - tensor([[-0.0211, -0.3575,  0.3366,  ...,  0.3890, -0.6571,  0.2551],\n",
            "        [ 0.7860, -0.7043,  0.6582,  ..., -0.2687, -0.8251, -0.2969],\n",
            "        [ 0.9903, -1.1183,  0.2806,  ...,  0.3358, -0.5048, -0.2960],\n",
            "        [ 0.1774, -0.5418, -0.0161,  ...,  0.7121, -0.3789,  0.4511]])\n",
            "Soma embedding Sentença4: Bom Dia, professor. - tensor(-113.3441)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MINDqF2LDA9z",
        "outputId": "1629ccc3-3efb-4945-ae45-8cb4b178d7b7"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaPermutado = tokenizer.tokenize(sentenca1Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1TokenizadaPermutado)\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, sentenca1Permutado, tokenizer)\n",
        "print('\\nSentença 1 Permutada=\\'', sentenca1Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca1Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaPermutado = tokenizer.tokenize(sentenca2Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2TokenizadaPermutado)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, sentenca2Permutado, tokenizer)\n",
        "print('\\nSentença 2 Permutada=\\'', sentenca2Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca2TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca2Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca2Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaPermutado = tokenizer.tokenize(sentenca3Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3TokenizadaPermutado)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, sentenca3Permutado, tokenizer)\n",
        "print('\\nSentença 3 Permutada=\\'', sentenca3Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca3TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca3Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca3Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaPermutado = tokenizer.tokenize(sentenca4Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4TokenizadaPermutado)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, sentenca4Permutado, tokenizer)\n",
        "print('\\nSentença 4 Permutada=\\'', sentenca4Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca4Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Permutado))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Bom Dia, professor.']\n",
            "\n",
            "Sentença 1 Permutada=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -199.66\n",
            "\n",
            "Sentença 2 Permutada=' Qual o conteúdo da prova? '\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -173.54\n",
            "\n",
            "Sentença 3 Permutada=' Vai cair tudo na prova? '\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 14 e término em 19\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -167.66\n",
            "\n",
            "Sentença 4 Permutada=' Bom Dia, professor. '\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 20 e término em 24\n",
            "    Formato modelo : torch.Size([5, 1024])\n",
            "    Soma embeddings:  -141.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UILLnj7KvHi"
      },
      "source": [
        "### Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eyEbV-7Kz6r",
        "outputId": "dd3fb01a-7622-4d95-bae8-513872d4ebc7"
      },
      "source": [
        "print('\\nSentença 4 Original=\\'', sentenca4Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaOriginal)\n",
        "print('    Formato modelo :', embeddingSentenca4Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Original))\n",
        "print('    Os 4 primeiros embeddings:', str(embeddingSentenca4Original[:4]))\n",
        "\n",
        "print('\\nSentença 1 Permutada=\\'', sentenca1Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaPermutado)\n",
        "print('    Formato modelo :', embeddingSentenca1Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Permutado))\n",
        "print('    Os 4 primeiros embeddings:', str(embeddingSentenca1Permutado[:4]))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sentença 4 Original=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -200.72\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.4339, -0.7168,  0.1173,  ...,  0.0768, -1.2036,  0.3863],\n",
            "        [-0.6806, -1.0194, -0.0765,  ..., -0.1768, -0.6326,  0.2895],\n",
            "        [ 1.1384, -0.8541,  0.8992,  ...,  0.2798, -0.0381,  0.5061],\n",
            "        [ 0.9206, -0.9862,  0.1347,  ...,  0.4182, -1.0164,  0.4713]])\n",
            "\n",
            "Sentença 1 Permutada=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -199.66\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.4472, -0.7700, -0.0164,  ...,  0.3660, -1.3074,  0.4681],\n",
            "        [-0.6242, -1.0259, -0.2210,  ..., -0.0656, -0.7453,  0.2866],\n",
            "        [ 1.3008, -1.0530,  0.9163,  ...,  0.3736, -0.1157,  0.4558],\n",
            "        [ 0.9188, -1.0255,  0.2020,  ...,  0.4574, -1.0556,  0.6117]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFf4FngNM7pU"
      },
      "source": [
        "### Diferença entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00Qor26BgvxM"
      },
      "source": [
        "#### Calcula a média da diferença entre os embeddings das sentenças do documento original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-jIEBrcM_PN",
        "outputId": "beb4f96f-1229-4f97-e83f-0678128370e4"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSsub = 0\n",
        "\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, Si, tokenizer)\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, Sj, tokenizer)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSi=\", mediaCamadasSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSj=\", mediaCamadasSj.shape)\n",
        "  \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    Ssub = torch.sub(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "\n",
        "    somaSsub = somaSsub + torch.mean(Ssub)\n",
        "\n",
        "DsubOriginal = float(somaSsub)/float(len(documento_original)-1)\n",
        "print(\"Ssub Original:\", DsubOriginal)\n",
        "    \n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ssub Original: 0.0001255537693699201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLsflYfog2Wr"
      },
      "source": [
        "#### Calcula a média da diferença entre os embeddings das sentenças do documento permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPgKoNklO-fm",
        "outputId": "409ae613-4e3a-4a31-f14f-d3a8e207176f"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSsub = 0\n",
        "\n",
        "for i in range(np-1):\n",
        "     # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, Si, tokenizer)\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, Sj, tokenizer)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSi=\", mediaCamadasSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSj=\", mediaCamadasSj.shape)\n",
        "  \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    Ssub = torch.sub(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "\n",
        "    somaSsub = somaSsub + torch.mean(Ssub)\n",
        "\n",
        "DsubPermutado = float(somaSsub)/float(np-1)\n",
        "print(\"Ssub permutado:\", DsubPermutado)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Bom Dia, professor.']\n",
            "Quantidade de sentenças: 4\n",
            "Ssub permutado: -4.3934366355339684e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cQDyISQg5PJ"
      },
      "source": [
        "#### Compara as médias da subtração dos embeddings das sentenças do documento original e permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zexmb_ZTPP2e",
        "outputId": "c8816bd2-0898-4e4f-cfaf-3f33d23e01a2"
      },
      "source": [
        "print(\"Dsub Original :\", DsubOriginal)\n",
        "print(\"Dsub Permutado:\", DsubPermutado)\n",
        "\n",
        "if (DsubOriginal < DsubPermutado):\n",
        "    print(\"Documento original tem menor diferença entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor diferença entre as entre as sentenças!\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dsub Original : 0.0001255537693699201\n",
            "Dsub Permutado: -4.3934366355339684e-05\n",
            "Documento Permutado tem menor diferença entre as entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGbwZ2m1im8A"
      },
      "source": [
        "### Diferença absoluta entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQQXbUgDim8D"
      },
      "source": [
        "#### Calcula a média da diferença absoluta entre os embeddings das sentenças do documento original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMNja-qWim8D",
        "outputId": "884f7532-4051-4bcf-97ab-fc75d4e46761"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSsubabs = 0\n",
        "\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, Si, tokenizer)\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, Sj, tokenizer)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSi=\", mediaCamadasSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSj=\", mediaCamadasSj.shape)\n",
        "  \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    Ssubabs = abs(torch.sub(mediaEmbeddingSi, mediaEmbeddingSj))\n",
        "\n",
        "    somaSsubabs = somaSsubabs + torch.mean(Ssubabs)\n",
        "\n",
        "DsubabsOriginal = float(somaSsubabs)/float(n-1)\n",
        "print(\"Dsubabs Original:\", DsubabsOriginal)\n",
        "    \n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dsubabs Original: 0.26808643341064453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM3_OJbuim8E"
      },
      "source": [
        "#### Calcula a média da diferença absoluta entre os embeddings das sentenças do documento permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld6YbYS3im8E",
        "outputId": "758b84ad-0bc5-4ba6-b4be-a749cc6dd78c"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSsubabs = 0\n",
        "\n",
        "for i in range(np-1):\n",
        "     # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, Si, tokenizer)\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, Sj, tokenizer)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSi=\", mediaCamadasSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSj=\", mediaCamadasSj.shape)\n",
        "  \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    Ssubabs = abs(torch.sub(mediaEmbeddingSi, mediaEmbeddingSj))\n",
        "\n",
        "    somaSsubabs = somaSsubabs + torch.mean(Ssubabs)\n",
        "\n",
        "DsubabsPermutado = float(somaSsubabs)/float(np-1)\n",
        "print(\"Dsubabs permutado:\", DsubabsPermutado)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Bom Dia, professor.']\n",
            "Quantidade de sentenças: 4\n",
            "Dsubabs permutado: 0.2773038148880005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhXm0QtZim8F"
      },
      "source": [
        "#### Compara as médias da subtração dos embeddings das sentenças do documento original e permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_52KU43Aim8F",
        "outputId": "8d1f6e8e-9c2d-4900-93aa-4f55824eea0d"
      },
      "source": [
        "print(\"Dsubabs Original :\", DsubabsOriginal)\n",
        "print(\"Dsubabs Permutado:\", DsubabsPermutado)\n",
        "\n",
        "if (DsubabsOriginal < DsubabsPermutado):\n",
        "    print(\"Documento original tem menor diferença absoluta entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor diferença absoluta entre as entre as sentenças!\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dsubabs Original : 0.26808643341064453\n",
            "Dsubabs Permutado: 0.2773038148880005\n",
            "Documento original tem menor diferença absoluta entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZwsYSleduFd"
      },
      "source": [
        "### Similaridade de cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9PYKV8Ld65J"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCoseno(sentenca1, sentenca2):\n",
        "  similaridade = 1 - cosine(sentenca1, sentenca2)\n",
        "  return similaridade"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERc7ys14hBbk"
      },
      "source": [
        "#### Calcula a média da similaridade de cosseno entre os embeddings das sentenças do documento original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnt64iRtduFh",
        "outputId": "c20b54f0-eefa-41e1-bc2c-6847ee6c479e"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSsim = 0\n",
        "\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, Si, tokenizer)\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, Sj, tokenizer)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSi=\", mediaCamadasSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSj=\", mediaCamadasSj.shape)\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    Ssim = similaridadeCoseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: <768 ou 1024>\n",
        "    \n",
        "    somaSsim = somaSsim + Ssim\n",
        "\n",
        "DsimOriginal = float(somaSsim)/float(n-1)\n",
        "print(\"Dsim Original:\", DsimOriginal)\n",
        "    \n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dsim Original: 0.7828846176465353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97FiaSGfhGIh"
      },
      "source": [
        "#### Calcula a média da similaridade de cosseno entre as sentenças dos embeddings do documento permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEKVMuR1duFi",
        "outputId": "a78443b4-233b-4b17-8155-9d8e1ad1f8ce"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSsim = 0\n",
        "\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, Si, tokenizer)\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, Sj, tokenizer)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSi=\", mediaCamadasSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSj=\", mediaCamadasSj.shape)\n",
        "  \n",
        "   # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    Ssim = similaridadeCoseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: <768 ou 1024>\n",
        "    \n",
        "    somaSsim = somaSsim + Ssim\n",
        "\n",
        "DsimPermutado = float(somaSsim)/float(np-1)\n",
        "print(\"Dsim Original:\", DsimPermutado)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Bom Dia, professor.']\n",
            "Quantidade de sentenças: 4\n",
            "Dsim Original: 0.7676135500272115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djNpamn-hOQs"
      },
      "source": [
        "#### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxlez0ymduFj",
        "outputId": "718ee592-304c-4129-dd9a-b5a769dd7d9c"
      },
      "source": [
        "print(\"Dsim Original :\", DsimOriginal)\n",
        "print(\"Dsim Permutado:\", DsimPermutado)\n",
        "\n",
        "if (DsimOriginal > DsimPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as entre as sentenças!\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dsim Original : 0.7828846176465353\n",
            "Dsim Permutado: 0.7676135500272115\n",
            "Documento original tem maior similaridade de cosseno entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g04L0ToQuyU9"
      },
      "source": [
        "### Resumo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeabcNrzu0DD",
        "outputId": "e39439b2-e3ef-45a4-9176-eb9c6436cba6"
      },
      "source": [
        "print(\"Resultado das medidas utilizando a última camada do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print('Dsub       :   {:.8f}          {:.8f}'.format(DsubOriginal,DsubPermutado))\n",
        "print('Dubabs     :   {:.8f}          {:.8f}'.format(DsubabsOriginal,DsubabsPermutado))\n",
        "print('Dsim       :   {:.8f}          {:.8f}'.format(DsimOriginal,DsimPermutado))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resultado das medidas utilizando a última camada do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Dsub       :   0.00012555          -0.00004393\n",
            "Dubabs     :   0.26808643          0.27730381\n",
            "Dsim       :   0.78288462          0.76761355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru02mC0Estsb"
      },
      "source": [
        "## 7 - Exemplo sentenças de documento original e permutado utilizando embedding a concatenação das 4 últimas camadas do BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7EY2vytstse"
      },
      "source": [
        "### Funções auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqtVb-PEstse"
      },
      "source": [
        "def getDocumentoTokenizado(documento, tokenizador):\n",
        "\n",
        "    # Adiciona os tokens especiais.\n",
        "    documentoMarcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Documento tokenizado\n",
        "    documentoTokenizado = tokenizador.tokenize(documentoMarcado)\n",
        "\n",
        "    return documentoTokenizado"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_CSmagkstsf"
      },
      "source": [
        "# Localiza os índices de início e fim de uma sublista em uma lista\n",
        "def encontrarIndiceSubLista(lista, sublista):\n",
        "    # Recupera o tamanho da lista \n",
        "    h = len(lista)\n",
        "    # Recupera o tamanho da sublista\n",
        "    n = len(sublista)    \n",
        "    skip = {sublista[i]: n - i - 1 for i in range(n - 1)}\n",
        "    i = n - 1\n",
        "    while i < h:\n",
        "        for j in range(n):\n",
        "            if lista[i - j] != sublista[-j - 1]:\n",
        "                i += skip.get(lista[i], n)\n",
        "                break\n",
        "        else:\n",
        "            indiceInicio = i - n + 1\n",
        "            indiceFim = indiceInicio + len(sublista)-1\n",
        "            return indiceInicio, indiceFim\n",
        "    return -1, -1"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3COIpaZastsf"
      },
      "source": [
        "def getEmbeddingSentencaDocumentoEmbedding(embeddingDocumento, documento, sentença, tokenizador):\n",
        "  # Tokeniza o documento\n",
        "  documentoTokenizado = getDocumentoTokenizado(documento, tokenizador)  \n",
        "  #print(documentoTokenizado)\n",
        "\n",
        "  # Tokeniza a sentença\n",
        "  sentençaTokenizada = getDocumentoTokenizado(sentença, tokenizador)\n",
        "  \n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentençaTokenizada.remove('[CLS]')\n",
        "  sentençaTokenizada.remove('[SEP]')  \n",
        "  #print(sentençaTokenizada)\n",
        "  #print(len(sentençaTokenizada))\n",
        "  \n",
        "  # Localiza os índices dos tokens da sentença no documento\n",
        "  inicio, fim = encontrarIndiceSubLista(documentoTokenizado,sentençaTokenizada)\n",
        "  #print(inicio,fim) \n",
        " \n",
        "  # Recupera os embeddings dos tokens da sentença a partir dos embeddings do documento\n",
        "  embeddingSentença = embeddingDocumento[inicio:fim+1]\n",
        "  #print(\"embeddingSentença=\", embeddingSentença.shape)\n",
        "  \n",
        "  # Retorna o embedding da sentença no documento\n",
        "  return embeddingSentença"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1vDmYJTstsg"
      },
      "source": [
        "### Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJOUyEpistsg",
        "outputId": "d7e84aef-c00f-4f5d-c19d-0d834ec22967"
      },
      "source": [
        "# Define um sentença de exemplo com diversos significados da palavra  \"pilha\"\n",
        "documento_original = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento\n",
        "documento_texto_original = ' '.join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + documento_texto_original + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print('{:>3} {:<12} {:>6,}'.format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4HrZqBfstsh"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0tDxh3Mstsh",
        "outputId": "8b4ee950-9818-4be4-8216-e265369146b1"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAQf9nM8stsh"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFqFcnx2stsh"
      },
      "source": [
        "# Importa a bibliteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJbHPnoAstsi"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda todas as camadas da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51R6f4Mistsi"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx2_AvR8tbnZ"
      },
      "source": [
        "Recupera a saída e concatena as 4 últimas camada do BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C0KcRUHstsj",
        "outputId": "561501e7-71d0-44f4-81aa-22ecb06beb8d"
      },
      "source": [
        "# Cria uma lista com os tensores a serem concatenados\n",
        "# Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "# Lista com os tensores a serem concatenados\n",
        "listaConcat = []\n",
        "# Percorre os 4 últimos\n",
        "for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "# Realiza a concatenação dos embeddings de todos as camadas\n",
        "# Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "# Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "\n",
        "print (\"O vetor da  concatenação das 4 últimas camadas oculta tem o formato:\", concat4_hidden_states.size())"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor da  concatenação das 4 últimas camadas oculta tem o formato: torch.Size([1, 26, 4096])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM4luftrstsk"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f18k_o-stsk",
        "outputId": "dd0eb41a-ce76-4fde-deec-413632b65d03"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "token_embeddings_original = torch.squeeze(concat4_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", token_embeddings_original.size())"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([26, 4096])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g0E9s2Rstsk"
      },
      "source": [
        "Confirmando vetores dependentes do contexto\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQGOQF-kstsk",
        "outputId": "4aad66bc-d4e9-4278-e540-5bbf45e43f59"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veMZsnAsstsl"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9RSPe2Hstsl",
        "outputId": "e854b2a7-9801-4dc5-f737-b595a620339b"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "embeddingSentenca1Original = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, sentenca1Original, tokenizer)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, sentenca2Original, tokenizer)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, sentenca3Original, tokenizer)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, sentenca4Original, tokenizer)\n",
        "\n",
        "print('Os primeiros 4 valores de cada sentença do documento original.')\n",
        "\n",
        "print('\\nSentença 1:', sentenca1Original,'-', str(embeddingSentenca1Original[:4]))\n",
        "print('Soma embedding Sentença1:', sentenca1Original,'-', str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print('\\nSentença 2:', sentenca2Original,'-', str(embeddingSentenca2Original[:4]))\n",
        "print('Soma embedding Sentença2:', sentenca2Original,'-', str(torch.sum(embeddingSentenca2Original[:4])))\n",
        "\n",
        "print('\\nSentença 3:', sentenca3Original,'-', str(embeddingSentenca3Original[:4]))\n",
        "print('Soma embedding Sentença3:', sentenca3Original,'-', str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print('\\nSentença 4:', sentenca4Original,'-', str(embeddingSentenca4Original[:4]))\n",
        "print('Soma embedding Sentença4:', sentenca4Original,'-', str(torch.sum(embeddingSentenca4Original[:4])))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: Bom Dia, professor. - tensor([[-0.5098, -0.2877,  0.0873,  ...,  0.9288, -0.0796,  0.3684],\n",
            "        [ 0.6078, -0.8869,  0.3736,  ...,  0.0124,  0.0094, -0.1719],\n",
            "        [ 0.9075, -1.1233, -0.0093,  ...,  0.1359, -0.3067, -0.4078],\n",
            "        [ 0.2499, -0.4717, -0.1217,  ...,  0.4604,  0.0075,  0.1555]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-176.3423)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.3987, -0.9450,  0.1785,  ...,  0.8476, -0.0572,  0.0365],\n",
            "        [ 0.2067, -0.2705,  0.7145,  ...,  0.2113,  0.2899,  0.5135],\n",
            "        [ 0.2355,  0.2686,  0.5669,  ...,  0.7205,  0.2510,  0.0924],\n",
            "        [ 0.5264, -0.4600,  0.4810,  ...,  0.1881,  0.0931,  0.1731]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-165.1487)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[ 0.5178,  0.0863,  0.7394,  ..., -0.3301, -0.0115,  0.1168],\n",
            "        [ 0.1617,  1.1516, -0.0350,  ..., -0.2189,  0.1096, -0.3660],\n",
            "        [ 0.9251,  0.5806,  0.2491,  ...,  0.0027,  0.2467, -0.1535],\n",
            "        [ 0.5782,  1.3571, -0.5161,  ...,  0.5285,  0.2553, -0.0197]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-177.0386)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[ 0.4339, -0.7168,  0.1173,  ...,  0.3840, -0.0936,  0.1355],\n",
            "        [-0.6806, -1.0194, -0.0765,  ...,  0.2201,  0.2731, -0.0384],\n",
            "        [ 1.1384, -0.8541,  0.8992,  ...,  0.5247,  0.2278,  0.0653],\n",
            "        [ 0.9206, -0.9862,  0.1347,  ...,  0.4393, -0.4792,  0.1743]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-169.8891)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGUkprWbstsl"
      },
      "source": [
        "Examinando os embeddings do documento original\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL-c0aqKstsl",
        "outputId": "4e7ba079-63df-486c-ef25-c2f42e597cf7"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento_original[0]\n",
        "sentenca2Original = documento_original[1]\n",
        "sentenca3Original = documento_original[2]\n",
        "sentenca4Original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaOriginal = tokenizer.tokenize(sentenca1Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1TokenizadaOriginal)\n",
        "embeddingSentenca1Original = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, sentenca1Original, tokenizer)\n",
        "print('\\nSentença 1 Original=\\'', sentenca1Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca1Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaOriginal = tokenizer.tokenize(sentenca2Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2TokenizadaOriginal)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, sentenca2Original, tokenizer)\n",
        "print('\\nSentença 2 Original=\\'', sentenca2Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca2TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca2Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca2Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaOriginal = tokenizer.tokenize(sentenca3Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3TokenizadaOriginal)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, sentenca3Original, tokenizer)\n",
        "print('\\nSentença 3 Original=\\'', sentenca3Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca3TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca3Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca3Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaOriginal = tokenizer.tokenize(sentenca4Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4TokenizadaOriginal)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, sentenca4Original, tokenizer)\n",
        "print('\\nSentença 4 Original=\\'', sentenca4Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca4Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Original))\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=' Bom Dia, professor. '\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 4096])\n",
            "    Soma embeddings:  -225.64\n",
            "\n",
            "Sentença 2 Original=' Qual o conteúdo da prova? '\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 4096])\n",
            "    Soma embeddings:  -252.06\n",
            "\n",
            "Sentença 3 Original=' Vai cair tudo na prova? '\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 4096])\n",
            "    Soma embeddings:  -260.65\n",
            "\n",
            "Sentença 4 Original=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 4096])\n",
            "    Soma embeddings:  -288.69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRdBVlt9stsm"
      },
      "source": [
        "### Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xsxt0Jistsm",
        "outputId": "e6471a65-5231-4ffb-d082-fe8bb6ae0407"
      },
      "source": [
        "# Define um sentença de exemplo com diversos significados da palavra  \"pilha\"\n",
        "documento_permutado = [\"Aguardo uma resposta, João.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Bom Dia, professor.\"]\n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento\n",
        "documento_texto_permutado = ' '.join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + documento_texto_permutado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print('{:>3} {:<12} {:>6,}'.format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 Vai          20,805\n",
            " 15 cair          9,322\n",
            " 16 tudo          2,745\n",
            " 17 na              229\n",
            " 18 prova         2,310\n",
            " 19 ?               136\n",
            " 20 Bom           8,399\n",
            " 21 Dia           3,616\n",
            " 22 ,               117\n",
            " 23 professor     2,917\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM5NtnGRuAsU"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GDGmFc_stsm",
        "outputId": "c0fddfe6-f475-4a07-bbcf-789db18cb3ba"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4WehJSxuDvo"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrvEtHp7stsm"
      },
      "source": [
        "# Importa a bibliteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQDiqh5UuMnc"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda todas as camadas da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tGQsVMpstsm"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5CmoS1at0YA"
      },
      "source": [
        "Recupera a saída e concatena as 4 últimas camada do BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRDYmUq9stsn"
      },
      "source": [
        "# Cria uma lista com os tensores a serem concatenados\n",
        "# Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "# Lista com os tensores a serem concatenados\n",
        "listaConcat = []\n",
        "# Percorre os 4 últimos\n",
        "for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "# Realiza a concatenação dos embeddings de todos as camadas\n",
        "# Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "# Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  "
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm9EVbDauSiI"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXNqk-oQstsn",
        "outputId": "50cfb6e5-c35a-411d-e414-9e6a4b0c62a6"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "token_embeddings_permutado = torch.squeeze(concat4_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", token_embeddings_permutado.size())"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([26, 4096])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbLgVumnstso"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1nUZ7OLstso",
        "outputId": "35834aed-8906-4a15-9ae4-fcd3ada1cc28"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, sentenca1Permutado, tokenizer)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, sentenca2Permutado, tokenizer)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, sentenca3Permutado, tokenizer)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, sentenca4Permutado, tokenizer)\n",
        "\n",
        "print('Os primeiros 4 valores de cada sentença do documento permutado.')\n",
        "\n",
        "print('\\nSentença 1:', sentenca1Permutado,'-', str(embeddingSentenca1Permutado[:4]))\n",
        "print('Soma embedding Sentença1:', sentenca1Original,'-', str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print('\\nSentença 2:', sentenca2Permutado,'-', str(embeddingSentenca2Permutado[:4]))\n",
        "print('Soma embedding Sentença2:', sentenca2Permutado,'-', str(torch.sum(embeddingSentenca2Permutado[:4])))\n",
        "\n",
        "print('\\nSentença 3:', sentenca3Permutado,'-', str(embeddingSentenca3Permutado[:4]))\n",
        "print('Soma embedding Sentença3:', sentenca3Permutado,'-', str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print('\\nSentença 4:', sentenca4Permutado,'-', str(embeddingSentenca4Permutado[:4]))\n",
        "print('Soma embedding Sentença4:', sentenca4Permutado,'-', str(torch.sum(embeddingSentenca4Permutado[:4])))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[ 0.4472, -0.7700, -0.0164,  ...,  0.5051, -0.1387,  0.1486],\n",
            "        [-0.6242, -1.0259, -0.2210,  ...,  0.0995,  0.2928, -0.0829],\n",
            "        [ 1.3008, -1.0530,  0.9163,  ...,  0.5652,  0.1623,  0.0638],\n",
            "        [ 0.9188, -1.0255,  0.2020,  ...,  0.5125, -0.5088,  0.1882]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-176.3423)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.3900, -0.9698,  0.1223,  ...,  0.7264, -0.0738,  0.2076],\n",
            "        [ 0.1376, -0.3465,  0.7442,  ...,  0.1908,  0.3050,  0.5651],\n",
            "        [ 0.2753,  0.3924,  0.4863,  ...,  0.7431,  0.2877,  0.0924],\n",
            "        [ 0.5488, -0.5081,  0.4872,  ...,  0.2678,  0.0907,  0.2537]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-164.9317)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[ 0.4106,  0.0771,  0.6933,  ..., -0.2983, -0.0386,  0.1818],\n",
            "        [ 0.2369,  1.1819, -0.0527,  ..., -0.2330,  0.0688, -0.2897],\n",
            "        [ 0.9528,  0.6457,  0.2524,  ..., -0.0151,  0.2300, -0.0513],\n",
            "        [ 0.5796,  1.3671, -0.5035,  ...,  0.5332,  0.1863, -0.0406]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-177.0386)\n",
            "\n",
            "Sentença 4: Bom Dia, professor. - tensor([[-0.0211, -0.3575,  0.3366,  ...,  0.4984, -0.0733,  0.0928],\n",
            "        [ 0.7860, -0.7043,  0.6582,  ..., -0.1228,  0.0325, -0.2024],\n",
            "        [ 0.9903, -1.1183,  0.2806,  ...,  0.0927, -0.3093, -0.4623],\n",
            "        [ 0.1774, -0.5418, -0.0161,  ...,  0.3971, -0.1176,  0.0996]])\n",
            "Soma embedding Sentença4: Bom Dia, professor. - tensor(-174.9091)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E_hSMahstso",
        "outputId": "97a6bcc1-4e9c-4cce-db4a-23690f6afad6"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Permutado = documento_permutado[0]\n",
        "sentenca2Permutado = documento_permutado[1]\n",
        "sentenca3Permutado = documento_permutado[2]\n",
        "sentenca4Permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaPermutado = tokenizer.tokenize(sentenca1Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1TokenizadaPermutado)\n",
        "embeddingSentenca1Permutado = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, sentenca1Permutado, tokenizer)\n",
        "print('\\nSentença 1 Permutada=\\'', sentenca1Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca1Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaPermutado = tokenizer.tokenize(sentenca2Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2TokenizadaPermutado)\n",
        "embeddingSentenca2Permutado = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, sentenca2Permutado, tokenizer)\n",
        "print('\\nSentença 2 Permutada=\\'', sentenca2Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca2TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca2Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca2Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaPermutado = tokenizer.tokenize(sentenca3Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3TokenizadaPermutado)\n",
        "embeddingSentenca3Permutado = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, sentenca3Permutado, tokenizer)\n",
        "print('\\nSentença 3 Permutada=\\'', sentenca3Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca3TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca3Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca3Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaPermutado = tokenizer.tokenize(sentenca4Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4TokenizadaPermutado)\n",
        "embeddingSentenca4Permutado = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, sentenca4Permutado, tokenizer)\n",
        "print('\\nSentença 4 Permutada=\\'', sentenca4Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingSentenca4Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Permutado))\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Bom Dia, professor.']\n",
            "\n",
            "Sentença 1 Permutada=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 4096])\n",
            "    Soma embeddings:  -292.39\n",
            "\n",
            "Sentença 2 Permutada=' Qual o conteúdo da prova? '\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 4096])\n",
            "    Soma embeddings:  -254.00\n",
            "\n",
            "Sentença 3 Permutada=' Vai cair tudo na prova? '\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 14 e término em 19\n",
            "    Formato modelo : torch.Size([6, 4096])\n",
            "    Soma embeddings:  -260.99\n",
            "\n",
            "Sentença 4 Permutada=' Bom Dia, professor. '\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 20 e término em 24\n",
            "    Formato modelo : torch.Size([5, 4096])\n",
            "    Soma embeddings:  -223.89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CsH6v4Dstso"
      },
      "source": [
        "### Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p5iSl9Nstso",
        "outputId": "75b0456f-5269-4b99-c056-d22b64fa81da"
      },
      "source": [
        "print('\\nSentença 4 Original=\\'', sentenca4Original, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca4TokenizadaOriginal)\n",
        "print('    Formato modelo :', embeddingSentenca4Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca4Original))\n",
        "print('    Os 4 primeiros embeddings:', str(embeddingSentenca4Original[:4]))\n",
        "\n",
        "print('\\nSentença 1 Permutada=\\'', sentenca1Permutado, '\\'')\n",
        "print('    Sentença tokenizada:', sentenca1TokenizadaPermutado)\n",
        "print('    Formato modelo :', embeddingSentenca1Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingSentenca1Permutado))\n",
        "print('    Os 4 primeiros embeddings:', str(embeddingSentenca1Permutado[:4]))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sentença 4 Original=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 4096])\n",
            "    Soma embeddings:  -288.69\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.4339, -0.7168,  0.1173,  ...,  0.3840, -0.0936,  0.1355],\n",
            "        [-0.6806, -1.0194, -0.0765,  ...,  0.2201,  0.2731, -0.0384],\n",
            "        [ 1.1384, -0.8541,  0.8992,  ...,  0.5247,  0.2278,  0.0653],\n",
            "        [ 0.9206, -0.9862,  0.1347,  ...,  0.4393, -0.4792,  0.1743]])\n",
            "\n",
            "Sentença 1 Permutada=' Aguardo uma resposta, João. '\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 4096])\n",
            "    Soma embeddings:  -292.39\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.4472, -0.7700, -0.0164,  ...,  0.5051, -0.1387,  0.1486],\n",
            "        [-0.6242, -1.0259, -0.2210,  ...,  0.0995,  0.2928, -0.0829],\n",
            "        [ 1.3008, -1.0530,  0.9163,  ...,  0.5652,  0.1623,  0.0638],\n",
            "        [ 0.9188, -1.0255,  0.2020,  ...,  0.5125, -0.5088,  0.1882]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_2A3Odfstsp"
      },
      "source": [
        "### Diferença entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4BzBeiXstsp"
      },
      "source": [
        "#### Calcula a média da diferença entre os embeddings das sentenças do documento original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpqIIsoOstsp",
        "outputId": "bcec890f-a366-490d-8aea-eb3c9d0d280c"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSsub = 0\n",
        "\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, Si, tokenizer)\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, Sj, tokenizer)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSi=\", mediaCamadasSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSj=\", mediaCamadasSj.shape)\n",
        "  \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    Ssub = torch.sub(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "\n",
        "    somaSsub = somaSsub + torch.mean(Ssub)\n",
        "\n",
        "DsubOriginal = float(somaSsub)/float(n-1)\n",
        "print(\"Dsub Original:\", DsubOriginal)\n",
        "    \n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dsub Original: -0.0003163199095676343\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09MtlE09stsr"
      },
      "source": [
        "#### Calcula a média da diferença entre os embeddings das sentenças do documento permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cfKK8fBstsr",
        "outputId": "2e0b3848-7e34-4a97-c674-50951a3237d2"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSsub = 0\n",
        "\n",
        "for i in range(np-1):\n",
        "     # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, Si, tokenizer)\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, Sj, tokenizer)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSi=\", mediaCamadasSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSj=\", mediaCamadasSj.shape)\n",
        "  \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    Ssub = torch.sub(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "\n",
        "    somaSsub = somaSsub + torch.mean(Ssub)\n",
        "\n",
        "DsubPermutado = float(somaSsub)/float(np-1)\n",
        "print(\"Dsub permutado:\", DsubPermutado)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Bom Dia, professor.']\n",
            "Quantidade de sentenças: 4\n",
            "Dsub permutado: 0.00024482708734770614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkeP9fBbstsr"
      },
      "source": [
        "#### Compara as médias da subtração dos embeddings das sentenças do documento original e permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nvqvilIstsr",
        "outputId": "a4baffef-7334-4b38-c060-6976e9818899"
      },
      "source": [
        "print(\"Dsub Original :\", DsubOriginal)\n",
        "print(\"Dsub Permutado:\", DsubPermutado)\n",
        "\n",
        "if (DsubOriginal < DsubPermutado):\n",
        "    print(\"Documento original tem menor diferença entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor diferença entre as entre as sentenças!\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dsub Original : -0.0003163199095676343\n",
            "Dsub Permutado: 0.00024482708734770614\n",
            "Documento original tem menor diferença entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trQHdMjIstss"
      },
      "source": [
        "### Diferença absoluta entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5yNQi5wstss"
      },
      "source": [
        "#### Calcula a média da diferença absoluta entre os embeddings das sentenças do documento original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9M3EfcXstss",
        "outputId": "24c505e9-4cf9-4594-9bcf-969213daad1f"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSsubabs = 0\n",
        "\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, Si, tokenizer)\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, Sj, tokenizer)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSi=\", mediaCamadasSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSj=\", mediaCamadasSj.shape)\n",
        "  \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    Ssubabs = abs(torch.sub(mediaEmbeddingSi, mediaEmbeddingSj))\n",
        "\n",
        "    somaSsubabs = somaSsubabs + torch.mean(Ssubabs)\n",
        "\n",
        "DsubabsOriginal = float(somaSsubabs)/float(n-1)\n",
        "print(\"Dsubabs Original:\", DsubabsOriginal)\n",
        "    \n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dsubabs Original: 0.22927141189575195\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-uobe9hstss"
      },
      "source": [
        "#### Calcula a média da diferença absoluta entre os embeddings das sentenças do documento permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80-Jc4Dxstss",
        "outputId": "a3b15d1e-efd3-453a-a210-972b2d82c08a"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSsubabs = 0\n",
        "\n",
        "for i in range(np-1):\n",
        "     # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, Si, tokenizer)\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, Sj, tokenizer)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSi=\", mediaCamadasSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSj=\", mediaCamadasSj.shape)\n",
        "  \n",
        "    # Subtração entre os embeddings de Si e Sj\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    Ssubabs = abs(torch.sub(mediaEmbeddingSi, mediaEmbeddingSj))\n",
        "\n",
        "    somaSsubabs = somaSsubabs + torch.mean(Ssubabs)\n",
        "\n",
        "DsubabsPermutado = float(somaSsubabs)/float(np-1)\n",
        "print(\"Ssubabs permutado:\", DsubabsPermutado)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Bom Dia, professor.']\n",
            "Quantidade de sentenças: 4\n",
            "Ssubabs permutado: 0.23909521102905273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SinZot06stst"
      },
      "source": [
        "#### Compara as médias da subtração dos embeddings das sentenças do documento original e permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQAlNnJ8stst",
        "outputId": "8764b130-04c4-44b3-c26f-52917e10feaa"
      },
      "source": [
        "print(\"Dsubabs Original :\", DsubabsOriginal)\n",
        "print(\"Dsubabs Permutado:\", DsubabsPermutado)\n",
        "\n",
        "if (DsubabsOriginal < DsubabsPermutado):\n",
        "    print(\"Documento original tem menor diferença absoluta entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor diferença absoluta entre as entre as sentenças!\")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dsubabs Original : 0.22927141189575195\n",
            "Dsubabs Permutado: 0.23909521102905273\n",
            "Documento original tem menor diferença absoluta entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAmvTHL5stsy"
      },
      "source": [
        "### Similaridade de cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9uCR8N9stsy"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCoseno(sentenca1, sentenca2):\n",
        "  similaridade = 1 - cosine(sentenca1, sentenca2)\n",
        "  return similaridade"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFIbYBfFstsy"
      },
      "source": [
        "#### Calcula a média da similaridade de cosseno entre os embeddings das sentenças do documento original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "venwNW5Ksts1",
        "outputId": "e8375eaf-941b-49f2-e257-f17817595442"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSsim = 0\n",
        "\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, Si, tokenizer)\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_original, documento_texto_original, Sj, tokenizer)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSi=\", mediaCamadasSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSj=\", mediaCamadasSj.shape)\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    Ssim = similaridadeCoseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: <768 ou 1024>\n",
        "    \n",
        "    somaSsim = somaSsim + Ssim\n",
        "\n",
        "DsimOriginal = float(somaSsim)/float(n-1)\n",
        "print(\"Dsim Original:\", DsimOriginal)\n",
        "    \n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Dsim Original: 0.8178287347157797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jlhTBpOsts2"
      },
      "source": [
        "#### Calcula a média da similaridade de cosseno entre as sentenças dos embeddings do documento permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbpKpdFksts2",
        "outputId": "d3a2a466-d07e-48e8-d467-78e12c356b67"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSsim = 0\n",
        "\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    embeddingSi = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, Si, tokenizer)\n",
        "    embeddingSj = getEmbeddingSentencaDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, Sj, tokenizer)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSi=\", mediaCamadasSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaCamadasSj=\", mediaCamadasSj.shape)\n",
        "  \n",
        "   # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    Ssim = similaridadeCoseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: <768 ou 1024>\n",
        "    \n",
        "    somaSsim = somaSsim + Ssim\n",
        "\n",
        "DsimPermutado = float(somaSsim)/float(np-1)\n",
        "print(\"Dsim Original:\", DsimPermutado)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Bom Dia, professor.']\n",
            "Quantidade de sentenças: 4\n",
            "Dsim Original: 0.8008809288342794\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akgjIi9vsts2"
      },
      "source": [
        "#### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7lxHeihsts3",
        "outputId": "02a01a85-3f9e-4a2b-c3b2-63735ca51db8"
      },
      "source": [
        "print(\"Dsim Original :\", DsimOriginal)\n",
        "print(\"Dsim Permutado:\", DsimPermutado)\n",
        "\n",
        "if (DsimOriginal > DsimPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as entre as sentenças!\")"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dsim Original : 0.8178287347157797\n",
            "Dsim Permutado: 0.8008809288342794\n",
            "Documento original tem maior similaridade de cosseno entre as sentenças!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_ZX4-mywkGj"
      },
      "source": [
        "### Resumo\n",
        "\n",
        "- Resultado das medidas utilizando a última camada do BERT\n",
        "- Documento  :   Original            Permutado\n",
        "- Ssub       :   0.00012555          -0.00004393\n",
        "- Subabs     :   0.26808643          0.27730381\n",
        "- Ssim       :   0.78288462          0.76761355"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQQVs67DwkGl",
        "outputId": "a9fc16e1-ca7f-43ab-d582-8cffec70c239"
      },
      "source": [
        "print(\"Resultado das medidas utilizando a concatenação das 4 últimas camadas do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print('Dsub       :   {:.8f}          {:.8f}'.format(DsubOriginal,DsubPermutado))\n",
        "print('Dubabs     :   {:.8f}          {:.8f}'.format(DsubabsOriginal,DsubabsPermutado))\n",
        "print('Dsim       :   {:.8f}          {:.8f}'.format(DsimOriginal,DsimPermutado))\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resultado das medidas utilizando a concatenação das 4 últimas camadas do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Dsub       :   -0.00031632          0.00024483\n",
            "Dubabs     :   0.22927141          0.23909521\n",
            "Dsim       :   0.81782873          0.80088093\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}