{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2e47f388094a4ea09a509f8b3e0b80e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_afde6320ac8949d8bd5c3ff05ed6c4bd",
              "IPY_MODEL_35be7c16210f4f52a8f763a93af65668",
              "IPY_MODEL_6e8644dbec1946509f0094ed378ecc7d"
            ],
            "layout": "IPY_MODEL_bd58717d99e84aa68c29e74ff5a68933"
          }
        },
        "afde6320ac8949d8bd5c3ff05ed6c4bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb6111b21e244311995b20ce7822aeea",
            "placeholder": "​",
            "style": "IPY_MODEL_e44915d3d7af41c49df3c5443c0c5b38",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "35be7c16210f4f52a8f763a93af65668": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_846505f181de43ecadeffd9e166c03e2",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9e18ae3f2634942b04380077a200ad9",
            "value": 647
          }
        },
        "6e8644dbec1946509f0094ed378ecc7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b890b4c551cd44ea97e8b737fcaca58f",
            "placeholder": "​",
            "style": "IPY_MODEL_d874ec26b5a9449ca93f5ce1f0469143",
            "value": " 647/647 [00:00&lt;00:00, 7.83kB/s]"
          }
        },
        "bd58717d99e84aa68c29e74ff5a68933": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb6111b21e244311995b20ce7822aeea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e44915d3d7af41c49df3c5443c0c5b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "846505f181de43ecadeffd9e166c03e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9e18ae3f2634942b04380077a200ad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b890b4c551cd44ea97e8b737fcaca58f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d874ec26b5a9449ca93f5ce1f0469143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64aef8becb2548e28f38c4a8847a5fd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbe32c9d106347a793ab330529ea1f82",
              "IPY_MODEL_b6ca93ecda19498ca737c1bfebcb67f9",
              "IPY_MODEL_72f941e65d8848ba8b13e074bfd7c7be"
            ],
            "layout": "IPY_MODEL_c726070b47314c4685681f346514a962"
          }
        },
        "dbe32c9d106347a793ab330529ea1f82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_920e88ed70d04c058d173ff1fcb04060",
            "placeholder": "​",
            "style": "IPY_MODEL_dbd792f0cbd74918a47aebe30c83ed4d",
            "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
          }
        },
        "b6ca93ecda19498ca737c1bfebcb67f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0db1e82af9624d5c8e346371b871b98d",
            "max": 438235074,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_247141eaa1d44a2e8cfab09cf6e4f4e9",
            "value": 438235074
          }
        },
        "72f941e65d8848ba8b13e074bfd7c7be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aaf2cdd0311452caaa557038a0ef4b4",
            "placeholder": "​",
            "style": "IPY_MODEL_85a4dd123b1a47caad24ae707a188177",
            "value": " 438M/438M [00:20&lt;00:00, 22.5MB/s]"
          }
        },
        "c726070b47314c4685681f346514a962": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "920e88ed70d04c058d173ff1fcb04060": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbd792f0cbd74918a47aebe30c83ed4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0db1e82af9624d5c8e346371b871b98d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "247141eaa1d44a2e8cfab09cf6e4f4e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8aaf2cdd0311452caaa557038a0ef4b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85a4dd123b1a47caad24ae707a188177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98abd1f8b7da4820aedb90487f4f14ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b7981da3b694a1f98ccc37ee454af14",
              "IPY_MODEL_4b0824cb296846dfa98755dc1151609b",
              "IPY_MODEL_f2ff309bfb484f6dae0e5023443182b4"
            ],
            "layout": "IPY_MODEL_a9b7f7970e224cfca8c01745d9b8721b"
          }
        },
        "1b7981da3b694a1f98ccc37ee454af14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78b1d8895ab4403093134adeacdbf7ec",
            "placeholder": "​",
            "style": "IPY_MODEL_cef163e47758419f8da704b383d05ab6",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "4b0824cb296846dfa98755dc1151609b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b4319f59fc04452956a0168a5991f89",
            "max": 209528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1cca50017b334f4680acd937e76e4aef",
            "value": 209528
          }
        },
        "f2ff309bfb484f6dae0e5023443182b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4473fe5741741afb7209ebe98b8c63a",
            "placeholder": "​",
            "style": "IPY_MODEL_9dc7049e281c4081b663b6847c7ab20f",
            "value": " 210k/210k [00:00&lt;00:00, 380kB/s]"
          }
        },
        "a9b7f7970e224cfca8c01745d9b8721b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78b1d8895ab4403093134adeacdbf7ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cef163e47758419f8da704b383d05ab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b4319f59fc04452956a0168a5991f89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cca50017b334f4680acd937e76e4aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4473fe5741741afb7209ebe98b8c63a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dc7049e281c4081b663b6847c7ab20f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "815fd79f45444e67b7d1fdc682c2c9b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cab8c8eea5454924b2e62563ee5c068f",
              "IPY_MODEL_c9991313eaf74278b421ba26ed070aee",
              "IPY_MODEL_5545fed192fe4bb8848ade20405a3642"
            ],
            "layout": "IPY_MODEL_eeacb88c1c34428bb20059c4f77005b0"
          }
        },
        "cab8c8eea5454924b2e62563ee5c068f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a207986c5ad44bd2944a360e31beb482",
            "placeholder": "​",
            "style": "IPY_MODEL_c65ceed4599944ed8771509a3d25e68d",
            "value": "Downloading (…)in/added_tokens.json: 100%"
          }
        },
        "c9991313eaf74278b421ba26ed070aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb37a8aced0646d4be7d438cbea1fbac",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_914ff92cb539455ab31cb49b615a0878",
            "value": 2
          }
        },
        "5545fed192fe4bb8848ade20405a3642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0bde143fa214cc19f18a3073aae767d",
            "placeholder": "​",
            "style": "IPY_MODEL_d78f410ce1de41ef9498c73fbef2f6f0",
            "value": " 2.00/2.00 [00:00&lt;00:00, 86.8B/s]"
          }
        },
        "eeacb88c1c34428bb20059c4f77005b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a207986c5ad44bd2944a360e31beb482": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c65ceed4599944ed8771509a3d25e68d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb37a8aced0646d4be7d438cbea1fbac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "914ff92cb539455ab31cb49b615a0878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0bde143fa214cc19f18a3073aae767d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d78f410ce1de41ef9498c73fbef2f6f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25d221ff230f47deadbd8a62053c71bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd1c1bd8c9054f329afb754342287612",
              "IPY_MODEL_1157167354b24b4cb99aab53eef48020",
              "IPY_MODEL_92580847b4ae46fd8795bba888b4d80b"
            ],
            "layout": "IPY_MODEL_09dbdf1297cf490589a0682cdf8af1a9"
          }
        },
        "cd1c1bd8c9054f329afb754342287612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4aaa2ab2724477bbd365f3f603800d0",
            "placeholder": "​",
            "style": "IPY_MODEL_3eb4a228739f496c8ae97154320e47c4",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "1157167354b24b4cb99aab53eef48020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c77c3879e3b44efaca25125fbdae30e",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1cd444d12754a3b8bbea83ebdde7e5c",
            "value": 112
          }
        },
        "92580847b4ae46fd8795bba888b4d80b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10a1a0ba643e4d08a2d1ce71d7774e4f",
            "placeholder": "​",
            "style": "IPY_MODEL_26226505d8ab4e75aaefd6ed50cf7328",
            "value": " 112/112 [00:00&lt;00:00, 3.24kB/s]"
          }
        },
        "09dbdf1297cf490589a0682cdf8af1a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4aaa2ab2724477bbd365f3f603800d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3eb4a228739f496c8ae97154320e47c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c77c3879e3b44efaca25125fbdae30e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1cd444d12754a3b8bbea83ebdde7e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10a1a0ba643e4d08a2d1ce71d7774e4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26226505d8ab4e75aaefd6ed50cf7328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ea31f763de84100a5313b963c79e417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1887176ad5f845bab7f231e932d1554b",
              "IPY_MODEL_22d2a75f85cf47b48764697a5446d976",
              "IPY_MODEL_9b1543afbdc1469c89f717fc0c8c4448"
            ],
            "layout": "IPY_MODEL_b6e4317fd2054edda5412935ec8d8a2a"
          }
        },
        "1887176ad5f845bab7f231e932d1554b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41784d607f1a40709f281cf226de5f14",
            "placeholder": "​",
            "style": "IPY_MODEL_f173a4753cfc46ae9c057a9279b07e48",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "22d2a75f85cf47b48764697a5446d976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bada40831114b459adf4ea7e4c0876b",
            "max": 43,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bb0232f83814c7ebd48cace4857ac60",
            "value": 43
          }
        },
        "9b1543afbdc1469c89f717fc0c8c4448": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4904aaa9e87247fd9355206bc66ba897",
            "placeholder": "​",
            "style": "IPY_MODEL_7a77e16bdf5b404a9cf84db27b7fda3e",
            "value": " 43.0/43.0 [00:00&lt;00:00, 1.32kB/s]"
          }
        },
        "b6e4317fd2054edda5412935ec8d8a2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41784d607f1a40709f281cf226de5f14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f173a4753cfc46ae9c057a9279b07e48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bada40831114b459adf4ea7e4c0876b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bb0232f83814c7ebd48cace4857ac60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4904aaa9e87247fd9355206bc66ba897": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a77e16bdf5b404a9cf84db27b7fda3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osmarbraz/exemplos_BERT/blob/main/ExemplosWordEmbeddingContextualBERT_pt_br_sentenca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78HE8FLsKN9Q"
      },
      "source": [
        "#Exemplo de comparação de sentenças e o contexto baseado nas sentenças(pt-br) usando BERT Transformers by HuggingFace\n",
        "\n",
        "# **A execução pode ser feita através do menu Ambiente de Execução opção Executar tudo.**\n",
        "\n",
        "Exemplos de **Comparação de Sentenças** usando **BERT** em documentos originais e permutados utilizando suas sentenças. No final do notebook estão os exemplos com os documentos:\n",
        "\n",
        "*   documento original e permutado\n",
        "\n",
        "**Link biblioteca Huggingface:**\n",
        "https://github.com/huggingface/transformers\n",
        "\n",
        "\n",
        "**Artigo original BERT Jacob Devlin:**\n",
        "https://arxiv.org/pdf/1506.06724.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyxb5Px3p1-e"
      },
      "source": [
        "# 0 - Preparação do ambiente\n",
        "Preparação do ambiente para execução do exemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAPVtRXQqDim"
      },
      "source": [
        "###Tratamento de logs\n",
        "\n",
        "Método para tratamento dos logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcopxbGZqDip"
      },
      "source": [
        "# Biblioteca de logging\n",
        "import logging\n",
        "\n",
        "# Formatando a mensagem de logging\n",
        "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GjYtXcMnSAe"
      },
      "source": [
        "## Identificando o ambiente Colab\n",
        "\n",
        "Cria uma variável para identificar que o notebook está sendo executado no Google Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMiH0E3OnRa1"
      },
      "source": [
        "# Se estiver executando no Google Colaboratory\n",
        "import sys\n",
        "\n",
        "# Retorna true ou false se estiver no Google Colaboratory\n",
        "IN_COLAB = \"google.colab\" in sys.modules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RufkKnojlwzu"
      },
      "source": [
        "## Instalação do spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0LeiOTx0Dlk"
      },
      "source": [
        "https://spacy.io/\n",
        "\n",
        "Modelos do spaCy para português:\n",
        "https://spacy.io/models/pt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Fvx0TVRQUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "831cb503-08c1-4b14-8753-ec3d1d3df5fb"
      },
      "source": [
        "# Instala o spacy\n",
        "!pip install -U spacy==2.3.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==2.3.5\n",
            "  Downloading spacy-2.3.5-cp38-cp38-manylinux2014_x86_64.whl (10.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.5) (0.7.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.5) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.5) (1.22.4)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.5) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.5) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.5) (4.64.1)\n",
            "Collecting srsly<1.1.0,>=1.0.2\n",
            "  Downloading srsly-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.5) (1.0.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.5) (3.0.8)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.5) (2.25.1)\n",
            "Collecting thinc<7.5.0,>=7.4.1\n",
            "  Downloading thinc-7.4.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (1.26.14)\n",
            "Installing collected packages: plac, srsly, catalogue, thinc, spacy\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.5\n",
            "    Uninstalling srsly-2.4.5:\n",
            "      Successfully uninstalled srsly-2.4.5\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.7\n",
            "    Uninstalling thinc-8.1.7:\n",
            "      Successfully uninstalled thinc-8.1.7\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.4\n",
            "    Uninstalling spacy-3.4.4:\n",
            "      Successfully uninstalled spacy-3.4.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.3.5 which is incompatible.\n",
            "confection 0.0.4 requires srsly<3.0.0,>=2.4.0, but you have srsly 1.0.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed catalogue-1.0.2 plac-1.1.3 spacy-2.3.5 srsly-1.0.6 thinc-7.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35GwcgkOlWi3"
      },
      "source": [
        "Realiza o download e carrega os modelos necessários a biblioteca\n",
        "\n",
        "https://spacy.io/models/pt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4LqE5kTwDYm"
      },
      "source": [
        "# Definição do nome do arquivo do modelo\n",
        "#ARQUIVOMODELO = \"pt_core_news_sm\"\n",
        "#ARQUIVOMODELO = \"pt_core_news_md\"\n",
        "ARQUIVOMODELO = \"pt_core_news_lg\"\n",
        "\n",
        "# Definição da versão da spaCy\n",
        "#VERSAOSPACY = \"-3.0.0a0\"\n",
        "VERSAOSPACY = \"-2.3.0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ2KB3UCp-ws"
      },
      "source": [
        "#Baixa automaticamente o arquivo do modelo.\n",
        "#!python -m spacy download {ARQUIVOMODELO}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASk5iFeUp9LE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71afd634-34c6-41fa-b849-fc252e680f89"
      },
      "source": [
        "# Realiza o download do arquivo do modelo para o diretório corrente\n",
        "!wget https://github.com/explosion/spacy-models/releases/download/{ARQUIVOMODELO}{VERSAOSPACY}/{ARQUIVOMODELO}{VERSAOSPACY}.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-01 17:52:56--  https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-2.3.0/pt_core_news_lg-2.3.0.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/84940268/a899e480-ab07-11ea-831b-b5aa9cc04510?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230301%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230301T175257Z&X-Amz-Expires=300&X-Amz-Signature=bb974859e0b0c83d61018b45bd2b8946587cc9f1f83ffcd71d6f286ef57f9c9e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Dpt_core_news_lg-2.3.0.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-03-01 17:52:57--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/84940268/a899e480-ab07-11ea-831b-b5aa9cc04510?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230301%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230301T175257Z&X-Amz-Expires=300&X-Amz-Signature=bb974859e0b0c83d61018b45bd2b8946587cc9f1f83ffcd71d6f286ef57f9c9e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Dpt_core_news_lg-2.3.0.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 576599832 (550M) [application/octet-stream]\n",
            "Saving to: ‘pt_core_news_lg-2.3.0.tar.gz’\n",
            "\n",
            "pt_core_news_lg-2.3 100%[===================>] 549.89M  6.31MB/s    in 81s     \n",
            "\n",
            "2023-03-01 17:54:18 (6.79 MB/s) - ‘pt_core_news_lg-2.3.0.tar.gz’ saved [576599832/576599832]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu_LkF7Nfm8_"
      },
      "source": [
        "Descompacta o arquivo do modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9fCQQJGeVEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15ec8d5-955e-4e2d-ede4-4036555d0498"
      },
      "source": [
        "# Descompacta o arquivo do modelo\n",
        "!tar -xvf  /content/{ARQUIVOMODELO}{VERSAOSPACY}.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pt_core_news_lg-2.3.0/\n",
            "pt_core_news_lg-2.3.0/PKG-INFO\n",
            "pt_core_news_lg-2.3.0/setup.py\n",
            "pt_core_news_lg-2.3.0/setup.cfg\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/dependency_links.txt\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/PKG-INFO\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/SOURCES.txt\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/requires.txt\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/top_level.txt\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/not-zip-safe\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/__init__.py\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/parser/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/parser/cfg\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/parser/moves\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/parser/model\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/ner/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/ner/cfg\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/ner/moves\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/ner/model\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tokenizer\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/lookups.bin\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/vectors\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/key2row\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/lookups_extra.bin\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/strings.json\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/accuracy.json\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tagger/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tagger/cfg\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tagger/tag_map\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tagger/model\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/meta.json\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/meta.json\n",
            "pt_core_news_lg-2.3.0/MANIFEST.in\n",
            "pt_core_news_lg-2.3.0/meta.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovOx-3Wb-JJW"
      },
      "source": [
        "# Coloca a pasta do modelo descompactado em uma pasta de nome mais simples\n",
        "!mv /content/{ARQUIVOMODELO}{VERSAOSPACY}/{ARQUIVOMODELO}/{ARQUIVOMODELO}{VERSAOSPACY} /content/{ARQUIVOMODELO}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STHT2c89qvwK"
      },
      "source": [
        "Carrega o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbELnrpgA4T1"
      },
      "source": [
        "import spacy\n",
        "\n",
        "CAMINHOMODELO = \"/content/\" + ARQUIVOMODELO\n",
        "\n",
        "#nlp = spacy.load(CAMINHOMODELO)\n",
        "# Necessário \"tagger\" para encontrar os substantivos\n",
        "nlp = spacy.load(CAMINHOMODELO, disable=[\"tokenizer\", \"lemmatizer\", \"ner\", \"parser\", \"textcat\", \"custom\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFTTdqxKQ1Ay"
      },
      "source": [
        "Recupera os stopwords do spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBInu7ayQ31J"
      },
      "source": [
        "def getStopwords(nlp):\n",
        "    \"\"\"\n",
        "    Recupera as stop words do nlp(Spacy).\n",
        "    \n",
        "    Parâmetros:\n",
        "    `nlp` - Um modelo spaCy carregado.           \n",
        "    \"\"\"\n",
        "    \n",
        "    spacy_stopwords = nlp.Defaults.stop_words\n",
        "\n",
        "    return spacy_stopwords "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_EYNu-_RX7k"
      },
      "source": [
        "Lista dos stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUSaUJEWRbnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5691d5b-eaad-4729-f675-a5795cb1e5be"
      },
      "source": [
        "print(\"Quantidade de stopwords:\", len(getStopwords(nlp)))\n",
        "\n",
        "print(getStopwords(nlp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de stopwords: 413\n",
            "{'tente', 'muitos', 'ver', 'possivelmente', 'eventual', 'onde', 'inclusive', 'tudo', 'bom', 'usar', 'teu', 'todo', 'debaixo', 'ou', 'tive', 'vais', 'fazemos', 'à', 'desta', 'posição', 'sétimo', 'com', 'dizer', 'apoio', 'vossa', 'perto', 'devem', 'para', 'grandes', 'poderá', 'além', 'corrente', 'certeza', 'essas', 'seus', 'se', 'me', 'uns', 'ambas', 'esses', 'dezanove', 'ambos', 'enquanto', 'cento', 'num', 'des', 'fazem', 'somos', 'nível', 'as', 'fim', 'vez', 'fazer', 'mal', 'ora', 'isto', 'fostes', 'és', 'dar', 'naquela', 'pelas', 'pelo', 'pois', 'acerca', 'usa', 'tanta', 'máximo', 'do', 'mesmo', 'vinte', 'quanto', 'vêm', 'estivemos', 'vens', 'alguns', 'vós', 'sempre', 'até', 'certamente', 'tiveste', 'ontem', 'primeiro', 'baixo', 'nessa', 'conhecido', 'direita', 'último', 'foste', 'talvez', 'dezasseis', 'nosso', 'sua', 'cada', 'toda', 'outras', 'tu', 'quais', 'estará', 'vão', 'nunca', 'nesse', 'estive', 'tal', 'porque', 'próximo', 'final', 'te', 'terceiro', 'partir', 'nove', 'porquê', 'fui', 'então', 'tendes', 'aos', 'maiorias', 'ir', 'meu', 'mas', 'aí', 'obrigado', 'treze', 'grupo', 'sabe', 'segundo', 'dessa', 'todos', 'cinco', 'foram', 'qual', 'maioria', 'breve', 'tentei', 'inicio', 'qualquer', 'quieto', 'embora', 'vossos', 'é', 'fará', 'tua', 'tão', 'apontar', 'primeira', 'comprida', 'tens', 'dentro', 'esteve', 'menor', 'suas', 'tentar', 'vossas', 'nenhuma', 'nesta', 'dezassete', 'quando', 'um', 'vezes', 'catorze', 'estes', 'por', 'estiveste', 'atrás', 'geral', 'ainda', 'próxima', 'meio', 'sou', 'lugar', 'como', 'na', 'ademais', 'sexto', 'lá', 'quinta', 'nuns', 'quê', 'for', 'tenho', 'naquele', 'assim', 'exemplo', 'de', 'lhe', 'cima', 'vinda', 'pouco', 'bem', 'questão', 'pode', 'seu', 'algo', 'uma', 'ela', 'maior', 'coisa', 'sois', 'poder', 'das', 'vários', 'nos', 'tivemos', 'antes', 'logo', 'através', 'nossos', 'somente', 'numa', 'não', 'ao', 'agora', 'sexta', 'estiveram', 'eles', 'pela', 'tarde', 'muito', 'números', 'nós', 'apenas', 'caminho', 'três', 'põem', 'eu', 'dez', 'segunda', 'aqueles', 'relação', 'longe', 'põe', 'vindo', 'estas', 'seis', 'aquele', 'tipo', 'fazeis', 'fazia', 'entre', 'ligado', 'meses', 'são', 'todas', 'próprio', 'daquele', 'irá', 'menos', 'minhas', 'novo', 'ponto', 'querem', 'daquela', 'dá', 'lado', 'temos', 'elas', 'momento', 'ele', 'este', 'saber', 'seria', 'meus', 'zero', 'doze', 'aquilo', 'teve', 'dizem', 'favor', 'minha', 'cedo', 'umas', 'sétima', 'contra', 'tais', 'fora', 'forma', 'vos', 'veja', 'isso', 'pelos', 'estás', 'sete', 'foi', 'você', 'vosso', 'nada', 'grande', 'oitavo', 'sob', 'nas', 'quarto', 'estado', 'porquanto', 'conhecida', 'sem', 'obrigada', 'algumas', 'vocês', 'desde', 'demais', 'dezoito', 'número', 'está', 'ter', 'tiveram', 'da', 'neste', 'tem', 'outra', 'cá', 'diante', 'pontos', 'pouca', 'nossas', 'tentaram', 'novos', 'que', 'já', 'quatro', 'deve', 'pôde', 'bastante', 'aquelas', 'tivestes', 'porém', 'cujo', 'nossa', 'fazes', 'aqui', 'no', 'em', 'oito', 'onze', 'aquela', 'desse', 'teus', 'duas', 'povo', 'novas', 'podia', 'portanto', 'cuja', 'esse', 'só', 'nova', 'conselho', 'quinze', 'fez', 'posso', 'após', 'mais', 'fomos', 'às', 'contudo', 'dão', 'depois', 'estar', 'sim', 'têm', 'podem', 'vai', 'sobre', 'falta', 'ali', 'quieta', 'custa', 'vem', 'essa', 'ser', 'local', 'os', 'mês', 'dois', 'quinto', 'faz', 'também', 'estava', 'quer', 'comprido', 'diz', 'quem', 'quarta', 'outros', 'apoia', 'sistema', 'parte', 'dos', 'tempo', 'possível', 'deste', 'faço', 'tuas', 'puderam', 'valor', 'estão', 'disso', 'tanto', 'adeus', 'parece', 'estivestes', 'deverá', 'área', 'boa', 'esta', 'quero', 'terceira', 'era', 'pegar', 'sei', 'iniciar', 'nem', 'mil', 'estou', 'oitava'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqa-7WXBAw8q"
      },
      "source": [
        "## Instalação do BERT da Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCdqJCtQN52l"
      },
      "source": [
        "Instala a interface pytorch para o BERT by Hugging Face. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RfUN_KolV-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74eb5490-bb53-4059-9888-6db1969fd1ed"
      },
      "source": [
        "# Instala a última versão da biblioteca\n",
        "#!pip install transformers\n",
        "\n",
        "# Instala uma versão específica da biblioteca\n",
        "!pip install -U transformers==4.5.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.5.1\n",
            "  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.1) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.1) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.1) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.1) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.1) (3.9.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.1) (2.25.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.5.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.5.1) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.5.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.5.1) (1.26.14)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.5.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.5.1) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.5.1) (1.2.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=11fe312bec7a6ff27d5777bd99b3e9408c28e0b296cd43089841227fed0c2ba9\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1AiEg2vTBje"
      },
      "source": [
        "# 1 - Carregando BERT Pre-Treinado "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtCA05oSTBje"
      },
      "source": [
        "Instale a interface pytorch para o BERT by Hugging Face. (Esta biblioteca contém interfaces para outros modelos de linguagem pré-treinados, como o GPT e o GPT-2 da OpenAI.)\n",
        "\n",
        "Selecionamos a interface pytorch porque ela encontra um bom equilíbrio entre as APIs de alto nível (que são fáceis de usar, mas não fornecem informações sobre como as coisas funcionam) e o código do tensorflow (que contém muitos detalhes, mas muitas vezes nos desvia lições sobre o fluxo tensor, quando o objetivo aqui é o BERT!).\n",
        "\n",
        "Se você estiver executando esse código no Google Colab, precisará instalar esta biblioteca sempre que se reconectar; a célula a seguir cuidará disso para você."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ce70075-6c05-49ce-8269-fefdd99150cf",
        "id": "sAsND4wPTBjf"
      },
      "source": [
        "# Instala a última versão da biblioteca\n",
        "##!pip install transformers\n",
        "\n",
        "# Instala uma versão específica da biblioteca\n",
        "!pip install -U transformers==4.26.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.26.1\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.1) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.1) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.1) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.1) (2.25.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.1) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.1) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.1) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.1) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.1) (1.26.14)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.10.3\n",
            "    Uninstalling tokenizers-0.10.3:\n",
            "      Successfully uninstalled tokenizers-0.10.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.5.1\n",
            "    Uninstalling transformers-4.5.1:\n",
            "      Successfully uninstalled transformers-4.5.1\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSXImOxMPdNg"
      },
      "source": [
        "\n",
        "Agora vamos importar pytorch, o modelo pré-treinado BERT e um tokenizer BERT. Explicaremos o modelo BERT em detalhes em um tutorial posterior, mas este é o modelo pré-treinado lançado pelo Google, que funcionou por muitas e muitas horas na Wikipedia e no [Book Corpus](https://arxiv.org/pdf/1506.06724.pdf), um conjunto de dados contendo 10.000 livros de diferentes gêneros. Esse modelo é responsável (com uma pequena modificação) por superar os benchmarks de PNL em várias tarefas. O Google lançou algumas variações dos modelos BERT, mas o que usaremos aqui é o menor dos dois tamanhos disponíveis (\"base\" e \"large\") e ignora o maiúscula e minúsculo, portanto, \"uncased\". \"\n",
        "\n",
        "Por default(`do_lower_case=True`) todas as letras são colocadas para minúsculas. Para ignorar a conversão para minúsculo use o parâmetro `do_lower_case=False`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando o modelo Pré-treinado BERT\n",
        "\n",
        "Lista de modelos da comunidade:\n",
        "* https://huggingface.co/models\n",
        "\n",
        "Português(https://github.com/neuralmind-ai/portuguese-bert):  \n",
        "* **\"neuralmind/bert-base-portuguese-cased\"**\n",
        "* **\"neuralmind/bert-large-portuguese-cased\"**"
      ],
      "metadata": {
        "id": "MBGTMy8Ic7GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MODELO_BERT = \"bert-large-cased\"\n",
        "#MODELO_BERT = \"bert-base-cased\"\n",
        "#MODELO_BERT = \"neuralmind/bert-large-portuguese-cased\"\n",
        "MODELO_BERT = \"neuralmind/bert-base-portuguese-cased\"\n",
        "#MODELO_BERT = \"bert-base-multilingual-cased\"\n",
        "#MODELO_BERT = \"bert-base-multilingual-uncased\""
      ],
      "metadata": {
        "id": "2734mI5HfIbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import BertModel\n",
        "\n",
        "# Carrega o modelo\n",
        "model = BertModel.from_pretrained(MODELO_BERT,\n",
        "                                  output_attentions=False,\n",
        "                                  output_hidden_states=True)"
      ],
      "metadata": {
        "id": "sP07W-LLN8oS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "2e47f388094a4ea09a509f8b3e0b80e7",
            "afde6320ac8949d8bd5c3ff05ed6c4bd",
            "35be7c16210f4f52a8f763a93af65668",
            "6e8644dbec1946509f0094ed378ecc7d",
            "bd58717d99e84aa68c29e74ff5a68933",
            "fb6111b21e244311995b20ce7822aeea",
            "e44915d3d7af41c49df3c5443c0c5b38",
            "846505f181de43ecadeffd9e166c03e2",
            "d9e18ae3f2634942b04380077a200ad9",
            "b890b4c551cd44ea97e8b737fcaca58f",
            "d874ec26b5a9449ca93f5ce1f0469143",
            "64aef8becb2548e28f38c4a8847a5fd8",
            "dbe32c9d106347a793ab330529ea1f82",
            "b6ca93ecda19498ca737c1bfebcb67f9",
            "72f941e65d8848ba8b13e074bfd7c7be",
            "c726070b47314c4685681f346514a962",
            "920e88ed70d04c058d173ff1fcb04060",
            "dbd792f0cbd74918a47aebe30c83ed4d",
            "0db1e82af9624d5c8e346371b871b98d",
            "247141eaa1d44a2e8cfab09cf6e4f4e9",
            "8aaf2cdd0311452caaa557038a0ef4b4",
            "85a4dd123b1a47caad24ae707a188177"
          ]
        },
        "outputId": "fc9ba628-cd9c-488b-bd93-c2b0c82b2a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e47f388094a4ea09a509f8b3e0b80e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64aef8becb2548e28f38c4a8847a5fd8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando o tokenizador BERT"
      ],
      "metadata": {
        "id": "6tKcaIfReqdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O tokenizador utiliza WordPiece, veja em [artigo original](https://arxiv.org/pdf/1609.08144.pdf)."
      ],
      "metadata": {
        "id": "e8n7Z5s-QZF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import das bibliotecas\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Carrega o tokenizador\n",
        "tokenizer = BertTokenizer.from_pretrained(MODELO_BERT, \n",
        "                                          do_lower_case=False)"
      ],
      "metadata": {
        "id": "IR3EocJGNRX-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "98abd1f8b7da4820aedb90487f4f14ac",
            "1b7981da3b694a1f98ccc37ee454af14",
            "4b0824cb296846dfa98755dc1151609b",
            "f2ff309bfb484f6dae0e5023443182b4",
            "a9b7f7970e224cfca8c01745d9b8721b",
            "78b1d8895ab4403093134adeacdbf7ec",
            "cef163e47758419f8da704b383d05ab6",
            "8b4319f59fc04452956a0168a5991f89",
            "1cca50017b334f4680acd937e76e4aef",
            "b4473fe5741741afb7209ebe98b8c63a",
            "9dc7049e281c4081b663b6847c7ab20f",
            "815fd79f45444e67b7d1fdc682c2c9b2",
            "cab8c8eea5454924b2e62563ee5c068f",
            "c9991313eaf74278b421ba26ed070aee",
            "5545fed192fe4bb8848ade20405a3642",
            "eeacb88c1c34428bb20059c4f77005b0",
            "a207986c5ad44bd2944a360e31beb482",
            "c65ceed4599944ed8771509a3d25e68d",
            "bb37a8aced0646d4be7d438cbea1fbac",
            "914ff92cb539455ab31cb49b615a0878",
            "e0bde143fa214cc19f18a3073aae767d",
            "d78f410ce1de41ef9498c73fbef2f6f0",
            "25d221ff230f47deadbd8a62053c71bc",
            "cd1c1bd8c9054f329afb754342287612",
            "1157167354b24b4cb99aab53eef48020",
            "92580847b4ae46fd8795bba888b4d80b",
            "09dbdf1297cf490589a0682cdf8af1a9",
            "d4aaa2ab2724477bbd365f3f603800d0",
            "3eb4a228739f496c8ae97154320e47c4",
            "0c77c3879e3b44efaca25125fbdae30e",
            "e1cd444d12754a3b8bbea83ebdde7e5c",
            "10a1a0ba643e4d08a2d1ce71d7774e4f",
            "26226505d8ab4e75aaefd6ed50cf7328",
            "3ea31f763de84100a5313b963c79e417",
            "1887176ad5f845bab7f231e932d1554b",
            "22d2a75f85cf47b48764697a5446d976",
            "9b1543afbdc1469c89f717fc0c8c4448",
            "b6e4317fd2054edda5412935ec8d8a2a",
            "41784d607f1a40709f281cf226de5f14",
            "f173a4753cfc46ae9c057a9279b07e48",
            "1bada40831114b459adf4ea7e4c0876b",
            "0bb0232f83814c7ebd48cace4857ac60",
            "4904aaa9e87247fd9355206bc66ba897",
            "7a77e16bdf5b404a9cf84db27b7fda3e"
          ]
        },
        "outputId": "7b92b50c-f610-4a47-eacc-d4a7b3ffe66a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/210k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98abd1f8b7da4820aedb90487f4f14ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "815fd79f45444e67b7d1fdc682c2c9b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25d221ff230f47deadbd8a62053c71bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ea31f763de84100a5313b963c79e417"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU3wHzNUmmBP"
      },
      "source": [
        "# 2 - Funções auxiliares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWqMsrb-ew5T"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm98RoojJcqP"
      },
      "source": [
        "# Import das bibliotecas\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xaeX0oTVQ5t"
      },
      "source": [
        "##removeStopWords\n",
        "\n",
        "Remove as stopwords de um documento ou senteça."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIaQ9bzBVQ5t"
      },
      "source": [
        "def removeStopWord(documento, stopwords):\n",
        "    \"\"\"\n",
        "    Remove as stopwords de um documento.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `documento` - Um documento com stopwords.\n",
        "    `stopwords` - Uma lista com as stopwords.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remoção das stop words do documento\n",
        "    documento_sem_stopwords = [palavra for palavra in documento.split() if palavra.lower() not in stopwords]\n",
        "\n",
        "    # Concatena o documento sem os stopwords\n",
        "    documentoLimpo = \" \".join(documento_sem_stopwords)\n",
        "\n",
        "    # Retorna o documento\n",
        "    return documentoLimpo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7NAe8ogCf1y"
      },
      "source": [
        "## retornaRelevante\n",
        "\n",
        "Retorna somente os palavras do documento ou sentença do tipo especificado."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g7-8PVNWgNQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNNfykypChn-"
      },
      "source": [
        "def retornaPalavraRelevante(documento, nlp, classe_palavra_relevante=\"NOUN\"):\n",
        "    \"\"\"\n",
        "    Retorna somente os palavras do documento ou sentença do tipo especificado.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `documento` - Um documento com todas as palavras.\n",
        "    `nlp` - Processador de linguagem natural.\n",
        "    `classe_palavra_relevante` - Classe morfossintática da palavra relevante a ser selecionada.\n",
        "    \n",
        "    Retorno:\n",
        "    `documento_com_relevantes_concatenado` - Documento somente com as palavras relevantes.\n",
        "    \"\"\"\n",
        "  \n",
        "    # Realiza o parsing no documento usando spacy\n",
        "    doc = nlp(documento)\n",
        "\n",
        "    # Retorna a lista das palavras relevantes de um tipo\n",
        "    documentoComRelevantes = [token.text for token in doc if token.pos_ == classe_palavra_relevante]\n",
        "\n",
        "    # Concatena o documento com as palavras relevantes\n",
        "    documento_com_relevantes_concatenado = \" \".join(documentoComRelevantes)\n",
        "\n",
        "    # Retorna o documento\n",
        "    return documento_com_relevantes_concatenado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s42mgtmSZ8MR"
      },
      "source": [
        "## getEmbeddingsCamadas\n",
        "\n",
        "Funções que recuperam os embeddings das camadas:\n",
        "- Primeira camada;\n",
        "- Penúltima camada;\n",
        "- Ùltima camada;\n",
        "- Soma das 4 últimas camadas;\n",
        "- Concatenação das 4 últimas camadas;\n",
        "- Soma de todas as camadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgo3EBTRZ9-3"
      },
      "source": [
        "def getEmbeddingPrimeiraCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][0]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingPenultimaCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][-2]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingUltimaCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "     \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][-1]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado    \n",
        "\n",
        "def getEmbeddingSoma4UltimasCamadas(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  embedding_camadas = output[2][-4:]\n",
        "  # Saída: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "\n",
        "  # Usa o método `stack` para criar uma nova dimensão no tensor \n",
        "  # com a concateção dos tensores dos embeddings.        \n",
        "  #Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado_stack = torch.stack(embedding_camadas, dim=0)\n",
        "  # Saída: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  \n",
        "  # Realiza a soma dos embeddings de todos os tokens para as camadas\n",
        "  # Entrada: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  resultado = torch.sum(resultado_stack, dim=0)\n",
        "  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingConcat4UltimasCamadas(output):  \n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Cria uma lista com os tensores a serem concatenados\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> x <768 ou 1024>)  \n",
        "  # Lista com os tensores a serem concatenados\n",
        "  lista_concat = []\n",
        "  # Percorre os 4 últimos\n",
        "  for i in [-1,-2,-3,-4]:\n",
        "      # Concatena da lista\n",
        "      lista_concat.append(output[2][i])\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> x <768 ou 1024>)  \n",
        "  \n",
        "  # Realiza a concatenação dos embeddings de todos as camadas\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> x <768 ou 1024>)  \n",
        "  resultado = torch.cat(lista_concat, dim=-1)\n",
        "  # Saída: Entrada: (<1(lote)> x <qtde_tokens> x <3072 ou 4096>)  \n",
        "    \n",
        "  return resultado   \n",
        "\n",
        "def getEmbeddingSomaTodasAsCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "   \n",
        "  # Retorna todas as camadas descontando a primeira(0)\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  embedding_camadas = output[2][1:]\n",
        "  # Saída: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  # Usa o método `stack` para criar uma nova dimensão no tensor \n",
        "  # com a concateção dos tensores dos embeddings.        \n",
        "  #Entrada: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado_stack = torch.stack(embedding_camadas, dim=0)\n",
        "  # Saída: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "    \n",
        "  # Realiza a soma dos embeddings de todos os tokens para as camadas\n",
        "  # Entrada: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  resultado = torch.sum(resultado_stack, dim=0)\n",
        "  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "    \n",
        "  return resultado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7nx_eZ8hSlr"
      },
      "source": [
        "## getEmbeddingsVisual\n",
        "\n",
        "Função para gerar as coordenadas de plotagem a partir das sentenças de embeddings.\n",
        "\n",
        "Existe uma função para os tipos de camadas utilizadas:\n",
        "- Ùltima camada;\n",
        "- Soma das 4 últimas camadas;\n",
        "- Concatenação das 4 últimas camadas;\n",
        "- Soma de todas as camadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLdbOT8-g43V"
      },
      "source": [
        "def getEmbeddingsVisualUltimaCamada(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingUltimaCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, documento_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAf9lJJ2hZbt"
      },
      "source": [
        "def getEmbeddingsVisualSoma4UltimasCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSoma4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, documento_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XpwSN1ghpnz"
      },
      "source": [
        "def getEmbeddingsVisualConcat4UltimasCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingConcat4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, documento_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3KU1EFrnSPK"
      },
      "source": [
        "def getEmbeddingsVisualSomaTodasAsCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSomaTodasAsCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, documento_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8MjE0utzlZT"
      },
      "source": [
        "## getEmbeddings\n",
        "\n",
        "Função para gerar os embeddings de sentenças.\n",
        "\n",
        "Existe uma função para os tipos de camadas utilizadas:\n",
        "- Ùltima camada;\n",
        "- Soma das 4 últimas camadas;\n",
        "- Concatenação das 4 últimas camadas;\n",
        "- Soma de todas as camadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QcqOuwS067Q"
      },
      "source": [
        "def getEmbeddingsUltimaCamada(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingUltimaCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        " \n",
        "    return token_embeddings, documento_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK1wDGBl067Y"
      },
      "source": [
        "def getEmbeddingsSoma4UltimasCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSoma4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "   \n",
        "    return token_embeddings, documento_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hym19Hxr067Y"
      },
      "source": [
        "def getEmbeddingsConcat4UltimasCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingConcat4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    return token_embeddings, documento_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-PLZiUR067Z"
      },
      "source": [
        "def getEmbeddingsSomaTodasAsCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSomaTodasAsCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    return token_embeddings, documento_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFd1rse11DpZ"
      },
      "source": [
        "## getDocumentoTokenizado \n",
        "Retorna o documento tokenizado"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getDocumentoTokenizado(documento, tokenizador):\n",
        "\n",
        "    \"\"\"\n",
        "    Retorna um documento tokenizado e concatenado com tokens especiais \"[CLS]\" no início e o token \"[SEP]\" no fim para ser submetido ao BERT.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `documento` - Um documento a ser tokenizado para o BERT.\n",
        "    `tokenizador` - Tokenizador BERT.\n",
        "    \n",
        "    Retorno:\n",
        "    `documento_tokenizado` - Documento tokenizado.\n",
        "    \"\"\"\n",
        "\n",
        "    # Adiciona os tokens especiais.\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Documento tokenizado\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    return documento_tokenizado"
      ],
      "metadata": {
        "id": "lHSnquWmuDCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wvgXwN81RCz"
      },
      "source": [
        "## encontrarIndiceSubLista \n",
        "\n",
        "Retorna os índices de início e fim da sublista na lista"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abS44M4yvFxf"
      },
      "source": [
        "# Localiza os índices de início e fim de uma sublista em uma lista\n",
        "def encontrarIndiceSubLista(lista, sublista):\n",
        "\n",
        "    \"\"\"\n",
        "      Localiza os índices de início e fim de uma sublista em uma lista.\n",
        "    \n",
        "      Parâmetros:\n",
        "      `lista` - Uma lista.\n",
        "      `sublista` - Uma sublista a ser localizada na lista.\n",
        "    \"\"\"    \n",
        "    # https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore%E2%80%93Horspool_algorithm\n",
        "\n",
        "    # Recupera o tamanho da lista \n",
        "    h = len(lista)\n",
        "    # Recupera o tamanho da sublista\n",
        "    n = len(sublista)    \n",
        "    skip = {sublista[i]: n - i - 1 for i in range(n - 1)}\n",
        "    i = n - 1\n",
        "    while i < h:\n",
        "        for j in range(n):\n",
        "            if lista[i - j] != sublista[-j - 1]:\n",
        "                i += skip.get(lista[i], n)\n",
        "                break\n",
        "        else:\n",
        "            indice_inicio = i - n + 1\n",
        "            indice_fim = indice_inicio + len(sublista)-1\n",
        "            return indice_inicio, indice_fim\n",
        "    return -1, -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_pnIh1h1Z_J"
      },
      "source": [
        "## getEmbeddingSentencaEmbeddingDocumentoComTodasPalavras\n",
        "\n",
        "Retorna os embeddings de uma sentença com todas as palavras a partir dos embeddings do documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSQs3O5QpJSj"
      },
      "source": [
        "def getEmbeddingSentencaEmbeddingDocumentoComTodasPalavras(embedding_documento, documento, sentenca, tokenizador):\n",
        "  \n",
        "  # Tokeniza o documento\n",
        "  documento_tokenizado = getDocumentoTokenizado(documento, tokenizador)  \n",
        "  #print(documento_tokenizado)\n",
        "\n",
        "  # Tokeniza a sentença\n",
        "  sentenca_tokenizada = getDocumentoTokenizado(sentenca, tokenizador)\n",
        "  \n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentenca_tokenizada.remove(\"[CLS]\")\n",
        "  sentenca_tokenizada.remove(\"[SEP]\")  \n",
        "  #print(sentenca_tokenizada)\n",
        "  #print(len(sentenca_tokenizada))\n",
        "  \n",
        "  # Localiza os índices dos tokens da sentença no documento\n",
        "  inicio, fim = encontrarIndiceSubLista(documento_tokenizado,sentenca_tokenizada)\n",
        "  #print(\"Sentença inicia em:\", inicio, \"até\", fim) \n",
        " \n",
        "  # Recupera os embeddings dos tokens da sentença a partir dos embeddings do documento\n",
        "  embedding_sentenca = embedding_documento[inicio:fim+1]\n",
        "  #print(\"embedding_sentenca=\", embedding_sentenca.shape)\n",
        "  \n",
        "  # Retorna o embedding da sentença do documento\n",
        "  return embedding_sentenca"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd9xmB9jwZZN"
      },
      "source": [
        "## getEmbeddingSentencaEmbeddingDocumentoSemStopWord\n",
        "\n",
        "Retorna os embeddings de uma sentença sem stopwords a partir dos embeddings do documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5XVsCsdwZZP"
      },
      "source": [
        "def getEmbeddingSentencaEmbeddingDocumentoSemStopWord(embedding_documento, documento, sentenca, tokenizador):\n",
        "  \n",
        "  # Tokeniza o documento\n",
        "  documento_tokenizado = getDocumentoTokenizado(documento, tokenizador)  \n",
        "  #print(documento_tokenizado)\n",
        "  \n",
        "  # Remove as stopword da sentença\n",
        "  sentencaSemStopWord = removeStopWord(sentenca, getStopwords(nlp))\n",
        "\n",
        "  # Tokeniza a sentença sem stopword\n",
        "  sentenca_tokenizada_sem_stopword = getDocumentoTokenizado(sentencaSemStopWord, tokenizador)\n",
        "\n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentenca_tokenizada_sem_stopword.remove(\"[CLS]\")\n",
        "  sentenca_tokenizada_sem_stopword.remove(\"[SEP]\")  \n",
        "  #print(sentenca_tokenizada_sem_stopword)\n",
        "  #print(len(sentenca_tokenizada_sem_stopword))\n",
        "\n",
        "  # Tokeniza a sentença\n",
        "  sentenca_tokenizada = getDocumentoTokenizado(sentenca, tokenizador)\n",
        "  \n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentenca_tokenizada.remove(\"[CLS]\")\n",
        "  sentenca_tokenizada.remove(\"[SEP]\")  \n",
        "  #print(sentenca_tokenizada)\n",
        "  #print(len(sentenca_tokenizada))\n",
        "\n",
        "  # Localiza os índices dos tokens da sentença no documento\n",
        "  inicio, fim = encontrarIndiceSubLista(documento_tokenizado,sentenca_tokenizada)\n",
        "  #print(\"Sentença inicia em:\", inicio, \"até\", fim) \n",
        "   \n",
        "  # Recupera os embeddings dos tokens da sentença a partir dos embeddings do documento\n",
        "  embedding_sentenca = embedding_documento[inicio:fim+1]\n",
        "  #print(\"embedding_sentenca=\", embedding_sentenca.shape)\n",
        "\n",
        "  # Lista com os tensores selecionados\n",
        "  lista_tokens_selecionados = []\n",
        "  # Localizar os embeddings dos tokens da sentença tokenizada sem stop word na sentença \n",
        "  # Procura somente no intervalo da sentença\n",
        "  for i, tokenSentenca in enumerate(sentenca_tokenizada):\n",
        "    for tokenSentencaSemStopWord in sentenca_tokenizada_sem_stopword: \n",
        "      if tokenSentenca == tokenSentencaSemStopWord:        \n",
        "        lista_tokens_selecionados.append(embedding_sentenca[i:i+1])\n",
        "  \n",
        "  # Concatena os vetores da lista pela dimensão 0\n",
        "  embedding_sentenca_sem_stopword = torch.cat(lista_tokens_selecionados, dim=0)\n",
        "  #print(\"embedding_sentenca_sem_stopword:\",embedding_sentenca_sem_stopword.shape)\n",
        "\n",
        "  # Retorna o embedding da sentença do documento\n",
        "  return embedding_sentenca_sem_stopword"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgW4gfEzh34J"
      },
      "source": [
        "## getEmbeddingSentencaEmbeddingDocumentoSomenteRelevante\n",
        "\n",
        "Retorna os embeddings de uma sentença somente com as palavras relevantes a partir dos embeddings do documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHbQJhzSh34T"
      },
      "source": [
        "def getEmbeddingSentencaEmbeddingDocumentoSomenteRelevante(embedding_documento, documento, sentenca, tokenizador, classeRelevante=\"NOUN\"):\n",
        "  \n",
        "  # Tokeniza o documento\n",
        "  documento_tokenizado = getDocumentoTokenizado(documento, tokenizador)  \n",
        "  #print(documento_tokenizado)\n",
        "  \n",
        "  # Retorna as palavras relevantes da sentença da classe especificada\n",
        "  sentenca_somente_relevante = retornaPalavraRelevante(sentenca, nlp, classeRelevante)\n",
        "\n",
        "  # Tokeniza a sentença \n",
        "  sentenca_tokenizada_somente_relevante = getDocumentoTokenizado(sentenca_somente_relevante, tokenizador)\n",
        "\n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentenca_tokenizada_somente_relevante.remove(\"[CLS]\")\n",
        "  sentenca_tokenizada_somente_relevante.remove(\"[SEP]\")  \n",
        "  #print(sentenca_tokenizada_somente_relevante)\n",
        "  #print(len(sentenca_tokenizada_somente_relevante))\n",
        "\n",
        "  # Tokeniza a sentença\n",
        "  sentenca_tokenizada = getDocumentoTokenizado(sentenca, tokenizador)\n",
        "  \n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentenca_tokenizada.remove(\"[CLS]\")\n",
        "  sentenca_tokenizada.remove(\"[SEP]\")  \n",
        "  #print(sentenca_tokenizada)\n",
        "  #print(len(sentenca_tokenizada))\n",
        "\n",
        "  # Localiza os índices dos tokens da sentença no documento\n",
        "  inicio, fim = encontrarIndiceSubLista(documento_tokenizado,sentenca_tokenizada)\n",
        "  #print(\"Sentença inicia em:\", inicio, \"até\", fim) \n",
        "   \n",
        "  # Recupera os embeddings dos tokens da sentença a partir dos embeddings do documento\n",
        "  embedding_sentenca = embedding_documento[inicio:fim+1]\n",
        "  #print(\"embedding_sentenca=\", embedding_sentenca.shape)\n",
        "\n",
        "  # Lista com os tensores selecionados\n",
        "  lista_tokens_selecionados = []\n",
        "  # Localizar os embeddings dos tokens da sentença tokenizada sem stop word na sentença \n",
        "  # Procura somente no intervalo da sentença\n",
        "  for i, tokenSentenca in enumerate(sentenca_tokenizada):\n",
        "    for token_sentenca_somente_relevante in sentenca_tokenizada_somente_relevante: \n",
        "      if tokenSentenca == token_sentenca_somente_relevante:        \n",
        "        lista_tokens_selecionados.append(embedding_sentenca[i:i+1])\n",
        "  \n",
        "  # Concatena os vetores da lista pela dimensão 0\n",
        "  embedding_sentenca_relevante = torch.cat(lista_tokens_selecionados, dim=0)\n",
        "  #print(\"embedding_sentenca_relevante:\",embedding_sentenca_relevante.shape)\n",
        "\n",
        "  # Retorna o embedding da sentença do documento\n",
        "  return embedding_sentenca_relevante"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jccxPKRSbBoK"
      },
      "source": [
        "## getEmbeddingSentencaEmbeddingDocumento\n",
        "\n",
        "Retorna os embeddings de uma sentença com ou sem stopwords a partir dos embeddings do documento sem os StopWords.\n",
        "\n",
        "Filtros:\n",
        "- ALL - Sentença com todas as palavras\n",
        "- CLEAN - Sentença sem as stopwords\n",
        "- NOUN - Sentença somente com substantivos\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRPeALoFbCAx"
      },
      "source": [
        "def getEmbeddingSentencaEmbeddingDocumento(embedding_documento, documento, sentenca, tokenizador, filtro=\"ALL\"):\n",
        "  if filtro == \"ALL\":\n",
        "    return getEmbeddingSentencaEmbeddingDocumentoComTodasPalavras(embedding_documento, documento, sentenca, tokenizador)\n",
        "  else:\n",
        "    if filtro == \"CLEAN\":\n",
        "        return getEmbeddingSentencaEmbeddingDocumentoSemStopWord(embedding_documento, documento, sentenca, tokenizador)\n",
        "    else:\n",
        "      if filtro == \"NOUN\":\n",
        "        return getEmbeddingSentencaEmbeddingDocumentoSomenteRelevante(embedding_documento, documento, sentenca, tokenizador, classeRelevante=\"NOUN\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-3QzDMwmfiJ"
      },
      "source": [
        "# 3 - Exemplo sentenças de documento original e permutado utilizando embedding da última camada do BERT usando a estratégia MEAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTmrN_IRmfiO"
      },
      "source": [
        "## Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYKIVpzTmfiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b2a869-1e14-4abb-a5fc-3794ae72f7e5"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_original = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documento_original_concatenado = \" \".join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + documento_original_concatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLsHMNz9mfiQ"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pXg2A6zmfiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c16b841a-a831-4307-f58c-57c7eb4e1904"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM5GUtVNmfiR"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ePiuflemfiS"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qhHUUoAmfiS"
      },
      "source": [
        "Gera os embeddings para o documentoginal. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o970gzwhmfiS"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnXcn_dvmfiT"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSiGmlpjmfiT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13772e8f-2a9a-4da3-e5f3-8da1e39b4148"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFDtOmN_mfiV"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM4Aosw4mfiV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb93131-a62e-49d5-c7c8-80761798461b"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embedding_documento_original = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", embedding_documento_original.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([26, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T68Aje2tmfiW"
      },
      "source": [
        "Confirmando vetores dependentes do condocumento\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeNaW--hmfiW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ceab17-ca63-401f-b9b7-e3c777d8aef3"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVDBdrHWmfiX"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkZrVaVFmfiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4973b2e1-7744-41d5-8fbc-dc7d5fa38d8d"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_original = documento_original[0]\n",
        "sentenca2_original = documento_original[1]\n",
        "sentenca3_original = documento_original[2]\n",
        "sentenca4_original = documento_original[3]\n",
        "\n",
        "embedding_sentenca1_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca1_original, tokenizer)\n",
        "embedding_sentenca2_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca2_original, tokenizer)\n",
        "embedding_sentenca3_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca3_original, tokenizer)\n",
        "embedding_sentenca4_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca4_original, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento original.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1_original,\"-\", str(embedding_sentenca1_original[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1_original,\"-\", str(torch.sum(embedding_sentenca1_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2_original,\"-\", str(embedding_sentenca2_original[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2_original,\"-\", str(torch.sum(embedding_sentenca2_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3_original,\"-\", str(embedding_sentenca3_original[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3_original,\"-\", str(torch.sum(embedding_sentenca3_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4_original,\"-\", str(embedding_sentenca4_original[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4_original,\"-\", str(torch.sum(embedding_sentenca4_original[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: Bom Dia, professor. - tensor([[-0.0680, -0.4615,  0.3552,  ..., -0.3943, -0.1818, -0.4821],\n",
            "        [-0.1000, -0.0630,  0.0840,  ..., -0.6630,  0.1641, -0.8297],\n",
            "        [-0.3165,  0.4208,  0.2178,  ..., -0.4981,  0.1935, -0.3366],\n",
            "        [ 0.1248,  0.2383,  0.8987,  ..., -0.4940, -0.4578, -0.0353]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-13.8934)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.5894, -0.4310,  0.1449,  ...,  0.1601, -0.2918, -0.5303],\n",
            "        [ 0.1349, -0.2476,  0.4605,  ..., -0.3036, -0.6972,  0.2135],\n",
            "        [ 0.4359, -0.6972,  0.4066,  ...,  0.0177, -0.5852, -0.0615],\n",
            "        [ 0.0544,  0.1606,  0.4150,  ..., -0.3822, -0.1052, -0.0296]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-13.9466)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[-0.3652, -0.5015, -0.1626,  ...,  0.0161, -0.5967,  0.1675],\n",
            "        [ 0.1421,  0.1797, -0.0014,  ..., -0.5730, -0.5169,  0.3205],\n",
            "        [-0.1274, -0.0926, -0.1861,  ...,  0.4762, -0.4671,  0.2165],\n",
            "        [-0.2886,  0.4056,  0.6759,  ...,  0.5541, -0.3019, -0.1783]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-7.3501)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[-0.0835, -0.4042,  0.4330,  ..., -0.3271,  0.2262, -0.4599],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.4231, -0.3089, -0.5329],\n",
            "        [-0.5474, -0.0118,  0.3950,  ..., -0.0949, -0.3534, -0.3717],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ...,  0.1161,  0.0407, -0.6904]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-9.1314)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exqsxerrmfiY"
      },
      "source": [
        "Examinando os embeddings do documentoginal\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5py_A7lVmfiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d4ffb3-29de-46d9-9273-d0cda8b4328c"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_original = documento_original[0]\n",
        "sentenca2_original = documento_original[1]\n",
        "sentenca3_original = documento_original[2]\n",
        "sentenca4_original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1_tokenizada_original = tokenizer.tokenize(sentenca1_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1_tokenizada_original)\n",
        "embedding_sentenca1_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca1_original, tokenizer)\n",
        "print(\"\\nSentença 1 Original=\\\"\", sentenca1_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2_tokenizada_original = tokenizer.tokenize(sentenca2_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2_tokenizada_original)\n",
        "embedding_sentenca2_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca2_original, tokenizer)\n",
        "print(\"\\nSentença 2 Original=\\\"\", sentenca2_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca2_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca2_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3_tokenizada_original = tokenizer.tokenize(sentenca3_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3_tokenizada_original)\n",
        "embedding_sentenca3_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca3_original, tokenizer)\n",
        "print(\"\\nSentença 3 Original=\\\"\", sentenca3_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca3_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca3_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4_tokenizada_original = tokenizer.tokenize(sentenca4_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4_tokenizada_original)\n",
        "embedding_sentenca4_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca4_original, tokenizer)\n",
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_original))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 768])\n",
            "    Soma embeddings:  -15.02\n",
            "\n",
            "Sentença 2 Original=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -17.08\n",
            "\n",
            "Sentença 3 Original=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -10.65\n",
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -12.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hg9eKyjEfE5"
      },
      "source": [
        "## Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqHzON3PCa49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad57222-0bb1-4244-ef37-9a49f63fadbd"
      },
      "source": [
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_permutado = [documento_original[3],   # \"Aguardo uma resposta, João.\",\n",
        "             documento_original[1],             # \"Qual o conteúdo da prova?\",              \n",
        "             documento_original[0],             # \"Vai cair tudo na prova?\",\n",
        "             documento_original[2]]             # \"Bom Dia, professor.\"]     \n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documento_permutado_concatenado = \" \".join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + documento_permutado_concatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 Bom           8,399\n",
            " 15 Dia           3,616\n",
            " 16 ,               117\n",
            " 17 professor     2,917\n",
            " 18 .               119\n",
            " 19 Vai          20,805\n",
            " 20 cair          9,322\n",
            " 21 tudo          2,745\n",
            " 22 na              229\n",
            " 23 prova         2,310\n",
            " 24 ?               136\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Snv8-ACy47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae7b5d09-587e-4e2e-89a5-62d44f85eb63"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLv52fBItM3I"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMFR5tSiCy48"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDFnt2yntIgn"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je2zyykXCy49"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZSIxolutQSp"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z09FmGtnCy49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96e18fb3-9047-4158-886c-62253d31ef44"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aetb3LVYtXnI"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkk3Ix9kC93C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9276ce0f-933b-45d0-f092-a6857163cf2a"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embedding_documento_permutado = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", embedding_documento_permutado.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([26, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIsMSKxNIUg9"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkHr7wEFIUhA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b3d118b-447a-45f9-df1d-5ab629072d19"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_permutado = documento_permutado[0]\n",
        "sentenca2_permutado = documento_permutado[1]\n",
        "sentenca3_permutado = documento_permutado[2]\n",
        "sentenca4_permutado = documento_permutado[3]\n",
        "\n",
        "embedding_sentenca1_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca1_permutado, tokenizer)\n",
        "embedding_sentenca2_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca2_permutado, tokenizer)\n",
        "embedding_sentenca3_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca3_permutado, tokenizer)\n",
        "embedding_sentenca4_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca4_permutado, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento permutado.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1_permutado,\"-\", str(embedding_sentenca1_permutado[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1_original,\"-\", str(torch.sum(embedding_sentenca1_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2_permutado,\"-\", str(embedding_sentenca2_permutado[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2_permutado,\"-\", str(torch.sum(embedding_sentenca2_permutado[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3_permutado,\"-\", str(embedding_sentenca3_permutado[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3_permutado,\"-\", str(torch.sum(embedding_sentenca3_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4_permutado,\"-\", str(embedding_sentenca4_permutado[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4_permutado,\"-\", str(torch.sum(embedding_sentenca4_permutado[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.4811,  0.2881, -0.4057],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.5534, -0.3164, -0.3582],\n",
            "        [-0.7358, -0.0988,  0.5145,  ..., -0.1814, -0.3193, -0.3159],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.0720, -0.0478, -0.4931]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-13.8934)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.4295, -0.3426,  0.1005,  ...,  0.1568, -0.3225, -0.3857],\n",
            "        [ 0.1047, -0.1946,  0.4177,  ..., -0.3388, -0.7529,  0.1693],\n",
            "        [ 0.4377, -0.5952,  0.5448,  ...,  0.0463, -0.5607, -0.1783],\n",
            "        [ 0.0898,  0.1063,  0.4610,  ..., -0.3313, -0.1764,  0.0920]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-13.8054)\n",
            "\n",
            "Sentença 3: Bom Dia, professor. - tensor([[-0.0458, -0.4268,  0.2399,  ..., -0.4504, -0.2225, -0.6343],\n",
            "        [ 0.0650, -0.1385,  0.0230,  ..., -0.7129,  0.1112, -0.8076],\n",
            "        [-0.1147,  0.4119,  0.0659,  ..., -0.3666,  0.0509, -0.2134],\n",
            "        [ 0.1290,  0.2181,  0.7986,  ..., -0.4771, -0.4328,  0.0041]])\n",
            "Soma embedding Sentença3: Bom Dia, professor. - tensor(-7.3501)\n",
            "\n",
            "Sentença 4: Vai cair tudo na prova? - tensor([[-0.0414, -0.5222, -0.1612,  ..., -0.0763, -0.6968,  0.0127],\n",
            "        [ 0.4540,  0.0075, -0.0953,  ..., -0.4919, -0.4762,  0.2358],\n",
            "        [ 0.0887, -0.1633, -0.3073,  ...,  0.4414, -0.5028,  0.0877],\n",
            "        [-0.0159,  0.2363,  0.7514,  ...,  0.3057, -0.4097, -0.2738]])\n",
            "Soma embedding Sentença4: Vai cair tudo na prova? - tensor(-4.3409)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MINDqF2LDA9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040897fe-468e-4131-de69-3b6e76a5b270"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_permutado = documento_permutado[0]\n",
        "sentenca2_permutado = documento_permutado[1]\n",
        "sentenca3_permutado = documento_permutado[2]\n",
        "sentenca4_permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1_tokenizada_permutado = tokenizer.tokenize(sentenca1_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1_tokenizada_permutado)\n",
        "embedding_sentenca1_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca1_permutado, tokenizer)\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2_tokenizada_permutado = tokenizer.tokenize(sentenca2_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2_tokenizada_permutado)\n",
        "embedding_sentenca2_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca2_permutado, tokenizer)\n",
        "print(\"\\nSentença 2 Permutada=\\\"\", sentenca2_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca2_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca2_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3_tokenizada_permutado = tokenizer.tokenize(sentenca3_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3_tokenizada_permutado)\n",
        "embedding_sentenca3_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca3_permutado, tokenizer)\n",
        "print(\"\\nSentença 3 Permutada=\\\"\", sentenca3_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca3_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca3_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4_tokenizada_permutado = tokenizer.tokenize(sentenca4_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4_tokenizada_permutado)\n",
        "embedding_sentenca4_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca4_permutado, tokenizer)\n",
        "print(\"\\nSentença 4 Permutada=\\\"\", sentenca4_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_permutado))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -14.89\n",
            "\n",
            "Sentença 2 Permutada=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -17.25\n",
            "\n",
            "Sentença 3 Permutada=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 14 e término em 18\n",
            "    Formato modelo : torch.Size([5, 768])\n",
            "    Soma embeddings:  -15.11\n",
            "\n",
            "Sentença 4 Permutada=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 19 e término em 24\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -6.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UILLnj7KvHi"
      },
      "source": [
        "## Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eyEbV-7Kz6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1378c51b-229b-4706-cfd0-a758b9228d4d"
      },
      "source": [
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_original)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_original))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embedding_sentenca4_original[:4]))\n",
        "\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_permutado)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_permutado))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embedding_sentenca1_permutado[:4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -12.13\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0835, -0.4042,  0.4330,  ..., -0.3271,  0.2262, -0.4599],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.4231, -0.3089, -0.5329],\n",
            "        [-0.5474, -0.0118,  0.3950,  ..., -0.0949, -0.3534, -0.3717],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ...,  0.1161,  0.0407, -0.6904]])\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -14.89\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.4811,  0.2881, -0.4057],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.5534, -0.3164, -0.3582],\n",
            "        [-0.7358, -0.0988,  0.5145,  ..., -0.1814, -0.3193, -0.3159],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.0720, -0.0478, -0.4931]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JplTToZvDLiX"
      },
      "source": [
        "## Similaridade de cosseno entre os embeddings das sentenças\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html\n",
        "\n",
        "Intervalo de [-1,1] \n",
        "\n",
        "Vetores iguais a distância é igual 1.\n",
        "\n",
        "Vetores diferentes medida próxima de -1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xvq7pgFQhHv"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCosseno(embeddings1, embeddings2):\n",
        "    \"\"\"\n",
        "      Similaridade do cosseno dos embeddings dos textos.\n",
        "      \n",
        "      Parâmetros:\n",
        "      `embeddings1` - Um embedding a ser medido.\n",
        "      `embeddings2` - Um embedding a ser medido.\n",
        "    \"\"\"    \n",
        "    similaridade = 1 - cosine(embeddings1, embeddings2)\n",
        "\n",
        "    return similaridade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av6tZHt6DLiY"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOQ9vWuADLiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee1a548-6c33-40fd-86a2-32e190cf35bd"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosOriginal = float(somaScos)/float(n-1)\n",
        "print(\"Ccos Original:\", CcosOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.6200485428174337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmTaSFZNDLiY"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYIO7AXCDLiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49ebb0b6-b19b-44c8-b328-d40eb63cb76e"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "   # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosPermutado = float(somaScos)/float(np-1)\n",
        "print(\"Ccos Original:\", CcosPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.5409909089406332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiJ_9-KPDLiY"
      },
      "source": [
        "### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 1.\n",
        "- Documentos com sentenças diferenntes resulta uma medida menor que 1.\n",
        "- Documento com sentenças muito diferentes apresentam valores menores que 1.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de similaridade.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ1pRGiEDLiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d2b0d94-8fe9-4359-88c1-eb4c7b3271b6"
      },
      "source": [
        "print(\"Ccos Original :\", CcosOriginal)\n",
        "print(\"Ccos Permutado:\", CcosPermutado)\n",
        "\n",
        "if (CcosOriginal > CcosPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos Original : 0.6200485428174337\n",
            "Ccos Permutado: 0.5409909089406332\n",
            "Documento original tem maior similaridade de cosseno entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tLfL6BFDLiZ"
      },
      "source": [
        "## Distância euclidiana entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância L2 ou norma L2.\n",
        "\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html#scipy.spatial.distance.euclidean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKrR5hMNDLiZ"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distanciaEuclidiana(embeddings1, embeddings2):\n",
        "    \"\"\"\n",
        "    Distância euclidiana entre os embeddings dos documentos.\n",
        "    Possui outros nomes como distância L2 ou norma L2.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `embeddings1` - Um embedding a ser medido.\n",
        "    `embeddings2` - Um embedding a ser medido.\n",
        "    \"\"\"\n",
        "    \n",
        "    distancia = euclidean(embeddings1, embeddings2)\n",
        "    \n",
        "    return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scio7VcxDLiZ"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qPGyX3WDLiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12c723a8-4f59-4744-bbdc-156b21af7809"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucOriginal = float(somaSeuc)/float(n-1)\n",
        "print(\"Ceuc Original:\", CeucOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 7.117095311482747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eiJkQ9tDLiZ"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCa8HJAjDLiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a828b5dd-1021-4cc1-a8fa-8bda2e6447fd"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucPermutado = float(somaSeuc)/float(np-1)\n",
        "print(\"Ceuc Original:\", CeucPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 7.993526935577393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz5eaOkEDLiZ"
      },
      "source": [
        "### Compara as médias da distância euclidiana dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhelRMqGDLia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eea281d0-24da-4d44-a5b4-27b9ead5918f"
      },
      "source": [
        "print(\"Ceuc Original :\", CeucOriginal)\n",
        "print(\"Ceuc Permutado:\", CeucPermutado)\n",
        "\n",
        "if (CeucOriginal < CeucPermutado):\n",
        "    print(\"Documento original tem menor distância euclidiana entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância euclidiana entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ceuc Original : 7.117095311482747\n",
            "Ceuc Permutado: 7.993526935577393\n",
            "Documento original tem menor distância euclidiana entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqPCQJ24DLia"
      },
      "source": [
        "## Distância Manhattan entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "\n",
        "Igual a distância de subtração absoluta.\n",
        "\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cityblock.html#scipy.spatial.distance.cityblock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCjpuWTRDLib"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def distanciaManhattan(embeddings1, embeddings2):\n",
        "    \"\"\"\n",
        "    Distância Manhattan entre os embeddings dos documentos \n",
        "    Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `embeddings1` - Um embedding a ser medido.\n",
        "    `embeddings2` - Um embedding a ser medido.\n",
        "    \"\"\"\n",
        "    \n",
        "    distancia = cityblock(embeddings1, embeddings2)\n",
        "\n",
        "    return distancia\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKSaAqoZDLib"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFLah0Q9DLib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "832b5d4b-4cb5-4de2-fa19-c0a209a245f2"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanOriginal = float(somaSman)/float(n-1)\n",
        "print(\"Cman Original:\", CmanOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Cman Original: 153.63687133789062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43BxjteRDLib"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E6e9k1YDLic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbf6f495-37d0-43b8-b2d2-322366216647"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanPermutado = float(somaSman)/float(n-1)\n",
        "print(\"Cman Permutado:\", CmanPermutado)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Cman Permutado: 171.98432413736978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ-427FYDLic"
      },
      "source": [
        "### Compara as médias da distância de manhattan dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3snpqLtIDLic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1010619c-5ac5-4de9-b442-8515c626e95b"
      },
      "source": [
        "print(\"Cman Original :\", CmanOriginal)\n",
        "print(\"Cman Permutado:\", CmanPermutado)\n",
        "\n",
        "if (CmanOriginal < CmanPermutado):\n",
        "    print(\"Documento original tem menor distância de manhattan entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância de manhattan entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cman Original : 153.63687133789062\n",
            "Cman Permutado: 171.98432413736978\n",
            "Documento original tem menor distância de manhattan entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJRDXLHua9ce"
      },
      "source": [
        "## Resumo\n",
        "\n",
        "Resultado das medidas utilizando a última camada do BERT.\n",
        "\n",
        "Base(MEAN):\n",
        "- Ccos       :   0.62004846          0.54099079\n",
        "- Ceuc       :   7.11709849          7.99353107\n",
        "- Cman       :   153.63694255          171.98441060\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4jEPcWRa9ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82ee5205-0e34-4337-a416-d5384f0b5e94"
      },
      "source": [
        "print(\"Resultado das medidas utilizando a última camada do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print(\"Ccos       :   {:.8f}          {:.8f}\".format(CcosOriginal,CcosPermutado))\n",
        "print(\"Ceuc       :   {:.8f}          {:.8f}\".format(CeucOriginal,CeucPermutado))\n",
        "print(\"Cman       :   {:.8f}          {:.8f}\".format(CmanOriginal,CmanPermutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado das medidas utilizando a última camada do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Ccos       :   0.62004854          0.54099091\n",
            "Ceuc       :   7.11709531          7.99352694\n",
            "Cman       :   153.63687134          171.98432414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5NHv8JQ2Om8"
      },
      "source": [
        "# 4 - Exemplo sentenças de documento original e permutado utilizando embedding da última camada do BERT usando estratégia MAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuFaynIX2OnA"
      },
      "source": [
        "## Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ird39LBl2OnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6981831e-f58b-4caa-98a2-6a707ded1f47"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_original = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documento_original_concatenado = \" \".join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + documento_original_concatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCazJyHf2OnB"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkzJFTWX2OnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "122af54f-b153-4c58-bd16-062d6eb0da73"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H624EpGv2OnB"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBfLtHz92OnB"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM5nK5Zr2OnB"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0SPckuI2OnC"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceWI29Ij2OnC"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ReZJfwR2OnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c592adbc-1007-4e5f-ae41-ccbbbde27741"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysFToAty2OnC"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp8ImZM52OnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f5cf53-edc6-4676-f816-40d68f0c2154"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embedding_documento_original = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", embedding_documento_original.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([26, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhiX25CW2OnC"
      },
      "source": [
        "Confirmando vetores dependentes do condocumento\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7C8abOy2OnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29687310-3940-4ad5-ed0f-f199f3a394ea"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZCfusGL2OnD"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaOvIge52OnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b254d9f9-2307-4f48-e853-175617364f10"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_original = documento_original[0]\n",
        "sentenca2_original = documento_original[1]\n",
        "sentenca3_original = documento_original[2]\n",
        "sentenca4_original = documento_original[3]\n",
        "\n",
        "embedding_sentenca1_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca1_original, tokenizer)\n",
        "embedding_sentenca2_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca2_original, tokenizer)\n",
        "embedding_sentenca3_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca3_original, tokenizer)\n",
        "embedding_sentenca4_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca4_original, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento original.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1_original,\"-\", str(embedding_sentenca1_original[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1_original,\"-\", str(torch.sum(embedding_sentenca1_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2_original,\"-\", str(embedding_sentenca2_original[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2_original,\"-\", str(torch.sum(embedding_sentenca2_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3_original,\"-\", str(embedding_sentenca3_original[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3_original,\"-\", str(torch.sum(embedding_sentenca3_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4_original,\"-\", str(embedding_sentenca4_original[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4_original,\"-\", str(torch.sum(embedding_sentenca4_original[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: Bom Dia, professor. - tensor([[-0.0680, -0.4615,  0.3552,  ..., -0.3943, -0.1818, -0.4821],\n",
            "        [-0.1000, -0.0630,  0.0840,  ..., -0.6630,  0.1641, -0.8297],\n",
            "        [-0.3165,  0.4208,  0.2178,  ..., -0.4981,  0.1935, -0.3366],\n",
            "        [ 0.1248,  0.2383,  0.8987,  ..., -0.4940, -0.4578, -0.0353]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-13.8934)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.5894, -0.4310,  0.1449,  ...,  0.1601, -0.2918, -0.5303],\n",
            "        [ 0.1349, -0.2476,  0.4605,  ..., -0.3036, -0.6972,  0.2135],\n",
            "        [ 0.4359, -0.6972,  0.4066,  ...,  0.0177, -0.5852, -0.0615],\n",
            "        [ 0.0544,  0.1606,  0.4150,  ..., -0.3822, -0.1052, -0.0296]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-13.9466)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[-0.3652, -0.5015, -0.1626,  ...,  0.0161, -0.5967,  0.1675],\n",
            "        [ 0.1421,  0.1797, -0.0014,  ..., -0.5730, -0.5169,  0.3205],\n",
            "        [-0.1274, -0.0926, -0.1861,  ...,  0.4762, -0.4671,  0.2165],\n",
            "        [-0.2886,  0.4056,  0.6759,  ...,  0.5541, -0.3019, -0.1783]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-7.3501)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[-0.0835, -0.4042,  0.4330,  ..., -0.3271,  0.2262, -0.4599],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.4231, -0.3089, -0.5329],\n",
            "        [-0.5474, -0.0118,  0.3950,  ..., -0.0949, -0.3534, -0.3717],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ...,  0.1161,  0.0407, -0.6904]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-9.1314)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbGSdUzw2OnD"
      },
      "source": [
        "Examinando os embeddings do documento original\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okFUjfDG2OnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18b68f65-a5d1-4e5f-e968-a636841d6669"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_original = documento_original[0]\n",
        "sentenca2_original = documento_original[1]\n",
        "sentenca3_original = documento_original[2]\n",
        "sentenca4_original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1_tokenizada_original = tokenizer.tokenize(sentenca1_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1_tokenizada_original)\n",
        "embedding_sentenca1_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca1_original, tokenizer)\n",
        "print(\"\\nSentença 1 Original=\\\"\", sentenca1_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2_tokenizada_original = tokenizer.tokenize(sentenca2_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2_tokenizada_original)\n",
        "embedding_sentenca2_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca2_original, tokenizer)\n",
        "print(\"\\nSentença 2 Original=\\\"\", sentenca2_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca2_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca2_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3_tokenizada_original = tokenizer.tokenize(sentenca3_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3_tokenizada_original)\n",
        "embedding_sentenca3_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_permutado_concatenado, sentenca3_original, tokenizer)\n",
        "print(\"\\nSentença 3 Original=\\\"\", sentenca3_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca3_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca3_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4_tokenizada_original = tokenizer.tokenize(sentenca4_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4_tokenizada_original)\n",
        "embedding_sentenca4_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca4_original, tokenizer)\n",
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_original))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 768])\n",
            "    Soma embeddings:  -15.02\n",
            "\n",
            "Sentença 2 Original=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -17.08\n",
            "\n",
            "Sentença 3 Original=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -11.21\n",
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -12.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVL1lBnB2OnD"
      },
      "source": [
        "## Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uMC8h3F2OnE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b249bc9d-19df-4fbe-d172-c02067202dc7"
      },
      "source": [
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_permutado = [documento_original[3],   # \"Aguardo uma resposta, João.\",\n",
        "             documento_original[1],             # \"Qual o conteúdo da prova?\",              \n",
        "             documento_original[0],             # \"Vai cair tudo na prova?\",\n",
        "             documento_original[2]]             # \"Bom Dia, professor.\"]     \n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documento_permutado_concatenado = \" \".join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + documento_permutado_concatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 Bom           8,399\n",
            " 15 Dia           3,616\n",
            " 16 ,               117\n",
            " 17 professor     2,917\n",
            " 18 .               119\n",
            " 19 Vai          20,805\n",
            " 20 cair          9,322\n",
            " 21 tudo          2,745\n",
            " 22 na              229\n",
            " 23 prova         2,310\n",
            " 24 ?               136\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPjSfaUR2OnE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b88b3d8-2543-4eac-9b4d-3cc18ccc9603"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXp0WqZa2OnE"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuwXz5Ye2OnE"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUmYOo7s2OnE"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gLUe6tT2OnE"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS0h6i4t2OnE"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1kcPJU82OnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77aeab66-7f63-4303-ab53-cbf7c02234c7"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Xu6swe2OnF"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOljOpq52OnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c778ae01-619f-4845-d507-a054621abb2c"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embedding_documento_permutado = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", embedding_documento_permutado.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([26, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BlaBKoE2OnF"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWAJMgs72OnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "428412e2-96aa-4280-f63d-cd95203ab4a0"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_permutado = documento_permutado[0]\n",
        "sentenca2_permutado = documento_permutado[1]\n",
        "sentenca3_permutado = documento_permutado[2]\n",
        "sentenca4_permutado = documento_permutado[3]\n",
        "\n",
        "embedding_sentenca1_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca1_permutado, tokenizer)\n",
        "embedding_sentenca2_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca2_permutado, tokenizer)\n",
        "embedding_sentenca3_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca3_permutado, tokenizer)\n",
        "embedding_sentenca4_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca4_permutado, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento permutado.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1_permutado,\"-\", str(embedding_sentenca1_permutado[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1_original,\"-\", str(torch.sum(embedding_sentenca1_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2_permutado,\"-\", str(embedding_sentenca2_permutado[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2_permutado,\"-\", str(torch.sum(embedding_sentenca2_permutado[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3_permutado,\"-\", str(embedding_sentenca3_permutado[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3_permutado,\"-\", str(torch.sum(embedding_sentenca3_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4_permutado,\"-\", str(embedding_sentenca4_permutado[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4_permutado,\"-\", str(torch.sum(embedding_sentenca4_permutado[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.4811,  0.2881, -0.4057],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.5534, -0.3164, -0.3582],\n",
            "        [-0.7358, -0.0988,  0.5145,  ..., -0.1814, -0.3193, -0.3159],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.0720, -0.0478, -0.4931]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-13.8934)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.4295, -0.3426,  0.1005,  ...,  0.1568, -0.3225, -0.3857],\n",
            "        [ 0.1047, -0.1946,  0.4177,  ..., -0.3388, -0.7529,  0.1693],\n",
            "        [ 0.4377, -0.5952,  0.5448,  ...,  0.0463, -0.5607, -0.1783],\n",
            "        [ 0.0898,  0.1063,  0.4610,  ..., -0.3313, -0.1764,  0.0920]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-13.8054)\n",
            "\n",
            "Sentença 3: Bom Dia, professor. - tensor([[-0.0458, -0.4268,  0.2399,  ..., -0.4504, -0.2225, -0.6343],\n",
            "        [ 0.0650, -0.1385,  0.0230,  ..., -0.7129,  0.1112, -0.8076],\n",
            "        [-0.1147,  0.4119,  0.0659,  ..., -0.3666,  0.0509, -0.2134],\n",
            "        [ 0.1290,  0.2181,  0.7986,  ..., -0.4771, -0.4328,  0.0041]])\n",
            "Soma embedding Sentença3: Bom Dia, professor. - tensor(-12.2997)\n",
            "\n",
            "Sentença 4: Vai cair tudo na prova? - tensor([[-0.0414, -0.5222, -0.1612,  ..., -0.0763, -0.6968,  0.0127],\n",
            "        [ 0.4540,  0.0075, -0.0953,  ..., -0.4919, -0.4762,  0.2358],\n",
            "        [ 0.0887, -0.1633, -0.3073,  ...,  0.4414, -0.5028,  0.0877],\n",
            "        [-0.0159,  0.2363,  0.7514,  ...,  0.3057, -0.4097, -0.2738]])\n",
            "Soma embedding Sentença4: Vai cair tudo na prova? - tensor(-4.3409)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jktC1hfL2OnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4720db02-6d04-46bd-b702-1ced886da5dc"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_permutado = documento_permutado[0]\n",
        "sentenca2_permutado = documento_permutado[1]\n",
        "sentenca3_permutado = documento_permutado[2]\n",
        "sentenca4_permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1_tokenizada_permutado = tokenizer.tokenize(sentenca1_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1_tokenizada_permutado)\n",
        "embedding_sentenca1_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca1_permutado, tokenizer)\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2_tokenizada_permutado = tokenizer.tokenize(sentenca2_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2_tokenizada_permutado)\n",
        "embedding_sentenca2_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca2_permutado, tokenizer)\n",
        "print(\"\\nSentença 2 Permutada=\\\"\", sentenca2_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca2_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca2_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3_tokenizada_permutado = tokenizer.tokenize(sentenca3_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3_tokenizada_permutado)\n",
        "embedding_sentenca3_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca3_permutado, tokenizer)\n",
        "print(\"\\nSentença 3 Permutada=\\\"\", sentenca3_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca3_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca3_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4_tokenizada_permutado = tokenizer.tokenize(sentenca4_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4_tokenizada_permutado)\n",
        "embedding_sentenca4_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca4_permutado, tokenizer)\n",
        "print(\"\\nSentença 4 Permutada=\\\"\", sentenca4_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_permutado))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -14.89\n",
            "\n",
            "Sentença 2 Permutada=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -17.25\n",
            "\n",
            "Sentença 3 Permutada=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 14 e término em 18\n",
            "    Formato modelo : torch.Size([5, 768])\n",
            "    Soma embeddings:  -15.11\n",
            "\n",
            "Sentença 4 Permutada=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 19 e término em 24\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -6.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45HsR7ey2OnF"
      },
      "source": [
        "## Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9pbUlC72OnG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91dd53d5-45e9-401d-eec2-7c6a5b8f74fd"
      },
      "source": [
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_original)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_original))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embedding_sentenca4_original[:4]))\n",
        "\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_permutado)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_permutado))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embedding_sentenca1_permutado[:4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -12.13\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0835, -0.4042,  0.4330,  ..., -0.3271,  0.2262, -0.4599],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.4231, -0.3089, -0.5329],\n",
            "        [-0.5474, -0.0118,  0.3950,  ..., -0.0949, -0.3534, -0.3717],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ...,  0.1161,  0.0407, -0.6904]])\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -14.89\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.4811,  0.2881, -0.4057],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.5534, -0.3164, -0.3582],\n",
            "        [-0.7358, -0.0988,  0.5145,  ..., -0.1814, -0.3193, -0.3159],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.0720, -0.0478, -0.4931]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd8YKgnyJuUv"
      },
      "source": [
        "## Similaridade de cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cREd7N1JuUv"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCosseno(embeddings1, embeddings2):\n",
        "    \"\"\"\n",
        "      Similaridade do cosseno dos embeddings dos textos.\n",
        "      \n",
        "      Parâmetros:\n",
        "      `embeddings1` - Um embedding a ser medido.\n",
        "      `embeddings2` - Um embedding a ser medido.\n",
        "    \"\"\"    \n",
        "    similaridade = 1 - cosine(embeddings1, embeddings2)\n",
        "\n",
        "    return similaridade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auu-XqHOJuUv"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bNpbR0QJuUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d3e1ab7-e168-46ec-aa5b-4e3d39941b42"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosOriginal = float(somaScos)/float(n-1)\n",
        "print(\"Ccos Original:\", CcosOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.79404350121816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWNj-6i5JuUv"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQEsVKJ1JuUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea8759f-b5f1-4855-a9b6-7c19e1458451"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosPermutado = float(somaScos)/float(np-1)\n",
        "print(\"Ccos Original:\", CcosPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.7607070207595825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ecHZJrCJuUw"
      },
      "source": [
        "### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 1.\n",
        "- Documentos com sentenças diferenntes resulta uma medida menor que 1.\n",
        "- Documento com sentenças muito diferentes apresentam valores menores que 1.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de similaridade.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTxA53uUJuUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "161b19f1-8f38-47ff-e8e0-f148028f92e4"
      },
      "source": [
        "print(\"Ccos Original :\", CcosOriginal)\n",
        "print(\"Ccos Permutado:\", CcosPermutado)\n",
        "\n",
        "if (CcosOriginal > CcosPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos Original : 0.79404350121816\n",
            "Ccos Permutado: 0.7607070207595825\n",
            "Documento original tem maior similaridade de cosseno entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPQwuHf5JuUw"
      },
      "source": [
        "## Distância euclidiana entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância L2 ou norma L2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RmL_qXCJuUw"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distanciaEuclidiana(sentenca1, sentenca2):\n",
        "  distancia = euclidean(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a61ckdmoJuUw"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yu-uTlhJuUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42155a0b-51d3-407b-bce8-0b6e520bd65f"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucOriginal = float(somaSeuc)/float(n-1)\n",
        "print(\"Ceuc Original:\", CeucOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 10.07102108001709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBRvVkI4JuUw"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNgY1epXJuUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3f0b62f-110c-48e0-89fd-71952e12462a"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucPermutado = float(somaSeuc)/float(np-1)\n",
        "print(\"Ceuc Original:\", CeucPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 10.701444943745932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI4qwrbWJuUx"
      },
      "source": [
        "### Compara as médias da distância euclidiana dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeJFDd5mJuUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd3bc6c-27f2-482e-e483-5957dea3ddef"
      },
      "source": [
        "print(\"Ceuc Original :\", CeucOriginal)\n",
        "print(\"Ceuc Permutado:\", CeucPermutado)\n",
        "\n",
        "if (CeucOriginal < CeucPermutado):\n",
        "    print(\"Documento original tem menor distância euclidiana entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância euclidiana entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ceuc Original : 10.07102108001709\n",
            "Ceuc Permutado: 10.701444943745932\n",
            "Documento original tem menor distância euclidiana entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EUoPQNyJuUx"
      },
      "source": [
        "## Distância Manhattan entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "\n",
        "Igual a subtração absoluta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IydTlzptJuUx"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def distanciaManhattan(sentenca1, sentenca2):\n",
        "  distancia = cityblock(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nuu93cfvJuUx"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1DavITCJuUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d1a7874-878c-4054-a31c-2546532ccfc8"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj)) \n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanOriginal = float(somaSman)/float(n-1)\n",
        "print(\"Cman Original:\", CmanOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Cman Original: 213.39678446451822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcWmFXmQJuUx"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6mpG5x0JuUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d79ac9e4-98ad-493f-9efa-2ff3cb05c6fa"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj)) \n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanPermutado = float(somaSman)/float(np-1)\n",
        "print(\"Ceuc Original:\", CmanPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 231.2981160481771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa8RFKDVJuUy"
      },
      "source": [
        "### Compara as médias da distância de manhattan dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB_oWMJ0JuUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c9e525d-7d06-4639-b8d1-7f4f4e05a8b4"
      },
      "source": [
        "print(\"Cman Original :\", CmanOriginal)\n",
        "print(\"Cman Permutado:\", CmanPermutado)\n",
        "\n",
        "if (CmanOriginal < CmanPermutado):\n",
        "    print(\"Documento original tem menor distância de manhattan entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância de manhattan entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cman Original : 213.39678446451822\n",
            "Cman Permutado: 231.2981160481771\n",
            "Documento original tem menor distância de manhattan entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRdr2bqZ2OnQ"
      },
      "source": [
        "## Resumo\n",
        "\n",
        "Resultado das medidas utilizando a última camada do BERT.\n",
        "\n",
        "Base(MEAN):\n",
        "- Ccos       :   0.62004846          0.54099079\n",
        "- Ceuc       :   7.11709849          7.99353107\n",
        "- Cman       :   153.63694255          171.98441060\n",
        "\n",
        "Base(MAX):\n",
        "- Ccos       :   0.79404344          0.76070702\n",
        "- Ceuc       :   10.07102553          10.70145098\n",
        "- Cman       :   213.39687602          231.29823303"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW0cKGq52OnQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a1a1f61-34f4-43d5-e12c-61a5ef948fb4"
      },
      "source": [
        "print(\"Resultado das medidas utilizando a última camada do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print(\"Ccos       :   {:.8f}          {:.8f}\".format(CcosOriginal,CcosPermutado))\n",
        "print(\"Ceuc       :   {:.8f}          {:.8f}\".format(CeucOriginal,CeucPermutado))\n",
        "print(\"Cman       :   {:.8f}          {:.8f}\".format(CmanOriginal,CmanPermutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado das medidas utilizando a última camada do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Ccos       :   0.79404350          0.76070702\n",
            "Ceuc       :   10.07102108          10.70144494\n",
            "Cman       :   213.39678446          231.29811605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru02mC0Estsb"
      },
      "source": [
        "# 5 - Exemplo sentenças de documento original e permutado utilizando embedding a concatenação das 4 últimas camadas do BERT usando estratégia a MEAN\n",
        "\n",
        "Como estamos utilizando os embeddings concatenado das 4 últimas camadas onde ocorre 768 entenda-se 3072 que é o resultado de 768 por 4 que é a dimensão do MCL BERT de tamanho base. E onde ocorre 1024 entenda-se 4096 que é o resultado de 1024 por 4 que é a dimensão do MCL BERT de tamanho large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1vDmYJTstsg"
      },
      "source": [
        "## Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJOUyEpistsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20dab4cc-bf62-40e3-cabe-cd5877a0f2d5"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_original = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documento_original_concatenado = \" \".join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + documento_original_concatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4HrZqBfstsh"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0tDxh3Mstsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03c33a42-0649-4be4-8de9-28c6667f81c8"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAQf9nM8stsh"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFqFcnx2stsh"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJbHPnoAstsi"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda todas as camadas da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51R6f4Mistsi"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx2_AvR8tbnZ"
      },
      "source": [
        "Recupera a saída e concatena as 4 últimas camada do BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C0KcRUHstsj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db928b88-d643-42e0-ee8b-849da7663eac"
      },
      "source": [
        "# Cria uma lista com os tensores a serem concatenados\n",
        "# Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "# Lista com os tensores a serem concatenados\n",
        "lista_concat = []\n",
        "# Percorre os 4 últimos\n",
        "for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    lista_concat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"lista_concat=\",len(lista_concat))\n",
        "\n",
        "# Realiza a concatenação dos embeddings de todos as camadas\n",
        "# Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "concat4_hidden_states = torch.cat(lista_concat, dim=-1)\n",
        "# Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "\n",
        "print (\"O vetor da  concatenação das 4 últimas camadas oculta tem o formato:\", concat4_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da  concatenação das 4 últimas camadas oculta tem o formato: torch.Size([1, 26, 3072])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM4luftrstsk"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f18k_o-stsk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb3b3ad9-9e56-4633-ba9e-b286feeb9184"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embedding_documento_original = torch.squeeze(concat4_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", embedding_documento_original.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([26, 3072])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g0E9s2Rstsk"
      },
      "source": [
        "Confirmando vetores dependentes do condocumento\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQGOQF-kstsk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75893015-effb-4343-cc17-82d27e6f6593"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veMZsnAsstsl"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9RSPe2Hstsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d09661d-6adf-420e-dcc4-3518806aa91e"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_original = documento_original[0]\n",
        "sentenca2_original = documento_original[1]\n",
        "sentenca3_original = documento_original[2]\n",
        "sentenca4_original = documento_original[3]\n",
        "\n",
        "embedding_sentenca1_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca1_original, tokenizer)\n",
        "embedding_sentenca2_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca2_original, tokenizer)\n",
        "embedding_sentenca3_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca3_original, tokenizer)\n",
        "embedding_sentenca4_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca4_original, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento original.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1_original,\"-\", str(embedding_sentenca1_original[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1_original,\"-\", str(torch.sum(embedding_sentenca1_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2_original,\"-\", str(embedding_sentenca2_original[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2_original,\"-\", str(torch.sum(embedding_sentenca2_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3_original,\"-\", str(embedding_sentenca3_original[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3_original,\"-\", str(torch.sum(embedding_sentenca3_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4_original,\"-\", str(embedding_sentenca4_original[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4_original,\"-\", str(torch.sum(embedding_sentenca4_original[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: Bom Dia, professor. - tensor([[-0.0680, -0.4615,  0.3552,  ...,  0.1657,  0.3478,  0.4525],\n",
            "        [-0.1000, -0.0630,  0.0840,  ..., -0.3926,  0.7971,  0.3287],\n",
            "        [-0.3165,  0.4208,  0.2178,  ..., -0.3834,  0.9520, -0.2181],\n",
            "        [ 0.1248,  0.2383,  0.8987,  ..., -0.1989,  0.1001,  0.1140]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-327.0100)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.5894, -0.4310,  0.1449,  ...,  0.2399, -0.8495, -0.1672],\n",
            "        [ 0.1349, -0.2476,  0.4605,  ..., -0.1353, -0.1674,  0.4057],\n",
            "        [ 0.4359, -0.6972,  0.4066,  ...,  0.5851, -0.0441, -0.0027],\n",
            "        [ 0.0544,  0.1606,  0.4150,  ..., -0.8060, -0.3698,  0.0466]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-315.7253)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[-0.3652, -0.5015, -0.1626,  ..., -0.5431, -0.5805,  0.4617],\n",
            "        [ 0.1421,  0.1797, -0.0014,  ..., -0.7004, -0.4356,  0.7459],\n",
            "        [-0.1274, -0.0926, -0.1861,  ...,  0.6395, -0.4126,  0.7672],\n",
            "        [-0.2886,  0.4056,  0.6759,  ..., -0.2171, -0.4010, -0.5805]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-311.2223)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[-0.0835, -0.4042,  0.4330,  ...,  0.2881,  0.1448,  0.7778],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.0102,  0.1851,  0.4587],\n",
            "        [-0.5474, -0.0118,  0.3950,  ...,  0.1932, -0.0735,  0.2649],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ..., -0.0370,  0.3236, -0.3671]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-307.8871)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGUkprWbstsl"
      },
      "source": [
        "Examinando os embeddings do documento original\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL-c0aqKstsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecff87c3-d5eb-443f-eaf3-13a8254a4100"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_original = documento_original[0]\n",
        "sentenca2_original = documento_original[1]\n",
        "sentenca3_original = documento_original[2]\n",
        "sentenca4_original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1_tokenizada_original = tokenizer.tokenize(sentenca1_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1_tokenizada_original)\n",
        "embedding_sentenca1_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca1_original, tokenizer)\n",
        "print(\"\\nSentença 1 Original=\\\"\", sentenca1_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2_tokenizada_original = tokenizer.tokenize(sentenca2_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2_tokenizada_original)\n",
        "embedding_sentenca2_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca2_original, tokenizer)\n",
        "print(\"\\nSentença 2 Original=\\\"\", sentenca2_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca2_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca2_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3_tokenizada_original = tokenizer.tokenize(sentenca3_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3_tokenizada_original)\n",
        "embedding_sentenca3_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca3_original, tokenizer)\n",
        "print(\"\\nSentença 3 Original=\\\"\", sentenca3_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca3_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca3_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4_tokenizada_original = tokenizer.tokenize(sentenca4_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4_tokenizada_original)\n",
        "embedding_sentenca4_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca4_original, tokenizer)\n",
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_original))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 3072])\n",
            "    Soma embeddings:  -398.53\n",
            "\n",
            "Sentença 2 Original=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 3072])\n",
            "    Soma embeddings:  -467.44\n",
            "\n",
            "Sentença 3 Original=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 3072])\n",
            "    Soma embeddings:  -466.27\n",
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 3072])\n",
            "    Soma embeddings:  -525.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRdBVlt9stsm"
      },
      "source": [
        "## Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xsxt0Jistsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2f17528-f911-406f-ac62-a73e7da87557"
      },
      "source": [
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_permutado = [documento_original[3],   # \"Aguardo uma resposta, João.\",\n",
        "             documento_original[1],             # \"Qual o conteúdo da prova?\",              \n",
        "             documento_original[0],             # \"Vai cair tudo na prova?\",\n",
        "             documento_original[2]]             # \"Bom Dia, professor.\"]     \n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documento_permutado_concatenado = \" \".join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + documento_permutado_concatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 Bom           8,399\n",
            " 15 Dia           3,616\n",
            " 16 ,               117\n",
            " 17 professor     2,917\n",
            " 18 .               119\n",
            " 19 Vai          20,805\n",
            " 20 cair          9,322\n",
            " 21 tudo          2,745\n",
            " 22 na              229\n",
            " 23 prova         2,310\n",
            " 24 ?               136\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM5NtnGRuAsU"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GDGmFc_stsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eaf7909-75fb-4dab-d946-8e7a17c3b49d"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4WehJSxuDvo"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrvEtHp7stsm"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQDiqh5UuMnc"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda todas as camadas da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tGQsVMpstsm"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5CmoS1at0YA"
      },
      "source": [
        "Recupera a saída e concatena as 4 últimas camada do BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRDYmUq9stsn"
      },
      "source": [
        "# Cria uma lista com os tensores a serem concatenados\n",
        "# Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "# Lista com os tensores a serem concatenados\n",
        "lista_concat = []\n",
        "# Percorre os 4 últimos\n",
        "for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    lista_concat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"lista_concat=\",len(lista_concat))\n",
        "\n",
        "# Realiza a concatenação dos embeddings de todos as camadas\n",
        "# Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "concat4_hidden_states = torch.cat(lista_concat, dim=-1)\n",
        "# Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm9EVbDauSiI"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXNqk-oQstsn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed72da7-4b0e-4b52-b27a-99d779fc4937"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embedding_documento_permutado = torch.squeeze(concat4_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", embedding_documento_permutado.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([26, 3072])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbLgVumnstso"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1nUZ7OLstso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43041068-6af3-40b0-9dba-aaf9b03caf9f"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_permutado = documento_permutado[0]\n",
        "sentenca2_permutado = documento_permutado[1]\n",
        "sentenca3_permutado = documento_permutado[2]\n",
        "sentenca4_permutado = documento_permutado[3]\n",
        "\n",
        "embedding_sentenca1_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca1_permutado, tokenizer)\n",
        "embedding_sentenca2_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca2_permutado, tokenizer)\n",
        "embedding_sentenca3_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca3_permutado, tokenizer)\n",
        "embedding_sentenca4_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca4_permutado, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento permutado.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1_permutado,\"-\", str(embedding_sentenca1_permutado[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1_original,\"-\", str(torch.sum(embedding_sentenca1_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2_permutado,\"-\", str(embedding_sentenca2_permutado[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2_permutado,\"-\", str(torch.sum(embedding_sentenca2_permutado[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3_permutado,\"-\", str(embedding_sentenca3_permutado[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3_permutado,\"-\", str(torch.sum(embedding_sentenca3_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4_permutado,\"-\", str(embedding_sentenca4_permutado[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4_permutado,\"-\", str(torch.sum(embedding_sentenca4_permutado[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.1775,  0.2231,  0.8036],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.3783,  0.1529,  0.5125],\n",
            "        [-0.7358, -0.0988,  0.5145,  ...,  0.0164, -0.1749,  0.2557],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.2708,  0.1942, -0.0740]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-327.0100)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.4295, -0.3426,  0.1005,  ...,  0.2608, -0.8301, -0.1044],\n",
            "        [ 0.1047, -0.1946,  0.4177,  ..., -0.1734, -0.2517,  0.4004],\n",
            "        [ 0.4377, -0.5952,  0.5448,  ...,  0.4539, -0.1388, -0.0339],\n",
            "        [ 0.0898,  0.1063,  0.4610,  ..., -0.5308, -0.4884,  0.2119]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-314.9911)\n",
            "\n",
            "Sentença 3: Bom Dia, professor. - tensor([[-0.0458, -0.4268,  0.2399,  ...,  0.4282,  0.3110,  0.0241],\n",
            "        [ 0.0650, -0.1385,  0.0230,  ..., -0.3674,  0.5677,  0.0532],\n",
            "        [-0.1147,  0.4119,  0.0659,  ..., -0.0354,  0.8405, -0.2598],\n",
            "        [ 0.1290,  0.2181,  0.7986,  ..., -0.1129,  0.0972,  0.1581]])\n",
            "Soma embedding Sentença3: Bom Dia, professor. - tensor(-311.2223)\n",
            "\n",
            "Sentença 4: Vai cair tudo na prova? - tensor([[-0.0414, -0.5222, -0.1612,  ..., -0.5124, -0.7616,  0.1765],\n",
            "        [ 0.4540,  0.0075, -0.0953,  ..., -0.7207, -0.3354,  0.4786],\n",
            "        [ 0.0887, -0.1633, -0.3073,  ...,  0.6813, -0.4292,  0.4572],\n",
            "        [-0.0159,  0.2363,  0.7514,  ..., -0.2911, -0.2646, -0.9683]])\n",
            "Soma embedding Sentença4: Vai cair tudo na prova? - tensor(-299.0246)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E_hSMahstso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "612bdd82-3626-4467-d10f-c1c5d7d78ffe"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_permutado = documento_permutado[0]\n",
        "sentenca2_permutado = documento_permutado[1]\n",
        "sentenca3_permutado = documento_permutado[2]\n",
        "sentenca4_permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1_tokenizada_permutado = tokenizer.tokenize(sentenca1_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1_tokenizada_permutado)\n",
        "embedding_sentenca1_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca1_permutado, tokenizer)\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2_tokenizada_permutado = tokenizer.tokenize(sentenca2_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2_tokenizada_permutado)\n",
        "embedding_sentenca2_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca2_permutado, tokenizer)\n",
        "print(\"\\nSentença 2 Permutada=\\\"\", sentenca2_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca2_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca2_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3_tokenizada_permutado = tokenizer.tokenize(sentenca3_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3_tokenizada_permutado)\n",
        "embedding_sentenca3_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca3_permutado, tokenizer)\n",
        "print(\"\\nSentença 3 Permutada=\\\"\", sentenca3_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca3_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca3_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4_tokenizada_permutado = tokenizer.tokenize(sentenca4_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4_tokenizada_permutado)\n",
        "embedding_sentenca4_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca4_permutado, tokenizer)\n",
        "print(\"\\nSentença 4 Permutada=\\\"\", sentenca4_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_permutado))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 3072])\n",
            "    Soma embeddings:  -532.56\n",
            "\n",
            "Sentença 2 Permutada=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 3072])\n",
            "    Soma embeddings:  -467.65\n",
            "\n",
            "Sentença 3 Permutada=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 14 e término em 18\n",
            "    Formato modelo : torch.Size([5, 3072])\n",
            "    Soma embeddings:  -402.97\n",
            "\n",
            "Sentença 4 Permutada=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 19 e término em 24\n",
            "    Formato modelo : torch.Size([6, 3072])\n",
            "    Soma embeddings:  -447.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CsH6v4Dstso"
      },
      "source": [
        "## Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p5iSl9Nstso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61a4023f-3084-4ffb-a471-1d219545aa57"
      },
      "source": [
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_original)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_original))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embedding_sentenca4_original[:4]))\n",
        "\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_permutado)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_permutado))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embedding_sentenca1_permutado[:4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 3072])\n",
            "    Soma embeddings:  -525.26\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0835, -0.4042,  0.4330,  ...,  0.2881,  0.1448,  0.7778],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.0102,  0.1851,  0.4587],\n",
            "        [-0.5474, -0.0118,  0.3950,  ...,  0.1932, -0.0735,  0.2649],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ..., -0.0370,  0.3236, -0.3671]])\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 3072])\n",
            "    Soma embeddings:  -532.56\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.1775,  0.2231,  0.8036],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.3783,  0.1529,  0.5125],\n",
            "        [-0.7358, -0.0988,  0.5145,  ...,  0.0164, -0.1749,  0.2557],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.2708,  0.1942, -0.0740]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX-s-jyF9UFT"
      },
      "source": [
        "## Similaridade de cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSSZT6m9UFT"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCosseno(embeddings1, embeddings2):\n",
        "    \"\"\"\n",
        "      Similaridade do cosseno dos embeddings dos textos.\n",
        "      \n",
        "      Parâmetros:\n",
        "      `embeddings1` - Um embedding a ser medido.\n",
        "      `embeddings2` - Um embedding a ser medido.\n",
        "    \"\"\"    \n",
        "    similaridade = 1 - cosine(embeddings1, embeddings2)\n",
        "\n",
        "    return similaridade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2xmrGvd9UFT"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbyuNamR9UFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3163d6-8599-4924-fb8c-94c38559864a"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>) \n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosOriginal = float(somaScos)/float(n-1)\n",
        "print(\"Ccos Original:\", CcosOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.8077195882797241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WUjUZw_9UFT"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unc7fF7i9UFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8f910e-abc9-47e7-8438-0fa41df5b2b9"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosPermutado = float(somaScos)/float(np-1)\n",
        "print(\"Ccos Original:\", CcosPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.7713711063067118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPygSiX_9UFT"
      },
      "source": [
        "### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 1.\n",
        "- Documentos com sentenças diferenntes resulta uma medida menor que 1.\n",
        "- Documento com sentenças muito diferentes apresentam valores menores que 1.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de similaridade.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zD7FTPz9UFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf521f76-ee84-41ef-8a13-0d806f2c8fd3"
      },
      "source": [
        "print(\"Ccos Original :\", CcosOriginal)\n",
        "print(\"Ccos Permutado:\", CcosPermutado)\n",
        "\n",
        "if (CcosOriginal > CcosPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos Original : 0.8077195882797241\n",
            "Ccos Permutado: 0.7713711063067118\n",
            "Documento original tem maior similaridade de cosseno entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVcRGoM09UFU"
      },
      "source": [
        "## Distância euclidiana entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância L2 ou norma L2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqK7DcTJ9UFU"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distanciaEuclidiana(sentenca1, sentenca2):\n",
        "  distancia = euclidean(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtzn3Ll69UFU"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McejBc0P9UFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d981644e-96de-4e53-f8f9-5cbf2d405da5"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Distância euclidiana entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)  \n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucOriginal = float(somaSeuc)/float(n-1)\n",
        "print(\"Ceuc Original:\", CeucOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 20.710796356201172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7_eRCMV9UFU"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBEbKqiH9UFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b194a8c8-7ae4-457d-bc93-52df626e7554"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Distância euclidiana entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>) \n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucPermutado = float(somaSeuc)/float(np-1)\n",
        "print(\"Ceuc Original:\", CeucPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 23.169553756713867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5qzdOVy9UFU"
      },
      "source": [
        "### Compara as médias da distância euclidiana dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnBVeDrv9UFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aecbfa8-8468-44e0-e8eb-c1f3317bbfa6"
      },
      "source": [
        "print(\"Ceuc Original :\", CeucOriginal)\n",
        "print(\"Ceuc Permutado:\", CeucPermutado)\n",
        "\n",
        "if (CeucOriginal < CeucPermutado):\n",
        "    print(\"Documento original tem menor distância euclidiana entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância euclidiana entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ceuc Original : 20.710796356201172\n",
            "Ceuc Permutado: 23.169553756713867\n",
            "Documento original tem menor distância euclidiana entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojaQA2C49UFV"
      },
      "source": [
        "## Distância Manhattan entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "\n",
        "Igual a subtração absoluta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4VBoLbH9UFV"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def distanciaManhattan(sentenca1, sentenca2):\n",
        "  distancia = cityblock(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN96DPuI9UFV"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyAvO6WV9UFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89f03093-04ab-4f28-b892-2a4a514308ca"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Distância de manhattan entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>) \n",
        "    Sman = distanciaManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanOriginal = float(somaSman)/float(n-1)\n",
        "print(\"Cman Original:\", CmanOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Cman Original: 883.5126749674479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oenwZkqO9UFV"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg81Wnik9UFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d9adc3-58c3-4379-cbae-58255549c71e"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Distância de manhattan entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanPermutado = float(somaSman)/float(np-1)\n",
        "print(\"Ceuc Original:\", CmanPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 983.2882690429688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcTxjUcc9UFV"
      },
      "source": [
        "### Compara as médias da distância de manhattan dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JzGE3he9UFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac4e74f-d8b9-4ba2-8f9d-40f49dc2a63d"
      },
      "source": [
        "print(\"Cman Original :\", CmanOriginal)\n",
        "print(\"Cman Permutado:\", CmanPermutado)\n",
        "\n",
        "if (CmanOriginal > CmanPermutado):\n",
        "    print(\"Documento original tem menor distância de manhattan entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância de manhattan entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cman Original : 883.5126749674479\n",
            "Cman Permutado: 983.2882690429688\n",
            "Documento Permutado tem maior distância de manhattan entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaWJ6lU1a3RN"
      },
      "source": [
        "## Resumo\n",
        "\n",
        "Resultado das medidas utilizando as quatro últimas camadas do BERT.\n",
        "\n",
        "\n",
        "Base(MEAN):\n",
        "- Ccos       :   0.62004846          0.54099079\n",
        "- Ceuc       :   7.11709849          7.99353107\n",
        "- Cman       :   153.63694255          171.98441060\n",
        "\n",
        "Base(MAX):\n",
        "- Ccos       :   0.79404344          0.76070702\n",
        "- Ceuc       :   10.07102553          10.70145098\n",
        "- Cman       :   213.39687602          231.29823303\n",
        "\n",
        "Base(MEAN):\n",
        "- Ccos       :   0.80771943          0.77137093\n",
        "- Ceuc       :   20.71080144          23.16955884\n",
        "- Cman       :   883.51291911          983.28851318"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhAP-gfja3RN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab79e816-7587-4e5d-dcc6-e038ab81cd44"
      },
      "source": [
        "print(\"Resultado das medidas utilizando as quatro últimas camada do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print(\"Ccos       :   {:.8f}          {:.8f}\".format(CcosOriginal,CcosPermutado))\n",
        "print(\"Ceuc       :   {:.8f}          {:.8f}\".format(CeucOriginal,CeucPermutado))\n",
        "print(\"Cman       :   {:.8f}          {:.8f}\".format(CmanOriginal,CmanPermutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado das medidas utilizando as quatro últimas camada do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Ccos       :   0.80771959          0.77137111\n",
            "Ceuc       :   20.71079636          23.16955376\n",
            "Cman       :   883.51267497          983.28826904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJKcCTMPL341"
      },
      "source": [
        "# 6 - Exemplo sentenças de documento original e permutado utilizando embedding da concatenação das 4 últimas camadas do BERT usando estratégia MAX e todas as palavras.\n",
        "\n",
        "Como estamos utilizando os embeddings concatenado das 4 últimas camadas onde ocorre 768 entenda-se 3072 que é o resultado de 768 por 4 que é a dimensão do MCL BERT de tamanho base. E onde ocorre 1024 entenda-se 4096 que é o resultado de 1024 por 4 que é a dimensão do MCL BERT de tamanho large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC_lpJhbM0iL"
      },
      "source": [
        "## Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzLY--ZCM0iS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f1c7ad-f487-4c95-af5a-9362dd21b65c"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_original = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documento_original_concatenado = \" \".join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + documento_original_concatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWF6G0EqM0iT"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPokJjkHM0iT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d34ec27a-76ef-4c30-8f0c-69617921bfa7"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvGb4Lr9M0iT"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x23ik0ehM0iU"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpDIoOYgM0iU"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda todas as camadas da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mGvXQN2M0iU"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VajVWPxSM0iU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc42884b-c2a9-4fa4-f6c4-e86065a992bc"
      },
      "source": [
        "# Cria uma lista com os tensores a serem concatenados\n",
        "# Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "# Lista com os tensores a serem concatenados\n",
        "lista_concat = []\n",
        "# Percorre os 4 últimos\n",
        "for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    lista_concat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"lista_concat=\",len(lista_concat))\n",
        "\n",
        "# Realiza a concatenação dos embeddings de todos as camadas\n",
        "# Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "concat4_hidden_states = torch.cat(lista_concat, dim=-1)\n",
        "# Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "\n",
        "print (\"O vetor da  concatenação das 4 últimas camadas oculta tem o formato:\", concat4_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da  concatenação das 4 últimas camadas oculta tem o formato: torch.Size([1, 26, 3072])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f-UKTRuM0iU"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ue_2PX2M0iV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1899aee5-690f-4b0b-f67f-587d1dc17b47"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embedding_documento_original = torch.squeeze(concat4_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", embedding_documento_original.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([26, 3072])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCwadAdmM0iV"
      },
      "source": [
        "Confirmando vetores dependentes do condocumento\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb8Cm77zM0iV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8adb7516-974c-4337-9809-963d758248ce"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGjE2y_pM0iV"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W28pu2G9M0iV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e66d8b06-f21e-4263-8472-23aa79cc599c"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_original = documento_original[0]\n",
        "sentenca2_original = documento_original[1]\n",
        "sentenca3_original = documento_original[2]\n",
        "sentenca4_original = documento_original[3]\n",
        "\n",
        "embedding_sentenca1_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca1_original, tokenizer)\n",
        "embedding_sentenca2_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca2_original, tokenizer)\n",
        "embedding_sentenca3_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca3_original, tokenizer)\n",
        "embedding_sentenca4_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca4_original, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento original.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1_original,\"-\", str(embedding_sentenca1_original[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1_original,\"-\", str(torch.sum(embedding_sentenca1_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2_original,\"-\", str(embedding_sentenca2_original[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2_original,\"-\", str(torch.sum(embedding_sentenca2_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3_original,\"-\", str(embedding_sentenca3_original[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3_original,\"-\", str(torch.sum(embedding_sentenca3_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4_original,\"-\", str(embedding_sentenca4_original[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4_original,\"-\", str(torch.sum(embedding_sentenca4_original[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: Bom Dia, professor. - tensor([[-0.0680, -0.4615,  0.3552,  ...,  0.1657,  0.3478,  0.4525],\n",
            "        [-0.1000, -0.0630,  0.0840,  ..., -0.3926,  0.7971,  0.3287],\n",
            "        [-0.3165,  0.4208,  0.2178,  ..., -0.3834,  0.9520, -0.2181],\n",
            "        [ 0.1248,  0.2383,  0.8987,  ..., -0.1989,  0.1001,  0.1140]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-327.0100)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.5894, -0.4310,  0.1449,  ...,  0.2399, -0.8495, -0.1672],\n",
            "        [ 0.1349, -0.2476,  0.4605,  ..., -0.1353, -0.1674,  0.4057],\n",
            "        [ 0.4359, -0.6972,  0.4066,  ...,  0.5851, -0.0441, -0.0027],\n",
            "        [ 0.0544,  0.1606,  0.4150,  ..., -0.8060, -0.3698,  0.0466]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-315.7253)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[-0.3652, -0.5015, -0.1626,  ..., -0.5431, -0.5805,  0.4617],\n",
            "        [ 0.1421,  0.1797, -0.0014,  ..., -0.7004, -0.4356,  0.7459],\n",
            "        [-0.1274, -0.0926, -0.1861,  ...,  0.6395, -0.4126,  0.7672],\n",
            "        [-0.2886,  0.4056,  0.6759,  ..., -0.2171, -0.4010, -0.5805]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-311.2223)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[-0.0835, -0.4042,  0.4330,  ...,  0.2881,  0.1448,  0.7778],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.0102,  0.1851,  0.4587],\n",
            "        [-0.5474, -0.0118,  0.3950,  ...,  0.1932, -0.0735,  0.2649],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ..., -0.0370,  0.3236, -0.3671]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-307.8871)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDAG_xYHM0iV"
      },
      "source": [
        "Examinando os embeddings do documento original\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN1ZnWWLM0iW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c17cc09-3677-4a38-c5fb-e0df96c19295"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_original = documento_original[0]\n",
        "sentenca2_original = documento_original[1]\n",
        "sentenca3_original = documento_original[2]\n",
        "sentenca4_original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1_tokenizada_original = tokenizer.tokenize(sentenca1_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1_tokenizada_original)\n",
        "embedding_sentenca1_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca1_original, tokenizer)\n",
        "print(\"\\nSentença 1 Original=\\\"\", sentenca1_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2_tokenizada_original = tokenizer.tokenize(sentenca2_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2_tokenizada_original)\n",
        "embedding_sentenca2_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca2_original, tokenizer)\n",
        "print(\"\\nSentença 2 Original=\\\"\", sentenca2_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca2_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca2_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3_tokenizada_original = tokenizer.tokenize(sentenca3_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3_tokenizada_original)\n",
        "embedding_sentenca3_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca3_original, tokenizer)\n",
        "print(\"\\nSentença 3 Original=\\\"\", sentenca3_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca3_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca3_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4_tokenizada_original = tokenizer.tokenize(sentenca4_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4_tokenizada_original)\n",
        "embedding_sentenca4_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca4_original, tokenizer)\n",
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_original))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 3072])\n",
            "    Soma embeddings:  -398.53\n",
            "\n",
            "Sentença 2 Original=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 3072])\n",
            "    Soma embeddings:  -467.44\n",
            "\n",
            "Sentença 3 Original=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 3072])\n",
            "    Soma embeddings:  -466.27\n",
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 3072])\n",
            "    Soma embeddings:  -525.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWXVLJ1bM0iW"
      },
      "source": [
        "## Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPWjgUWeM0iW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28ae0b2c-9d05-4b1d-f6da-44737b903d99"
      },
      "source": [
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_permutado = [documento_original[3],   # \"Aguardo uma resposta, João.\",\n",
        "             documento_original[1],             # \"Qual o conteúdo da prova?\",              \n",
        "             documento_original[0],             # \"Vai cair tudo na prova?\",\n",
        "             documento_original[2]]             # \"Bom Dia, professor.\"]     \n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documento_permutado_concatenado = \" \".join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + documento_permutado_concatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 Bom           8,399\n",
            " 15 Dia           3,616\n",
            " 16 ,               117\n",
            " 17 professor     2,917\n",
            " 18 .               119\n",
            " 19 Vai          20,805\n",
            " 20 cair          9,322\n",
            " 21 tudo          2,745\n",
            " 22 na              229\n",
            " 23 prova         2,310\n",
            " 24 ?               136\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN2heM1aM0iW"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjakxc4xM0iW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16aa9b2a-10f1-415e-b689-7cfdaf3393f8"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkJeVNRgM0iW"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67NUmOkIM0iX"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TIWhsT3M0iX"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda todas as camadas da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07xtO-rnM0iX"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvV4gue4M0iX"
      },
      "source": [
        "Recupera a saída e concatena as 4 últimas camada do BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svq_3keEM0iX"
      },
      "source": [
        "# Cria uma lista com os tensores a serem concatenados\n",
        "# Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "# Lista com os tensores a serem concatenados\n",
        "lista_concat = []\n",
        "# Percorre os 4 últimos\n",
        "for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    lista_concat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"lista_concat=\",len(lista_concat))\n",
        "\n",
        "# Realiza a concatenação dos embeddings de todos as camadas\n",
        "# Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "concat4_hidden_states = torch.cat(lista_concat, dim=-1)\n",
        "# Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybtNemHbM0iX"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSav8HKIM0iX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b3e1bb4-7872-45c3-bacb-c681cf625cc4"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embedding_documento_permutado = torch.squeeze(concat4_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", embedding_documento_permutado.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([26, 3072])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2EqEuq6M0iY"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_hCQhZnM0iY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3226d0b3-7fa0-401a-a3fe-3e158c732fff"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_permutado = documento_permutado[0]\n",
        "sentenca2_permutado = documento_permutado[1]\n",
        "sentenca3_permutado = documento_permutado[2]\n",
        "sentenca4_permutado = documento_permutado[3]\n",
        "\n",
        "embedding_sentenca1_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca1_permutado, tokenizer)\n",
        "embedding_sentenca2_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca2_permutado, tokenizer)\n",
        "embedding_sentenca3_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca3_permutado, tokenizer)\n",
        "embedding_sentenca4_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca4_permutado, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento permutado.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1_permutado,\"-\", str(embedding_sentenca1_permutado[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1_original,\"-\", str(torch.sum(embedding_sentenca1_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2_permutado,\"-\", str(embedding_sentenca2_permutado[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2_permutado,\"-\", str(torch.sum(embedding_sentenca2_permutado[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3_permutado,\"-\", str(embedding_sentenca3_permutado[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3_permutado,\"-\", str(torch.sum(embedding_sentenca3_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4_permutado,\"-\", str(embedding_sentenca4_permutado[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4_permutado,\"-\", str(torch.sum(embedding_sentenca4_permutado[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.1775,  0.2231,  0.8036],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.3783,  0.1529,  0.5125],\n",
            "        [-0.7358, -0.0988,  0.5145,  ...,  0.0164, -0.1749,  0.2557],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.2708,  0.1942, -0.0740]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-327.0100)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.4295, -0.3426,  0.1005,  ...,  0.2608, -0.8301, -0.1044],\n",
            "        [ 0.1047, -0.1946,  0.4177,  ..., -0.1734, -0.2517,  0.4004],\n",
            "        [ 0.4377, -0.5952,  0.5448,  ...,  0.4539, -0.1388, -0.0339],\n",
            "        [ 0.0898,  0.1063,  0.4610,  ..., -0.5308, -0.4884,  0.2119]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-314.9911)\n",
            "\n",
            "Sentença 3: Bom Dia, professor. - tensor([[-0.0458, -0.4268,  0.2399,  ...,  0.4282,  0.3110,  0.0241],\n",
            "        [ 0.0650, -0.1385,  0.0230,  ..., -0.3674,  0.5677,  0.0532],\n",
            "        [-0.1147,  0.4119,  0.0659,  ..., -0.0354,  0.8405, -0.2598],\n",
            "        [ 0.1290,  0.2181,  0.7986,  ..., -0.1129,  0.0972,  0.1581]])\n",
            "Soma embedding Sentença3: Bom Dia, professor. - tensor(-311.2223)\n",
            "\n",
            "Sentença 4: Vai cair tudo na prova? - tensor([[-0.0414, -0.5222, -0.1612,  ..., -0.5124, -0.7616,  0.1765],\n",
            "        [ 0.4540,  0.0075, -0.0953,  ..., -0.7207, -0.3354,  0.4786],\n",
            "        [ 0.0887, -0.1633, -0.3073,  ...,  0.6813, -0.4292,  0.4572],\n",
            "        [-0.0159,  0.2363,  0.7514,  ..., -0.2911, -0.2646, -0.9683]])\n",
            "Soma embedding Sentença4: Vai cair tudo na prova? - tensor(-299.0246)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXJIz3jPM0iY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e18b6ea7-8807-4a73-d8a5-70109f716a07"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_permutado = documento_permutado[0]\n",
        "sentenca2_permutado = documento_permutado[1]\n",
        "sentenca3_permutado = documento_permutado[2]\n",
        "sentenca4_permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1_tokenizada_permutado = tokenizer.tokenize(sentenca1_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1_tokenizada_permutado)\n",
        "embedding_sentenca1_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca1_permutado, tokenizer)\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2_tokenizada_permutado = tokenizer.tokenize(sentenca2_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2_tokenizada_permutado)\n",
        "embedding_sentenca2_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca2_permutado, tokenizer)\n",
        "print(\"\\nSentença 2 Permutada=\\\"\", sentenca2_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca2_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca2_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3_tokenizada_permutado = tokenizer.tokenize(sentenca3_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3_tokenizada_permutado)\n",
        "embedding_sentenca3_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca3_permutado, tokenizer)\n",
        "print(\"\\nSentença 3 Permutada=\\\"\", sentenca3_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca3_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca3_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4_tokenizada_permutado = tokenizer.tokenize(sentenca4_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4_tokenizada_permutado)\n",
        "embedding_sentenca4_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca4_permutado, tokenizer)\n",
        "print(\"\\nSentença 4 Permutada=\\\"\", sentenca4_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_permutado))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 3072])\n",
            "    Soma embeddings:  -532.56\n",
            "\n",
            "Sentença 2 Permutada=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 3072])\n",
            "    Soma embeddings:  -467.65\n",
            "\n",
            "Sentença 3 Permutada=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 14 e término em 18\n",
            "    Formato modelo : torch.Size([5, 3072])\n",
            "    Soma embeddings:  -402.97\n",
            "\n",
            "Sentença 4 Permutada=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 19 e término em 24\n",
            "    Formato modelo : torch.Size([6, 3072])\n",
            "    Soma embeddings:  -447.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyOCD7COM0iY"
      },
      "source": [
        "## Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ucrFp95M0iY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36675f6f-dfac-4a22-d253-be882a69601f"
      },
      "source": [
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_original)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_original))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embedding_sentenca4_original[:4]))\n",
        "\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_permutado)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_permutado))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embedding_sentenca1_permutado[:4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 3072])\n",
            "    Soma embeddings:  -525.26\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0835, -0.4042,  0.4330,  ...,  0.2881,  0.1448,  0.7778],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.0102,  0.1851,  0.4587],\n",
            "        [-0.5474, -0.0118,  0.3950,  ...,  0.1932, -0.0735,  0.2649],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ..., -0.0370,  0.3236, -0.3671]])\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 3072])\n",
            "    Soma embeddings:  -532.56\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.1775,  0.2231,  0.8036],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.3783,  0.1529,  0.5125],\n",
            "        [-0.7358, -0.0988,  0.5145,  ...,  0.0164, -0.1749,  0.2557],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.2708,  0.1942, -0.0740]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJLbGeGl9rXA"
      },
      "source": [
        "## Similaridade de cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txsn241y9rXA"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCosseno(embeddings1, embeddings2):\n",
        "    \"\"\"\n",
        "      Similaridade do cosseno dos embeddings dos textos.\n",
        "      \n",
        "      Parâmetros:\n",
        "      `embeddings1` - Um embedding a ser medido.\n",
        "      `embeddings2` - Um embedding a ser medido.\n",
        "    \"\"\"    \n",
        "    similaridade = 1 - cosine(embeddings1, embeddings2)\n",
        "\n",
        "    return similaridade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5MH4W1E9rXA"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q30qFwSb9rXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87e25d46-ff67-494f-91f5-8420ec43c940"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosOriginal = float(somaScos)/float(n-1)\n",
        "print(\"Ccos Original:\", CcosOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.8309433460235596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZLT9K6r9rXA"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF5f2DSb9rXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f047d1-f174-41b4-a3ee-7f72e1390d88"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosPermutado = float(somaScos)/float(np-1)\n",
        "print(\"Ccos Original:\", CcosPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.8036014437675476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIKvfbWo9rXB"
      },
      "source": [
        "### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 1.\n",
        "- Documentos com sentenças diferenntes resulta uma medida menor que 1.\n",
        "- Documento com sentenças muito diferentes apresentam valores menores que 1.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de similaridade.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCITgUD59rXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc44aec-7b32-4226-a0b0-d9bf18c3d27d"
      },
      "source": [
        "print(\"Ccos Original :\", CcosOriginal)\n",
        "print(\"Ccos Permutado:\", CcosPermutado)\n",
        "\n",
        "if (CcosOriginal > CcosPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos Original : 0.8309433460235596\n",
            "Ccos Permutado: 0.8036014437675476\n",
            "Documento original tem maior similaridade de cosseno entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poXedMfZ9rXB"
      },
      "source": [
        "## Distância euclidiana entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância L2 ou norma L2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgMLv7Xr9rXB"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distanciaEuclidiana(sentenca1, sentenca2):\n",
        "  distancia = euclidean(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFcLYcv_9rXB"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLPkoJU49rXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9cbed81-f789-4748-c124-9b4518b8d76f"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucOriginal = float(somaSeuc)/float(n-1)\n",
        "print(\"Ceuc Original:\", CeucOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 27.86793390909831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYF1Bm-g9rXB"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1rSfTES9rXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71aaff3d-3aa8-42b0-9d95-98dd055a277f"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj))\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucPermutado = float(somaSeuc)/float(np-1)\n",
        "print(\"Ceuc Original:\", CeucPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 30.032826741536457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_kqtJxd9rXC"
      },
      "source": [
        "### Compara as médias da distância euclidiana dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC4IlKjS9rXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2713f9c-4eac-4171-9a10-43b3b122985a"
      },
      "source": [
        "print(\"Ceuc Original :\", CeucOriginal)\n",
        "print(\"Ceuc Permutado:\", CeucPermutado)\n",
        "\n",
        "if (CeucOriginal < CeucPermutado):\n",
        "    print(\"Documento original tem menor distância euclidiana entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância euclidiana entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ceuc Original : 27.86793390909831\n",
            "Ceuc Permutado: 30.032826741536457\n",
            "Documento original tem menor distância euclidiana entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ron62mX39rXC"
      },
      "source": [
        "## Distância Manhattan entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "\n",
        "\n",
        "Igual a subtração absoluta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU8WUSNJ9rXC"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def distanciaManhattan(sentenca1, sentenca2):\n",
        "  distancia = cityblock(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp7lYFkQ9rXC"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZsETQjN9rXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "469ffa31-421d-4709-fb6d-18c19fa370cc"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj)) \n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanOriginal = float(somaSman)/float(n-1)\n",
        "print(\"Cman Original:\", CmanOriginal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Cman Original: 1175.8931477864583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hraWMnO49rXC"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1lpFQpW9rXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6996eda-9fbf-4691-92b5-85a1b18a54b7"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Encontra os maiores embeddings os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSi, linha = torch.max(embeddingSi, dim=0)        \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSi:\", len(maiorEmbeddingSi))\n",
        "        \n",
        "    # Encontra os maiores embeddings os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSj, linha = torch.max(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"maiorEmbeddingSj:\", len(maiorEmbeddingSj)) \n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(maiorEmbeddingSi, maiorEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanPermutado = float(somaSman)/float(np-1)\n",
        "print(\"Ceuc Original:\", CmanPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 1273.9385172526042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW9oqevL9rXD"
      },
      "source": [
        "### Compara as médias da distância de manhattan dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E71-ic6l9rXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ccf05ce-da5f-4e47-b746-271d956f507a"
      },
      "source": [
        "print(\"Cman Original :\", CmanOriginal)\n",
        "print(\"Cman Permutado:\", CmanPermutado)\n",
        "\n",
        "if (CmanOriginal < CmanPermutado):\n",
        "    print(\"Documento original tem menor distância de manhattan entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância de manhattan entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cman Original : 1175.8931477864583\n",
            "Cman Permutado: 1273.9385172526042\n",
            "Documento original tem menor distância de manhattan entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqVSa_XBL35H"
      },
      "source": [
        "## Resumo\n",
        "\n",
        "Resultado das medidas utilizando as quatro últimas camadas do BERT.\n",
        "\n",
        "Base(MEAN):\n",
        "- Ccos       :   0.62004846          0.54099079\n",
        "- Ceuc       :   7.11709849          7.99353107\n",
        "- Cman       :   153.63694255          171.98441060\n",
        "\n",
        "Base(MAX):\n",
        "- Ccos       :   0.79404344          0.76070702\n",
        "- Ceuc       :   10.07102553          10.70145098\n",
        "- Cman       :   213.39687602          231.29823303\n",
        "\n",
        "Base(MEAN):\n",
        "- Ccos       :   0.80771943          0.77137093\n",
        "- Ceuc       :   20.71080144          23.16955884\n",
        "- Cman       :   883.51291911          983.28851318\n",
        "\n",
        "Base(MAX):\n",
        "- Ccos       :   0.83094325          0.80360138\n",
        "- Ceuc       :   27.86794090          30.03283564\n",
        "- Cman       :   1175.89337158          1273.93892415"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yac6Etu7L35I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "263d1962-4f8c-4389-d957-bb11dd8b7aea"
      },
      "source": [
        "print(\"Resultado das medidas utilizando a última camada do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print(\"Ccos       :   {:.8f}          {:.8f}\".format(CcosOriginal,CcosPermutado))\n",
        "print(\"Ceuc       :   {:.8f}          {:.8f}\".format(CeucOriginal,CeucPermutado))\n",
        "print(\"Cman       :   {:.8f}          {:.8f}\".format(CmanOriginal,CmanPermutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado das medidas utilizando a última camada do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Ccos       :   0.83094335          0.80360144\n",
            "Ceuc       :   27.86793391          30.03282674\n",
            "Cman       :   1175.89314779          1273.93851725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsBQPbuNsFmj"
      },
      "source": [
        "# 7 - Exemplo sentenças de documento original e permutado utilizando embedding da última camada do BERT usando a estratégia MEAN e palavras relevantes(CLEAN - Stopword)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19AATfMisFmo"
      },
      "source": [
        "## Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU9FaJm4sFmo",
        "outputId": "5e21a7ea-7d4c-49fc-e53b-95470d0c54f2"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_original = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documento_original_concatenado = \" \".join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + documento_original_concatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGAC2fxisFmr"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRDaFwzosFmr",
        "outputId": "1a384e9c-1c38-4d24-d4cf-35cb730d2a60"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2Ky0UDDsFms"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p30SEmzsFms"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtpDq1ACsFmt"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX-Rty6PsFmt"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdxPC0A8sFmt"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVj5fIjjsFmu",
        "outputId": "9571b5aa-8773-42f5-bd6b-0ef4d3835026"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJrBKm9DsFmu"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zauxa9psFmu",
        "outputId": "f925ddd1-9bea-48d2-9697-21574f73bae2"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embedding_documento_original = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", embedding_documento_original.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([26, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBreB1FdsFmv"
      },
      "source": [
        "Confirmando vetores dependentes do condocumento\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp6eo0DCsFmv",
        "outputId": "990a669e-783d-45f0-c072-88d1e76858d6"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMxG57QasFmw"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfWmF0eOsFmw",
        "outputId": "3b9684d6-827b-42a7-d3b0-885f51a423fe"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_original = documento_original[0]\n",
        "sentenca2_original = documento_original[1]\n",
        "sentenca3_original = documento_original[2]\n",
        "sentenca4_original = documento_original[3]\n",
        "\n",
        "embedding_sentenca1_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca1_original, tokenizer)\n",
        "embedding_sentenca2_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca2_original, tokenizer)\n",
        "embedding_sentenca3_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca3_original, tokenizer)\n",
        "embedding_sentenca4_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca4_original, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento original.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1_original,\"-\", str(embedding_sentenca1_original[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1_original,\"-\", str(torch.sum(embedding_sentenca1_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2_original,\"-\", str(embedding_sentenca2_original[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2_original,\"-\", str(torch.sum(embedding_sentenca2_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3_original,\"-\", str(embedding_sentenca3_original[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3_original,\"-\", str(torch.sum(embedding_sentenca3_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4_original,\"-\", str(embedding_sentenca4_original[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4_original,\"-\", str(torch.sum(embedding_sentenca4_original[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: Bom Dia, professor. - tensor([[-0.0680, -0.4615,  0.3552,  ..., -0.3943, -0.1818, -0.4821],\n",
            "        [-0.1000, -0.0630,  0.0840,  ..., -0.6630,  0.1641, -0.8297],\n",
            "        [-0.3165,  0.4208,  0.2178,  ..., -0.4981,  0.1935, -0.3366],\n",
            "        [ 0.1248,  0.2383,  0.8987,  ..., -0.4940, -0.4578, -0.0353]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-13.8934)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.5894, -0.4310,  0.1449,  ...,  0.1601, -0.2918, -0.5303],\n",
            "        [ 0.1349, -0.2476,  0.4605,  ..., -0.3036, -0.6972,  0.2135],\n",
            "        [ 0.4359, -0.6972,  0.4066,  ...,  0.0177, -0.5852, -0.0615],\n",
            "        [ 0.0544,  0.1606,  0.4150,  ..., -0.3822, -0.1052, -0.0296]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-13.9466)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[-0.3652, -0.5015, -0.1626,  ...,  0.0161, -0.5967,  0.1675],\n",
            "        [ 0.1421,  0.1797, -0.0014,  ..., -0.5730, -0.5169,  0.3205],\n",
            "        [-0.1274, -0.0926, -0.1861,  ...,  0.4762, -0.4671,  0.2165],\n",
            "        [-0.2886,  0.4056,  0.6759,  ...,  0.5541, -0.3019, -0.1783]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-7.3501)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[-0.0835, -0.4042,  0.4330,  ..., -0.3271,  0.2262, -0.4599],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.4231, -0.3089, -0.5329],\n",
            "        [-0.5474, -0.0118,  0.3950,  ..., -0.0949, -0.3534, -0.3717],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ...,  0.1161,  0.0407, -0.6904]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-9.1314)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AT4hmrosFmw"
      },
      "source": [
        "Examinando os embeddings do documento original\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlNrBzHFsFmx",
        "outputId": "de319e55-9ac4-4c2a-b5f7-a00c33972d57"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_original = documento_original[0]\n",
        "sentenca2_original = documento_original[1]\n",
        "sentenca3_original = documento_original[2]\n",
        "sentenca4_original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1_tokenizada_original = tokenizer.tokenize(sentenca1_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1_tokenizada_original)\n",
        "embedding_sentenca1_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca1_original, tokenizer)\n",
        "print(\"\\nSentença 1 Original=\\\"\", sentenca1_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2_tokenizada_original = tokenizer.tokenize(sentenca2_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2_tokenizada_original)\n",
        "embedding_sentenca2_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca2_original, tokenizer)\n",
        "print(\"\\nSentença 2 Original=\\\"\", sentenca2_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca2_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca2_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3_tokenizada_original = tokenizer.tokenize(sentenca3_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3_tokenizada_original)\n",
        "embedding_sentenca3_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca3_original, tokenizer)\n",
        "print(\"\\nSentença 3 Original=\\\"\", sentenca3_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca3_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca3_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4_tokenizada_original = tokenizer.tokenize(sentenca4_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4_tokenizada_original)\n",
        "embedding_sentenca4_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca4_original, tokenizer)\n",
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_original))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 768])\n",
            "    Soma embeddings:  -15.02\n",
            "\n",
            "Sentença 2 Original=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -17.08\n",
            "\n",
            "Sentença 3 Original=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -10.65\n",
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -12.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWsYpv_bsFmx"
      },
      "source": [
        "## Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e-102FSsFmx",
        "outputId": "f73f22d1-ae6a-4da5-a6da-9739b4999f96"
      },
      "source": [
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_permutado = [documento_original[3],   # \"Aguardo uma resposta, João.\",\n",
        "             documento_original[1],             # \"Qual o conteúdo da prova?\",              \n",
        "             documento_original[0],             # \"Vai cair tudo na prova?\",\n",
        "             documento_original[2]]             # \"Bom Dia, professor.\"]     \n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documento_permutado_concatenado = \" \".join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + documento_permutado_concatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 Bom           8,399\n",
            " 15 Dia           3,616\n",
            " 16 ,               117\n",
            " 17 professor     2,917\n",
            " 18 .               119\n",
            " 19 Vai          20,805\n",
            " 20 cair          9,322\n",
            " 21 tudo          2,745\n",
            " 22 na              229\n",
            " 23 prova         2,310\n",
            " 24 ?               136\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoWi9iDGsFmy",
        "outputId": "4af8fa6b-ac05-44c1-dac4-0fe63815ee53"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUf--8eJsFmy"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPXl21tpsFmz"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx2aMCAcsFmz"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLkXV0DvsFmz"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eRSCaP4sFm0"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUMEhXZjsFm0",
        "outputId": "0f168de4-e49e-44d9-b105-437f39b8c95e"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAOnH5jBsFm1"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzMYCJr5sFm1",
        "outputId": "dfdac893-c1e6-4cbc-f7ef-8230c8fe515d"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embedding_documento_permutado = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", embedding_documento_permutado.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([26, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E_mRhwVsFm1"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QEGPIF7sFm2",
        "outputId": "c65c810b-f00e-45eb-a052-d41afcbfb771"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_permutado = documento_permutado[0]\n",
        "sentenca2_permutado = documento_permutado[1]\n",
        "sentenca3_permutado = documento_permutado[2]\n",
        "sentenca4_permutado = documento_permutado[3]\n",
        "\n",
        "embedding_sentenca1_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca1_permutado, tokenizer)\n",
        "embedding_sentenca2_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca2_permutado, tokenizer)\n",
        "embedding_sentenca3_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca3_permutado, tokenizer)\n",
        "embedding_sentenca4_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca4_permutado, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento permutado.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1_permutado,\"-\", str(embedding_sentenca1_permutado[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1_original,\"-\", str(torch.sum(embedding_sentenca1_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2_permutado,\"-\", str(embedding_sentenca2_permutado[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2_permutado,\"-\", str(torch.sum(embedding_sentenca2_permutado[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3_permutado,\"-\", str(embedding_sentenca3_permutado[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3_permutado,\"-\", str(torch.sum(embedding_sentenca3_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4_permutado,\"-\", str(embedding_sentenca4_permutado[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4_permutado,\"-\", str(torch.sum(embedding_sentenca4_permutado[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.4811,  0.2881, -0.4057],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.5534, -0.3164, -0.3582],\n",
            "        [-0.7358, -0.0988,  0.5145,  ..., -0.1814, -0.3193, -0.3159],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.0720, -0.0478, -0.4931]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-13.8934)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.4295, -0.3426,  0.1005,  ...,  0.1568, -0.3225, -0.3857],\n",
            "        [ 0.1047, -0.1946,  0.4177,  ..., -0.3388, -0.7529,  0.1693],\n",
            "        [ 0.4377, -0.5952,  0.5448,  ...,  0.0463, -0.5607, -0.1783],\n",
            "        [ 0.0898,  0.1063,  0.4610,  ..., -0.3313, -0.1764,  0.0920]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-13.8054)\n",
            "\n",
            "Sentença 3: Bom Dia, professor. - tensor([[-0.0458, -0.4268,  0.2399,  ..., -0.4504, -0.2225, -0.6343],\n",
            "        [ 0.0650, -0.1385,  0.0230,  ..., -0.7129,  0.1112, -0.8076],\n",
            "        [-0.1147,  0.4119,  0.0659,  ..., -0.3666,  0.0509, -0.2134],\n",
            "        [ 0.1290,  0.2181,  0.7986,  ..., -0.4771, -0.4328,  0.0041]])\n",
            "Soma embedding Sentença3: Bom Dia, professor. - tensor(-7.3501)\n",
            "\n",
            "Sentença 4: Vai cair tudo na prova? - tensor([[-0.0414, -0.5222, -0.1612,  ..., -0.0763, -0.6968,  0.0127],\n",
            "        [ 0.4540,  0.0075, -0.0953,  ..., -0.4919, -0.4762,  0.2358],\n",
            "        [ 0.0887, -0.1633, -0.3073,  ...,  0.4414, -0.5028,  0.0877],\n",
            "        [-0.0159,  0.2363,  0.7514,  ...,  0.3057, -0.4097, -0.2738]])\n",
            "Soma embedding Sentença4: Vai cair tudo na prova? - tensor(-4.3409)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kehz0vDBsFm2",
        "outputId": "b0a4e769-615b-4f96-b17e-c3fe9313f4b8"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_permutado = documento_permutado[0]\n",
        "sentenca2_permutado = documento_permutado[1]\n",
        "sentenca3_permutado = documento_permutado[2]\n",
        "sentenca4_permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1_tokenizada_permutado = tokenizer.tokenize(sentenca1_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1_tokenizada_permutado)\n",
        "embedding_sentenca1_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca1_permutado, tokenizer)\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2_tokenizada_permutado = tokenizer.tokenize(sentenca2_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2_tokenizada_permutado)\n",
        "embedding_sentenca2_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca2_permutado, tokenizer)\n",
        "print(\"\\nSentença 2 Permutada=\\\"\", sentenca2_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca2_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca2_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3_tokenizada_permutado = tokenizer.tokenize(sentenca3_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3_tokenizada_permutado)\n",
        "embedding_sentenca3_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca3_permutado, tokenizer)\n",
        "print(\"\\nSentença 3 Permutada=\\\"\", sentenca3_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca3_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca3_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4_tokenizada_permutado = tokenizer.tokenize(sentenca4_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4_tokenizada_permutado)\n",
        "embedding_sentenca4_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca4_permutado, tokenizer)\n",
        "print(\"\\nSentença 4 Permutada=\\\"\", sentenca4_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_permutado))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -14.89\n",
            "\n",
            "Sentença 2 Permutada=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -17.25\n",
            "\n",
            "Sentença 3 Permutada=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 14 e término em 18\n",
            "    Formato modelo : torch.Size([5, 768])\n",
            "    Soma embeddings:  -15.11\n",
            "\n",
            "Sentença 4 Permutada=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 19 e término em 24\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -6.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvUnrHFysFm2"
      },
      "source": [
        "## Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRXHN3O-sFm2",
        "outputId": "3ef93504-f5e7-4049-8368-f37d7b2d0975"
      },
      "source": [
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_original)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_original))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embedding_sentenca4_original[:4]))\n",
        "\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_permutado)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_permutado))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embedding_sentenca1_permutado[:4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -12.13\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0835, -0.4042,  0.4330,  ..., -0.3271,  0.2262, -0.4599],\n",
            "        [-0.2782, -0.1630,  0.0997,  ..., -0.4231, -0.3089, -0.5329],\n",
            "        [-0.5474, -0.0118,  0.3950,  ..., -0.0949, -0.3534, -0.3717],\n",
            "        [ 0.0146,  0.2901,  0.3920,  ...,  0.1161,  0.0407, -0.6904]])\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -14.89\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0052, -0.4706,  0.6481,  ..., -0.4811,  0.2881, -0.4057],\n",
            "        [-0.1516, -0.2450,  0.2693,  ..., -0.5534, -0.3164, -0.3582],\n",
            "        [-0.7358, -0.0988,  0.5145,  ..., -0.1814, -0.3193, -0.3159],\n",
            "        [-0.1278,  0.3571,  0.5481,  ..., -0.0720, -0.0478, -0.4931]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvXgM6N1sFm3"
      },
      "source": [
        "## Similaridade de cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKzuxFWJsFm3"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCosseno(embeddings1, embeddings2):\n",
        "    \"\"\"\n",
        "      Similaridade do cosseno dos embeddings dos textos.\n",
        "      \n",
        "      Parâmetros:\n",
        "      `embeddings1` - Um embedding a ser medido.\n",
        "      `embeddings2` - Um embedding a ser medido.\n",
        "    \"\"\"    \n",
        "    similaridade = 1 - cosine(embeddings1, embeddings2)\n",
        "\n",
        "    return similaridade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDib4hLGsFm3"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHXaYmvGsFm3",
        "outputId": "64ccad2b-a598-4b2c-fd0c-80802a11d7ce"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer, filtro=\"CLEAN\")        \n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosOriginal = float(somaScos)/float(n-1)\n",
        "print(\"Ccos Original:\", CcosOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.604610542456309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeDBXHS7sFm4"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEUtq3GXsFm4",
        "outputId": "b42a47b7-a143-46d0-ac84-2bb0e4f0f9e9"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "   # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosPermutado = float(somaScos)/float(np-1)\n",
        "print(\"Ccos Original:\", CcosPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.5301453073819479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj7h64hJsFm4"
      },
      "source": [
        "### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 1.\n",
        "- Documentos com sentenças diferenntes resulta uma medida menor que 1.\n",
        "- Documento com sentenças muito diferentes apresentam valores menores que 1.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de similaridade.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQLa6sp9sFm4",
        "outputId": "038563ef-7f37-4468-c62c-b8c74e866706"
      },
      "source": [
        "print(\"Ccos Original :\", CcosOriginal)\n",
        "print(\"Ccos Permutado:\", CcosPermutado)\n",
        "\n",
        "if (CcosOriginal > CcosPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos Original : 0.604610542456309\n",
            "Ccos Permutado: 0.5301453073819479\n",
            "Documento original tem maior similaridade de cosseno entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjvPrHXZsFm5"
      },
      "source": [
        "## Distância euclidiana entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância L2 ou norma L2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL_xEvs-sFm5"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distanciaEuclidiana(sentenca1, sentenca2):\n",
        "  distancia = euclidean(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqFVlwDBsFm5"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3uj0JMvsFm5",
        "outputId": "666978f7-5558-43a6-8f13-b0bb94472e88"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer, filtro=\"CLEAN\")        \n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucOriginal = float(somaSeuc)/float(n-1)\n",
        "print(\"Ceuc Original:\", CeucOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 7.444601535797119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv5VUtoMsFm5"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QsCz48XsFm6",
        "outputId": "c0a092cc-06ce-40e0-a062-e9d13b9b2b2c"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucPermutado = float(somaSeuc)/float(np-1)\n",
        "print(\"Ceuc Original:\", CeucPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 8.278315703074137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWtkCP0dsFm6"
      },
      "source": [
        "### Compara as médias da distância euclidiana dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SISdwB4RsFm6",
        "outputId": "614f8bd9-cb30-4732-92ea-01b865a59964"
      },
      "source": [
        "print(\"Ceuc Original :\", CeucOriginal)\n",
        "print(\"Ceuc Permutado:\", CeucPermutado)\n",
        "\n",
        "if (CeucOriginal < CeucPermutado):\n",
        "    print(\"Documento original tem menor distância euclidiana entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância euclidiana entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ceuc Original : 7.444601535797119\n",
            "Ceuc Permutado: 8.278315703074137\n",
            "Documento original tem menor distância euclidiana entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln6IuKWfsFm6"
      },
      "source": [
        "## Distância Manhattan entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "\n",
        "Igual subtração absoluta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJSt1k0esFm7"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def distanciaManhattan(sentenca1, sentenca2):\n",
        "  distancia = cityblock(sentenca1, sentenca2)\n",
        "\n",
        "  return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBO2UR1IsFm7"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxrMfVszsFm7",
        "outputId": "16d4c9b5-c7da-4381-a0d3-b1b6655cd803"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer, filtro=\"CLEAN\")        \n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanOriginal = float(somaSman)/float(n-1)\n",
        "print(\"Cman Original:\", CmanOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Cman Original: 161.34429931640625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2POGH7msFm7"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ji9F1JOsFm7",
        "outputId": "2d68cd6d-5fa1-499d-cd78-10b8b455d0c0"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanPermutado = float(somaSman)/float(np-1)\n",
        "print(\"Ceuc Original:\", CmanPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 178.6237996419271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF2QrnIVsFm8"
      },
      "source": [
        "### Compara as médias da distância de manhattan dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gCvmLXqsFm8",
        "outputId": "b0872b70-019a-4f54-8702-67ee90f66d3e"
      },
      "source": [
        "print(\"Cman Original :\", CmanOriginal)\n",
        "print(\"Cman Permutado:\", CmanPermutado)\n",
        "\n",
        "if (CmanOriginal < CmanPermutado):\n",
        "    print(\"Documento original tem menor distância de manhattan entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância de manhattan entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cman Original : 161.34429931640625\n",
            "Cman Permutado: 178.6237996419271\n",
            "Documento original tem menor distância de manhattan entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5rbxNyXsFm8"
      },
      "source": [
        "## Resumo\n",
        "\n",
        "Resultado das medidas utilizando a última camada do BERT.\n",
        "\n",
        "Base(MEAN):\n",
        "- Ccos       :   0.67999101          0.69653825\n",
        "- Ceuc       :   6.34085274          6.15486606\n",
        "- Cman       :   136.53579712          132.14489237\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh4_TmjNsFm8",
        "outputId": "8a79c8ab-755a-452e-beb7-617d050f0c12"
      },
      "source": [
        "print(\"Resultado das medidas utilizando a última camada do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print(\"Ccos       :   {:.8f}          {:.8f}\".format(CcosOriginal,CcosPermutado))\n",
        "print(\"Ceuc       :   {:.8f}          {:.8f}\".format(CeucOriginal,CeucPermutado))\n",
        "print(\"Cman       :   {:.8f}          {:.8f}\".format(CmanOriginal,CmanPermutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado das medidas utilizando a última camada do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Ccos       :   0.60461054          0.53014531\n",
            "Ceuc       :   7.44460154          8.27831570\n",
            "Cman       :   161.34429932          178.62379964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQSCarFgl9j5"
      },
      "source": [
        "# 8 - Exemplo sentenças de documento original e permutado utilizando embedding da última camada do BERT usando a estratégia MEAN com palavras relavantes(NOUN-Substantivos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFVHojw1l9j9"
      },
      "source": [
        "## Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qeR3y4Cl9j9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1899ea5a-7627-4019-c177-85fe73dda8b5"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_original = [\"O que é Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documento_original_concatenado = \" \".join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + documento_original_concatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 O               231\n",
            "  2 que             179\n",
            "  3 é               253\n",
            "  4 Bom           8,399\n",
            "  5 Dia           3,616\n",
            "  6 ,               117\n",
            "  7 professor     2,917\n",
            "  8 .               119\n",
            "  9 Qual         13,082\n",
            " 10 o               146\n",
            " 11 conteúdo      5,015\n",
            " 12 da              180\n",
            " 13 prova         2,310\n",
            " 14 ?               136\n",
            " 15 Vai          20,805\n",
            " 16 cair          9,322\n",
            " 17 tudo          2,745\n",
            " 18 na              229\n",
            " 19 prova         2,310\n",
            " 20 ?               136\n",
            " 21 Agu           8,125\n",
            " 22 ##ardo        2,222\n",
            " 23 uma             230\n",
            " 24 resposta      4,299\n",
            " 25 ,               117\n",
            " 26 João          1,453\n",
            " 27 .               119\n",
            " 28 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_HEZhCOl9j-"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMvf5JR9l9j-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e687e9b2-0ccb-49e4-c829-fd57a96ca71b"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpYRgyDKl9j-"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSkH4nStl9j_"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGIWW2frl9j_"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4UA1jqxl9j_"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rpP0syul9j_"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EumV200sl9kA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c2c89f-753e-4b60-b9d9-902e382f9843"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 29, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8UQ-CIIl9kA"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWdHs89nl9kA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ba69017-9727-4538-fc3a-1e977e3e7886"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embedding_documento_original = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", embedding_documento_original.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([29, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4Zflk6Fl9kA"
      },
      "source": [
        "Confirmando vetores dependentes do condocumento\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkjJEjmll9kA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f14efa-eac0-4733-8c38-f0b94372b683"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [CLS]\n",
            "1 O\n",
            "2 que\n",
            "3 é\n",
            "4 Bom\n",
            "5 Dia\n",
            "6 ,\n",
            "7 professor\n",
            "8 .\n",
            "9 Qual\n",
            "10 o\n",
            "11 conteúdo\n",
            "12 da\n",
            "13 prova\n",
            "14 ?\n",
            "15 Vai\n",
            "16 cair\n",
            "17 tudo\n",
            "18 na\n",
            "19 prova\n",
            "20 ?\n",
            "21 Agu\n",
            "22 ##ardo\n",
            "23 uma\n",
            "24 resposta\n",
            "25 ,\n",
            "26 João\n",
            "27 .\n",
            "28 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku6SY2qBl9kB"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0X1HZeil9kB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed09e448-4bee-4706-b1a4-50de0ea77bb3"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_original = documento_original[0]\n",
        "sentenca2_original = documento_original[1]\n",
        "sentenca3_original = documento_original[2]\n",
        "sentenca4_original = documento_original[3]\n",
        "\n",
        "embedding_sentenca1_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca1_original, tokenizer)\n",
        "embedding_sentenca2_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca2_original, tokenizer)\n",
        "embedding_sentenca3_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca3_original, tokenizer)\n",
        "embedding_sentenca4_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca4_original, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento original.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1_original,\"-\", str(embedding_sentenca1_original[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1_original,\"-\", str(torch.sum(embedding_sentenca1_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2_original,\"-\", str(embedding_sentenca2_original[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2_original,\"-\", str(torch.sum(embedding_sentenca2_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3_original,\"-\", str(embedding_sentenca3_original[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3_original,\"-\", str(torch.sum(embedding_sentenca3_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4_original,\"-\", str(embedding_sentenca4_original[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4_original,\"-\", str(torch.sum(embedding_sentenca4_original[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento original.\n",
            "\n",
            "Sentença 1: O que é Bom Dia, professor. - tensor([[-0.0102,  0.0373,  0.6649,  ..., -0.4246, -0.9479, -0.0711],\n",
            "        [-0.3262,  0.5584,  1.0965,  ...,  0.0591, -1.0677, -0.1463],\n",
            "        [ 0.5994,  0.3562,  0.9774,  ...,  0.0800, -0.2715, -0.2716],\n",
            "        [ 0.2056,  0.0379,  0.3316,  ..., -0.4781, -0.3953, -0.6865]])\n",
            "Soma embedding Sentença1: O que é Bom Dia, professor. - tensor(-25.7755)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.6281, -0.3937,  0.2737,  ...,  0.2525, -0.2877, -0.5052],\n",
            "        [ 0.1399, -0.2567,  0.5908,  ..., -0.3228, -0.6866,  0.1211],\n",
            "        [ 0.4652, -0.6698,  0.4403,  ...,  0.0370, -0.5818, -0.1134],\n",
            "        [ 0.0733,  0.2107,  0.4351,  ..., -0.4281, -0.0896,  0.0183]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-15.4762)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[-0.3447, -0.4489, -0.1350,  ..., -0.0057, -0.5487,  0.1713],\n",
            "        [ 0.1559,  0.1630, -0.0151,  ..., -0.6291, -0.4946,  0.3525],\n",
            "        [-0.1695, -0.0453, -0.2155,  ...,  0.4586, -0.4222,  0.1951],\n",
            "        [-0.2848,  0.4441,  0.6910,  ...,  0.4964, -0.2140, -0.1322]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-7.3697)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[-0.0493, -0.3044,  0.4427,  ..., -0.3811,  0.2865, -0.3818],\n",
            "        [-0.2567, -0.1567,  0.0947,  ..., -0.4835, -0.2826, -0.4569],\n",
            "        [-0.4932, -0.0126,  0.3327,  ..., -0.1175, -0.3180, -0.3177],\n",
            "        [ 0.0216,  0.2718,  0.4657,  ...,  0.1346,  0.0899, -0.6842]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-8.2151)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uat3GKd1l9kB"
      },
      "source": [
        "Examinando os embeddings do documento original\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w88VPPW8l9kB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635b4fd0-53d2-4408-ade9-66f04a0f6412"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_original = documento_original[0]\n",
        "sentenca2_original = documento_original[1]\n",
        "sentenca3_original = documento_original[2]\n",
        "sentenca4_original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1_tokenizada_original = tokenizer.tokenize(sentenca1_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1_tokenizada_original)\n",
        "embedding_sentenca1_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca1_original, tokenizer)\n",
        "print(\"\\nSentença 1 Original=\\\"\", sentenca1_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2_tokenizada_original = tokenizer.tokenize(sentenca2_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2_tokenizada_original)\n",
        "embedding_sentenca2_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca2_original, tokenizer)\n",
        "print(\"\\nSentença 2 Original=\\\"\", sentenca2_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca2_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca2_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3_tokenizada_original = tokenizer.tokenize(sentenca3_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3_tokenizada_original)\n",
        "embedding_sentenca3_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca3_original, tokenizer)\n",
        "print(\"\\nSentença 3 Original=\\\"\", sentenca3_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca3_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca3_original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4_tokenizada_original = tokenizer.tokenize(sentenca4_original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4_tokenizada_original)\n",
        "embedding_sentenca4_original = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, sentenca4_original, tokenizer)\n",
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_original)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_original))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original: ['O que é Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=\" O que é Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['O', 'que', 'é', 'Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 8\n",
            "    Formato modelo : torch.Size([8, 768])\n",
            "    Soma embeddings:  -40.22\n",
            "\n",
            "Sentença 2 Original=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 9 e término em 14\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -18.35\n",
            "\n",
            "Sentença 3 Original=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 15 e término em 20\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -10.76\n",
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 21 e término em 27\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -10.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyIawL-gl9kB"
      },
      "source": [
        "## Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m30dkANxl9kB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac18420-7830-41ac-98ac-5382314318f1"
      },
      "source": [
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_permutado = [documento_original[3],   # \"Aguardo uma resposta, João.\",\n",
        "             documento_original[1],             # \"Qual o conteúdo da prova?\",              \n",
        "             documento_original[0],             # \"Vai cair tudo na prova?\",\n",
        "             documento_original[2]]             # \"Bom Dia, professor.\"]     \n",
        "\n",
        "# Use o documento permutado igual ao original para testar se as medidas estão corretas\n",
        "#documento_permutado = documento_original\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documento_permutado_concatenado = \" \".join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + documento_permutado_concatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 O               231\n",
            " 15 que             179\n",
            " 16 é               253\n",
            " 17 Bom           8,399\n",
            " 18 Dia           3,616\n",
            " 19 ,               117\n",
            " 20 professor     2,917\n",
            " 21 .               119\n",
            " 22 Vai          20,805\n",
            " 23 cair          9,322\n",
            " 24 tudo          2,745\n",
            " 25 na              229\n",
            " 26 prova         2,310\n",
            " 27 ?               136\n",
            " 28 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Muz4w-9ol9kC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b056b72f-7c8a-460d-e097-877b5b5d5511"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laIJK9b3l9kC"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu5tQwZMl9kC"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14LzqtKbl9kC"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhLV2Leel9kC"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwA1d4C6l9kC"
      },
      "source": [
        "Recupera a saída da última camada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjWjud21l9kD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a1cf1c-b75f-4126-f31e-4c481e26d3a8"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 29, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiiNFe8ul9kD"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GeLFhGJl9kD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e76e1e7d-d717-43ec-d6dc-f283d63e1386"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embedding_documento_permutado = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", embedding_documento_permutado.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([29, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0d5gJKEl9kD"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfdswxTHl9kD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c626da00-c12a-4614-a8ed-b52709c0f325"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_permutado = documento_permutado[0]\n",
        "sentenca2_permutado = documento_permutado[1]\n",
        "sentenca3_permutado = documento_permutado[2]\n",
        "sentenca4_permutado = documento_permutado[3]\n",
        "\n",
        "embedding_sentenca1_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca1_permutado, tokenizer)\n",
        "embedding_sentenca2_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca2_permutado, tokenizer)\n",
        "embedding_sentenca3_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca3_permutado, tokenizer)\n",
        "embedding_sentenca4_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca4_permutado, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento permutado.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1_permutado,\"-\", str(embedding_sentenca1_permutado[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1_original,\"-\", str(torch.sum(embedding_sentenca1_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2_permutado,\"-\", str(embedding_sentenca2_permutado[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2_permutado,\"-\", str(torch.sum(embedding_sentenca2_permutado[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3_permutado,\"-\", str(embedding_sentenca3_permutado[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3_permutado,\"-\", str(torch.sum(embedding_sentenca3_original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4_permutado,\"-\", str(embedding_sentenca4_permutado[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4_permutado,\"-\", str(torch.sum(embedding_sentenca4_permutado[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento permutado.\n",
            "\n",
            "Sentença 1: Aguardo uma resposta, João. - tensor([[ 0.0441, -0.4007,  0.6877,  ..., -0.5086,  0.2947, -0.4032],\n",
            "        [-0.0931, -0.2006,  0.2647,  ..., -0.5701, -0.3075, -0.3390],\n",
            "        [-0.7200, -0.0675,  0.5023,  ..., -0.1611, -0.2567, -0.3192],\n",
            "        [-0.0799,  0.3343,  0.5963,  ..., -0.0312,  0.0399, -0.4640]])\n",
            "Soma embedding Sentença1: O que é Bom Dia, professor. - tensor(-25.7755)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.5751, -0.2834,  0.2825,  ...,  0.1232, -0.3330, -0.4592],\n",
            "        [-0.0515, -0.0348,  0.7009,  ..., -0.3574, -0.6834,  0.0542],\n",
            "        [ 0.3709, -0.5194,  0.6046,  ...,  0.0727, -0.4987, -0.2426],\n",
            "        [-0.0233,  0.2508,  0.5733,  ..., -0.3589, -0.1004, -0.0215]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-16.6053)\n",
            "\n",
            "Sentença 3: O que é Bom Dia, professor. - tensor([[ 0.0294,  0.0661,  0.8628,  ..., -0.3420, -0.8449, -0.2131],\n",
            "        [-0.3199,  0.5190,  1.1196,  ...,  0.3338, -0.9698, -0.2928],\n",
            "        [ 0.4861,  0.3013,  0.7731,  ...,  0.4438, -0.0932, -0.0968],\n",
            "        [ 0.0713,  0.1396,  0.3038,  ..., -0.2723, -0.3770, -0.7039]])\n",
            "Soma embedding Sentença3: O que é Bom Dia, professor. - tensor(-7.3697)\n",
            "\n",
            "Sentença 4: Vai cair tudo na prova? - tensor([[-0.0886, -0.4015, -0.0874,  ..., -0.1274, -0.6107, -0.0289],\n",
            "        [ 0.3942,  0.0873, -0.1043,  ..., -0.5682, -0.4987,  0.2188],\n",
            "        [ 0.1065, -0.0356, -0.3426,  ...,  0.4179, -0.4620,  0.0584],\n",
            "        [-0.1307,  0.3498,  0.7412,  ...,  0.3045, -0.2903, -0.2333]])\n",
            "Soma embedding Sentença4: Vai cair tudo na prova? - tensor(-4.6941)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufyOj-3Ql9kD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29a350e9-5141-4877-ccff-70169ddc0a59"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1_permutado = documento_permutado[0]\n",
        "sentenca2_permutado = documento_permutado[1]\n",
        "sentenca3_permutado = documento_permutado[2]\n",
        "sentenca4_permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1_tokenizada_permutado = tokenizer.tokenize(sentenca1_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca1_tokenizada_permutado)\n",
        "embedding_sentenca1_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca1_permutado, tokenizer)\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2_tokenizada_permutado = tokenizer.tokenize(sentenca2_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca2_tokenizada_permutado)\n",
        "embedding_sentenca2_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca2_permutado, tokenizer)\n",
        "print(\"\\nSentença 2 Permutada=\\\"\", sentenca2_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca2_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca2_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3_tokenizada_permutado = tokenizer.tokenize(sentenca3_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca3_tokenizada_permutado)\n",
        "embedding_sentenca3_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca3_permutado, tokenizer)\n",
        "print(\"\\nSentença 3 Permutada=\\\"\", sentenca3_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca3_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca3_permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4_tokenizada_permutado = tokenizer.tokenize(sentenca4_permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,sentenca4_tokenizada_permutado)\n",
        "embedding_sentenca4_permutado = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, sentenca4_permutado, tokenizer)\n",
        "print(\"\\nSentença 4 Permutada=\\\"\", sentenca4_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_permutado)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_permutado))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'O que é Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -13.57\n",
            "\n",
            "Sentença 2 Permutada=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -19.53\n",
            "\n",
            "Sentença 3 Permutada=\" O que é Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['O', 'que', 'é', 'Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 14 e término em 21\n",
            "    Formato modelo : torch.Size([8, 768])\n",
            "    Soma embeddings:  -36.07\n",
            "\n",
            "Sentença 4 Permutada=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 22 e término em 27\n",
            "    Formato modelo : torch.Size([6, 768])\n",
            "    Soma embeddings:  -6.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dceUXyTOl9kE"
      },
      "source": [
        "## Examinando as sentenças\n",
        "\n",
        "A mesma sentença apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL9sCL3vl9kE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c4ddd10-b24f-4258-9fc3-6dc6d3e484e2"
      },
      "source": [
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4_original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4_tokenizada_original)\n",
        "print(\"    Formato modelo :\", embedding_sentenca4_original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca4_original))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embedding_sentenca4_original[:4]))\n",
        "\n",
        "print(\"\\nSentença 1 Permutada=\\\"\", sentenca1_permutado, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1_tokenizada_permutado)\n",
        "print(\"    Formato modelo :\", embedding_sentenca1_permutado.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embedding_sentenca1_permutado))\n",
        "print(\"    Os 4 primeiros embeddings:\", str(embedding_sentenca1_permutado[:4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -10.72\n",
            "    Os 4 primeiros embeddings: tensor([[-0.0493, -0.3044,  0.4427,  ..., -0.3811,  0.2865, -0.3818],\n",
            "        [-0.2567, -0.1567,  0.0947,  ..., -0.4835, -0.2826, -0.4569],\n",
            "        [-0.4932, -0.0126,  0.3327,  ..., -0.1175, -0.3180, -0.3177],\n",
            "        [ 0.0216,  0.2718,  0.4657,  ...,  0.1346,  0.0899, -0.6842]])\n",
            "\n",
            "Sentença 1 Permutada=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 768])\n",
            "    Soma embeddings:  -13.57\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.0441, -0.4007,  0.6877,  ..., -0.5086,  0.2947, -0.4032],\n",
            "        [-0.0931, -0.2006,  0.2647,  ..., -0.5701, -0.3075, -0.3390],\n",
            "        [-0.7200, -0.0675,  0.5023,  ..., -0.1611, -0.2567, -0.3192],\n",
            "        [-0.0799,  0.3343,  0.5963,  ..., -0.0312,  0.0399, -0.4640]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jESgdoQ8l9kM"
      },
      "source": [
        "## Similaridade de cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cf1Y9arl9kM"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCosseno(embeddings1, embeddings2):\n",
        "    \"\"\"\n",
        "      Similaridade do cosseno dos embeddings dos textos.\n",
        "      \n",
        "      Parâmetros:\n",
        "      `embeddings1` - Um embedding a ser medido.\n",
        "      `embeddings2` - Um embedding a ser medido.\n",
        "    \"\"\"    \n",
        "    similaridade = 1 - cosine(embeddings1, embeddings2)\n",
        "\n",
        "    return similaridade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NVHFoysl9kM"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbUGxtIHl9kM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01168c74-93f0-4a74-f8e3-39665939bdea"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosOriginal = float(somaScos)/float(n-1)\n",
        "print(\"Ccos Original:\", CcosOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['O que é Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.558891087770462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh3HGUMIl9kM"
      },
      "source": [
        "### Calcula a média aritmética da similaridade do coseno entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ7qnwBRl9kM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b836b417-3336-4b59-f688-2e9ce8d8b927"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaScos = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "   # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "CcosPermutado = float(somaScos)/float(np-1)\n",
        "print(\"Ccos Original:\", CcosPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'O que é Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ccos Original: 0.43205444018046063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYHLZrgIl9kN"
      },
      "source": [
        "### Compara as médias da similaridade de cosseno dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 1.\n",
        "- Documentos com sentenças diferenntes resulta uma medida menor que 1.\n",
        "- Documento com sentenças muito diferentes apresentam valores menores que 1.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de similaridade.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeIb45RQl9kN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d54b4470-5692-491d-94c7-80afc55ce7bf"
      },
      "source": [
        "print(\"Ccos Original :\", CcosOriginal)\n",
        "print(\"Ccos Permutado:\", CcosPermutado)\n",
        "\n",
        "if (CcosOriginal > CcosPermutado):\n",
        "    print(\"Documento original tem maior similaridade de cosseno entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem menor similaridade de cosseno entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos Original : 0.558891087770462\n",
            "Ccos Permutado: 0.43205444018046063\n",
            "Documento original tem maior similaridade de cosseno entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RdcWF9Tl9kN"
      },
      "source": [
        "## Distância euclidiana entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância L2 ou norma L2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL4gMjoll9kN"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distanciaEuclidiana(documento1, documento2):\n",
        "    \"\"\"\n",
        "    Distância euclidiana entre os embeddings dos documentos.\n",
        "    Possui outros nomes como distância L2 ou norma L2.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `documento1` - Um documento a ser medido.           \n",
        "    `documento2` - Um documento a ser medido.\n",
        "    \"\"\"\n",
        "    \n",
        "    distancia = euclidean(documento1, documento2)\n",
        "    \n",
        "    return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHA388-ql9kN"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr5jWF8Il9kN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be9b944-b63a-4866-b5a6-860dd3068944"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer, filtro=\"NOUN\")        \n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucOriginal = float(somaSeuc)/float(n-1)\n",
        "print(\"Ceuc Original:\", CeucOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['O que é Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 10.241957823435465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpu9QRx4l9kO"
      },
      "source": [
        "### Calcula a média aritmética da distância euclidiana entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhLznobil9kO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61aaab68-a0c4-4469-f2f0-f5aa1c2f7e14"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSeuc = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Seuc = distanciaEuclidiana(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSeuc = somaSeuc + Seuc\n",
        "\n",
        "CeucPermutado = float(somaSeuc)/float(np-1)\n",
        "print(\"Ceuc Original:\", CeucPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'O que é Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 11.727805137634277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2G6XKN-l9kO"
      },
      "source": [
        "### Compara as médias da distância euclidiana dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4UFxZsVl9kO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92732485-00da-4e82-9e0a-94b0014d678f"
      },
      "source": [
        "print(\"Ceuc Original :\", CeucOriginal)\n",
        "print(\"Ceuc Permutado:\", CeucPermutado)\n",
        "\n",
        "if (CeucOriginal < CeucPermutado):\n",
        "    print(\"Documento original tem menor distância euclidiana entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância euclidiana entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ceuc Original : 10.241957823435465\n",
            "Ceuc Permutado: 11.727805137634277\n",
            "Documento original tem menor distância euclidiana entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxAIaJell9kO"
      },
      "source": [
        "## Distância Manhattan entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "\n",
        "Igual subtração absoluta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtUNIARWl9kO"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def distanciaManhattan(embeddings1, embeddings2):\n",
        "    \"\"\"\n",
        "    Distância Manhattan entre os embeddings dos textos \n",
        "    Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `embeddings1` - Um embedding a ser medido.\n",
        "    `embeddings2` - Um embedding a ser medido.\n",
        "    \"\"\"\n",
        "    \n",
        "    distancia = cityblock(embeddings1, embeddings2)\n",
        "\n",
        "    return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKdd7j1wl9kO"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento original. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUjQCxv-l9kP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d99551f5-4d35-46f8-8dd9-89658d9a01c4"
      },
      "source": [
        "print(\"Documento Original  :\", str(documento_original))\n",
        "print(\"Quantidade de sentenças:\",len(documento_original))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_original)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_original[i]\n",
        "    Sj = documento_original[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer, filtro=\"NOUN\")        \n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Sj, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    # print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanOriginal = float(somaSman)/float(n-1)\n",
        "print(\"Cman Original:\", CmanOriginal)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Original  : ['O que é Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Cman Original: 220.30327351888022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jz_oV3yl9kP"
      },
      "source": [
        "### Calcula a média aritmética da distância de manhattan entre os embeddings das sentenças utilizando a média aritmética dos tokens do documento permutado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oUizCe-l9kP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24071747-12c0-4411-f509-f28631b58564"
      },
      "source": [
        "print(\"Documento Permutado :\", str(documento_permutado))\n",
        "print(\"Quantidade de sentenças:\", len(documento_permutado))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "np = len(documento_permutado)\n",
        "\n",
        "somaSman = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(np-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_permutado[i]\n",
        "    Sj = documento_permutado[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento permutado    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Sj, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Diferença entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Sman = distanciaManhattan(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaSman = somaSman + Sman\n",
        "\n",
        "CmanPermutado = float(somaSman)/float(np-1)\n",
        "print(\"Ceuc Original:\", CmanPermutado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento Permutado : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'O que é Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n",
            "Ceuc Original: 253.8108164469401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3WYl6del9kP"
      },
      "source": [
        "### Compara as médias da distância de manhattan dos embeddings das sentenças do documento original e permutado\n",
        "\n",
        "Características das medidas:\n",
        "- Documentos com sentenças iguais resulta uma medida igual a 0.\n",
        "- Documentos com sentenças diferenntes resulta uma medida maior que 0.\n",
        "- Documento com sentenças muito diferentes apresentam valores maiores que 0.\n",
        "- Documentos iguais resultam em medidas iguais. \n",
        "- É uma medida de diferença.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq5uNunkl9kP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e87f6f-c900-4cec-d104-ee45c689e91f"
      },
      "source": [
        "print(\"Cman Original :\", CmanOriginal)\n",
        "print(\"Cman Permutado:\", CmanPermutado)\n",
        "\n",
        "if (CmanOriginal < CmanPermutado):\n",
        "    print(\"Documento original tem menor distância de manhattan entre as sentenças!\")\n",
        "else:\n",
        "    print(\"Documento Permutado tem maior distância de manhattan entre as sentenças!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cman Original : 220.30327351888022\n",
            "Cman Permutado: 253.8108164469401\n",
            "Documento original tem menor distância de manhattan entre as sentenças!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGqyDJIZl9kP"
      },
      "source": [
        "## Resumo\n",
        "\n",
        "Resultado das medidas utilizando a última camada do BERT.\n",
        "\n",
        "Base(MEAN):\n",
        "- Ccos       :   0.67999101          0.69653825\n",
        "- Ceuc       :   6.34085274          6.15486606\n",
        "- Cman       :   136.53579712          132.14489237\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEoyUyoUl9kQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039206b7-da5c-46c6-d667-fdd22513433f"
      },
      "source": [
        "print(\"Resultado das medidas utilizando a última camada do BERT\")\n",
        "print(\"Documento  :   Original            Permutado\")\n",
        "print(\"Ccos       :   {:.8f}          {:.8f}\".format(CcosOriginal,CcosPermutado))\n",
        "print(\"Ceuc       :   {:.8f}          {:.8f}\".format(CeucOriginal,CeucPermutado))\n",
        "print(\"Cman       :   {:.8f}          {:.8f}\".format(CmanOriginal,CmanPermutado))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado das medidas utilizando a última camada do BERT\n",
            "Documento  :   Original            Permutado\n",
            "Ccos       :   0.55889109          0.43205444\n",
            "Ceuc       :   10.24195782          11.72780514\n",
            "Cman       :   220.30327352          253.81081645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKVnNNkQ1Fk4"
      },
      "source": [
        "# Comparando documentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVw9sDeX5Gdd"
      },
      "source": [
        "## Função para retornar a medida de coerência entre sentenças de um documento usando similaridade do coseno."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbA0L_PE1SS0"
      },
      "source": [
        "def getMedidaCoerenciaCosseno(documento, concat4=True, filtro=\"ALL\"):\n",
        "\n",
        "  # Concatena as sentenças do documento em uma string\n",
        "  stringDocumento = \" \".join(documento)\n",
        "\n",
        "  # Adiciona os tokens especiais\n",
        "  documento_marcado = \"[CLS] \" + stringDocumento + \" [SEP]\"\n",
        "\n",
        "  # Divide a sentença em tokens\n",
        "  documento_tokenizado = tokenizer.tokenize(documento_marcado)\n",
        "\n",
        "  # Mapeia os tokens em seus índices do vocabulário\n",
        "  documento_tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n",
        "\n",
        "  # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "  mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "  # Importa a bibliteca\n",
        "  import torch\n",
        "\n",
        "  # Converte as entradas de listas para tensores do torch\n",
        "  tokens_tensores = torch.as_tensor([documento_tokens_indexados])\n",
        "  mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "\n",
        "  # Prediz os atributos dos estados ocultos para cada camada\n",
        "  with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "  if concat4 == True:\n",
        "    # Cria uma lista com os tensores a serem concatenados\n",
        "    # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "    # Lista com os tensores a serem concatenados\n",
        "    lista_concat = []\n",
        "    # Percorre os 4 últimos\n",
        "    for i in [-1,-2,-3,-4]:\n",
        "        # Concatena da lista\n",
        "        lista_concat.append(outputs[2][i])\n",
        "        # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "        #print(\"lista_concat=\",len(lista_concat))\n",
        "\n",
        "    # Realiza a concatenação dos embeddings de todos as camadas\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "    concat4_hidden_states = torch.cat(lista_concat, dim=-1)\n",
        "    # Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)\n",
        "\n",
        "    # Verifica se a primeira dimensão é igual 1 para remover a dimensão de lote \"batches\"\n",
        "    if concat4_hidden_states.shape[0] == 1:\n",
        "        # Usa o método \"squeeze\" para remover a primeira dimensão(0) pois possui tamanho 1\n",
        "        embedding_documento = torch.squeeze(concat4_hidden_states, dim=0)   \n",
        "\n",
        "  else:\n",
        "    # Recupera a última e única camada da saída\n",
        "    last_hidden_states = outputs[0]\n",
        "\n",
        "    # Verifica se a primeira dimensão é igual 1 para remover a dimensão de lote \"batches\"\n",
        "    if last_hidden_states.shape[0] == 1:\n",
        "        # Usa o método \"squeeze\" para remover a primeira dimensão(0) pois possui tamanho 1\n",
        "        embedding_documento = torch.squeeze(last_hidden_states, dim=0)   \n",
        "\n",
        "  # Quantidade de sentenças no documento\n",
        "  n = len(documento)\n",
        "\n",
        "  somaScos = 0\n",
        "\n",
        "  # Percorre as sentenças do documento\n",
        "  for i in range(n-1):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento[i]\n",
        "    Sj = documento[i+1]\n",
        "\n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento, stringDocumento, Si, tokenizer, filtro=filtro)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Sj (Sentença j), tokenizador\n",
        "    embeddingSj = getEmbeddingSentencaEmbeddingDocumento(embedding_documento, stringDocumento, Sj, tokenizer, filtro=filtro)\n",
        "    # Saída: <qtde_tokens_Sj> x <768 ou 1024>\n",
        "    #print(\"embeddingSj=\", embeddingSj.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Sj, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSj = torch.mean(embeddingSj, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSj=\", mediaEmbeddingSj.shape)\n",
        "  \n",
        "    # Similaridade entre os embeddings Si e Sj\n",
        "    # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "    Scos = similaridadeCosseno(mediaEmbeddingSi, mediaEmbeddingSj)\n",
        "    # Saída: Um número real\n",
        "    \n",
        "    # Acumula a medida\n",
        "    somaScos = somaScos + Scos\n",
        "\n",
        "  CcosOriginal = float(somaScos)/float(n-1)\n",
        "  return CcosOriginal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCZF2v_H811O"
      },
      "source": [
        "## Exemplo 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp0FIK1V5LV2"
      },
      "source": [
        "## Comparando dois documentos distintos usando a medida de similaridade do cosseno de sentenças considerando todas(ALL) as palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_sJroEK1Me9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d17b804f-9732-45eb-c33a-de23897c3c4c"
      },
      "source": [
        "# Define um documento\n",
        "documento_1 = [\"Fui de carro.\",\n",
        "             \"O caminhão tombou.\",\n",
        "             \"O ônibus atrasou.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [\"O gato saiu correndo\",   \n",
        "             \"O cachorro latiu\",             \n",
        "             \"O passáro está no ninho.\"]             \n",
        "\n",
        "Ccos1 =  getMedidaCoerenciaCosseno(documento_1)\n",
        "Ccos2 =  getMedidaCoerenciaCosseno(documento_2)\n",
        "print(\"Ccos1:\", Ccos1)\n",
        "print(\"Ccos2:\", Ccos2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos1: 0.8847753405570984\n",
            "Ccos2: 0.8620311915874481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl4LrTEHcTk0"
      },
      "source": [
        "## Comparando dois documentos distintos usando a medida de similaridade do cosseno de sentenças e palavras relevantes (CLEAN - stopwords)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L98YJPQtcTk2",
        "outputId": "c957c616-9fd9-459a-c1c0-b7670eca9ea1"
      },
      "source": [
        "# Define um documento\n",
        "documento_1 = [\"Fui de carro.\",\n",
        "             \"O caminhão tombou.\",              \n",
        "             \"O ônibus atrasou.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [\"O gato saiu correndo\",   \n",
        "             \"O cachorro latiu\",             \n",
        "             \"O passáro está no ninho.\"]             \n",
        "\n",
        "Ccos1 =  getMedidaCoerenciaCosseno(documento_1, filtro=\"CLEAN\")\n",
        "Ccos2 =  getMedidaCoerenciaCosseno(documento_2, filtro=\"CLEAN\")\n",
        "print(\"Ccos1:\", Ccos1)\n",
        "print(\"Ccos2:\", Ccos2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos1: 0.8804349005222321\n",
            "Ccos2: 0.8625308275222778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx1v76PJNQxz"
      },
      "source": [
        "## Comparando dois documentos distintos usando a medida de similaridade do cosseno de sentenças considerando palavras relevantes(NOUN - substantivos)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJhdhwZMNQxz",
        "outputId": "ad68acbd-6964-4aec-c8bc-b840e2da2089"
      },
      "source": [
        "# Define um documento\n",
        "documento_1 = [\"Fui de carro.\",\n",
        "             \"O caminhão tombou.\",              \n",
        "             \"O ônibus atrasou.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [\"O gato saiu correndo\",   \n",
        "             \"O cachorro latiu\",             \n",
        "             \"O passáro está no ninho.\"]             \n",
        "\n",
        "Ccos1 =  getMedidaCoerenciaCosseno(documento_1, filtro=\"NOUN\")\n",
        "Ccos2 =  getMedidaCoerenciaCosseno(documento_2, filtro=\"NOUN\")\n",
        "print(\"Ccos1:\", Ccos1)\n",
        "print(\"Ccos2:\", Ccos2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos1: 0.7068907916545868\n",
            "Ccos2: 0.6815352439880371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9FVGMBU5aqK"
      },
      "source": [
        "## Comparando dois documentos distintos usando a medida de similaridade do cosseno de sentenças considerando todas(ALL) as palavras relevantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lFQfJmS4uCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5afdf919-e9f1-4b81-90b7-e9862b709fab"
      },
      "source": [
        "# Define um documento\n",
        "documento_1 = [\"Fui de carro.\",\n",
        "             \"O caminhão tombou.\",              \n",
        "             \"O ônibus atrasou.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [\"O gato saiu correndo\",   \n",
        "             \"O cachorro latiu\",             \n",
        "             \"O jogo de futebol atrasou.\"]                         \n",
        "\n",
        "Ccos1 =  getMedidaCoerenciaCosseno(documento_1)\n",
        "Ccos2 =  getMedidaCoerenciaCosseno(documento_2)\n",
        "print(\"Ccos1:\", Ccos1)\n",
        "print(\"Ccos2:\", Ccos2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos1: 0.8847753405570984\n",
            "Ccos2: 0.8523620963096619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScpVYF5Kcwqa"
      },
      "source": [
        "## Comparando dois documentos distintos usando a medida de similaridade do cosseno de sentenças e palavras relevante (CLEAN-stopwords)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm-g4y8xcwqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa4d060b-9dd8-4762-e1fe-09d0c605d82f"
      },
      "source": [
        "# Define um documento\n",
        "documento_1 = [\"Fui de carro.\",\n",
        "             \"O caminhão tombou.\",              \n",
        "             \"O ônibus atrasou.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [\"O gato saiu correndo\",   \n",
        "             \"O cachorro latiu\",             \n",
        "             \"O jogo de futebol atrasou.\"]                         \n",
        "\n",
        "Ccos1 =  getMedidaCoerenciaCosseno(documento_1, filtro=\"CLEAN\")\n",
        "Ccos2 =  getMedidaCoerenciaCosseno(documento_2, filtro=\"CLEAN\")\n",
        "print(\"Ccos1:\", Ccos1)\n",
        "print(\"Ccos2:\", Ccos2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos1: 0.8804349005222321\n",
            "Ccos2: 0.8567197024822235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ2kKBHx88SI"
      },
      "source": [
        "## Exemplo 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBKnx0xW6OoR"
      },
      "source": [
        "## Comparando dois documentos com um contendo sentenças permutadas usando a medida de similaridade do cosseno de sentenças considerando todas(ALL) as palavras relevantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKooY6Gw5iUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9069b43c-7e1e-4afb-de1a-e35d5b4587e9"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_1 = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [documento_1[3],       # \"Aguardo uma resposta, João.\",\n",
        "             documento_1[1],     # \"Qual o conteúdo da prova?\",              \n",
        "             documento_1[0],     # \"Bom Dia, professor.\",\n",
        "             documento_1[2]]     # \"Vai cair tudo na prova?\"]     \n",
        "\n",
        "Ccos1 =  getMedidaCoerenciaCosseno(documento_1)\n",
        "Ccos2 =  getMedidaCoerenciaCosseno(documento_2)\n",
        "print(\"Ccos1:\", Ccos1)\n",
        "print(\"Ccos2:\", Ccos2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos1: 0.8077195882797241\n",
            "Ccos2: 0.7713711063067118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4sUw9_gc8HM"
      },
      "source": [
        "## Comparando dois documentos com um contendo sentenças permutadas usando a medida de similaridade do cosseno de sentenças e palavras relevantes(CLEAN-stopwords)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhXGBZU2c8HN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c52cb4-0dea-45df-b67e-94f2e1ce7cff"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_1 = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [documento_1[3],     # \"Aguardo uma resposta, João.\",\n",
        "           documento_1[1],     # \"Qual o conteúdo da prova?\",              \n",
        "           documento_1[0],     # \"Bom Dia, professor.\",\n",
        "           documento_1[2]]     # \"Vai cair tudo na prova?\"]     \n",
        "\n",
        "Ccos1 =  getMedidaCoerenciaCosseno(documento_1, filtro=\"CLEAN\")\n",
        "Ccos2 =  getMedidaCoerenciaCosseno(documento_2, filtro=\"CLEAN\")\n",
        "print(\"Ccos1:\", Ccos1)\n",
        "print(\"Ccos2:\", Ccos2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos1: 0.8046140074729919\n",
            "Ccos2: 0.7701888481775919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRunnvUyM5wn"
      },
      "source": [
        "## Comparando dois documentos com um contendo sentenças permutadas usando a medida de similaridade do cosseno de sentenças considerando as palavras relevantes(NOUN-substantivo)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHHszbUfM5wn",
        "outputId": "c21ba62b-8eef-4a5b-91f0-562a28ae2630"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento_1 = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [documento_1[3],     # \"Aguardo uma resposta, João.\",\n",
        "           documento_1[1],     # \"Qual o conteúdo da prova?\",              \n",
        "           documento_1[0],     # \"Bom Dia, professor.\",\n",
        "           documento_1[2]]     # \"Vai cair tudo na prova?\"]   \n",
        "\n",
        "Ccos1 =  getMedidaCoerenciaCosseno(documento_1, filtro=\"NOUN\")\n",
        "Ccos2 =  getMedidaCoerenciaCosseno(documento_2, filtro=\"NOUN\")\n",
        "print(\"Ccos1:\", Ccos1)\n",
        "print(\"Ccos2:\", Ccos2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ccos1: 0.7208682298660278\n",
            "Ccos2: 0.6549725333849589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyLmnlBWZ6c5"
      },
      "source": [
        "# Comparando graficamente os embeddings gerados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDhlpbAajCGO"
      },
      "source": [
        "import matplotlib\n",
        "\n",
        "#TAMANHO_FONTE = 28\n",
        "#matplotlib.rc(\"font\", size=TAMANHO_FONTE)          # Controla o tamanho do do documento default\n",
        "#matplotlib.rc(\"axes\", titlesize=TAMANHO_FONTE)     # Tamanho da fonte do eixo do título\n",
        "#matplotlib.rc(\"axes\", labelsize=TAMANHO_FONTE)     # Tamanho da fonte dos rótulos do eixo x e y\n",
        "#matplotlib.rc(\"xtick\", labelsize=TAMANHO_FONTE)    # Tamanho da fonte das marcações do eixo y\n",
        "#matplotlib.rc(\"ytick\", labelsize=TAMANHO_FONTE)    # Tamanho da fonte dos marcações do eixo x\n",
        "#matplotlib.rc(\"legend\", fontsize=TAMANHO_FONTE-2)  # Tamanho da fonte da legenda\n",
        "#matplotlib.rc(\"figure\", titlesize=TAMANHO_FONTE + 4)   # Tamanho da fonte do título da figura"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHaf_DQceo3G"
      },
      "source": [
        "## Gerando embeddings das sentenças separadamente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOvAhXTjLXN_"
      },
      "source": [
        "### Calculando a similaridade com a primeira sentença"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsecc4RhSKPH"
      },
      "source": [
        "# Import das biblioteca\n",
        "import pandas as pd\n",
        "\n",
        "# Define um documento com 4 sentenças\n",
        "documento_1 = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Define um documento com a permutação das sentenças do documento original\n",
        "documento_2 = [documento_1[3],     # \"Aguardo uma resposta, João.\",\n",
        "           documento_1[1],     # \"Qual o conteúdo da prova?\",              \n",
        "           documento_1[0],     # \"Bom Dia, professor.\",\n",
        "           documento_1[2]]     # \"Vai cair tudo na prova?\"]   \n",
        "\n",
        "# Converte o documento em um dataframe\n",
        "df1 = pd.DataFrame(documento_1, columns = [\"sentenca\"])\n",
        "\n",
        "df2 = pd.DataFrame(documento_2, columns = [\"sentenca\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0N9Tf0ie4Vg"
      },
      "source": [
        "Gera os embeddings de cada sentença"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5fIzeEzUEOa"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Guarda os embeddings das sentenças\n",
        "matrix_embedding1 = []\n",
        "\n",
        "# Calcula a média dos embeddings das sentenças\n",
        "matrix_media_embedding1 = []\n",
        "\n",
        "for i,sentenca in enumerate(documento_1):\n",
        "    # Gera os embeddings da sentença utiliza a concatenação das 4 últimas camadas\n",
        "    embedding, tokens = getEmbeddingsConcat4UltimasCamadas(sentenca, model, tokenizer)    \n",
        "    # Guarda os embeddings da sentença\n",
        "    matrix_embedding1.append(embedding)\n",
        "    # Calcula a média dos embeddings dos tokens da sentença\n",
        "    media_embedding = torch.mean(embedding, dim=0)    \n",
        "    # Converte em um array numpy\n",
        "    media = media_embedding.numpy()\n",
        "    # Adiciona na lista\n",
        "    matrix_media_embedding1.append(media)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsZ9OIyOe8HJ"
      },
      "source": [
        "Gera os embeddings de cada sentença"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB21PEwz7Ptg"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Guarda os embeddings das sentenças\n",
        "matrix_embedding2 = []\n",
        "\n",
        "# Calcula a média dos embeddings das sentenças\n",
        "matrix_media_embedding2 = []\n",
        "\n",
        "for i,sentenca in enumerate(documento_2):\n",
        "    # Gera os embeddings da sentença utiliza a concatenação das 4 últimas camadas\n",
        "    embedding, tokens = getEmbeddingsConcat4UltimasCamadas(sentenca, model, tokenizer)    \n",
        "    # Guarda os embeddings da sentença\n",
        "    matrix_embedding2.append(embedding)\n",
        "    # Calcula a média dos embeddings dos tokens da sentença\n",
        "    media_embedding = torch.mean(embedding, dim=0)    \n",
        "    # Converte em um array numpy\n",
        "    media = media_embedding.numpy()\n",
        "    # Adiciona na lista\n",
        "    matrix_media_embedding2.append(media)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcQ5yITSfBsG"
      },
      "source": [
        "Calcula a similaridade do cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_media_embedding1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvi8pFjrs562",
        "outputId": "e0f166fe-eef7-47a6-906f-1d378f2102aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([ 0.13736138,  0.06199554,  0.32553914, ..., -0.26460057,\n",
              "         0.37163997, -0.02382547], dtype=float32),\n",
              " array([ 0.08334768, -0.18268718,  0.592409  , ..., -0.12738705,\n",
              "        -0.00595763, -0.20753936], dtype=float32),\n",
              " array([ 0.13447309,  0.11853971,  0.06020072, ..., -0.07501118,\n",
              "        -0.25153604,  0.03504405], dtype=float32),\n",
              " array([ 0.03715988, -0.07364505,  0.33941993, ..., -0.07329401,\n",
              "         0.17373042,  0.24298403], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "sOitleB8LXYI",
        "outputId": "ecbb45ae-85c9-451e-bc0a-43cdaf668315"
      },
      "source": [
        "# Importa a biblioteca\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Coloca todos os embeddings de sentença em uma matriz\n",
        "embed_matrix1 = np.array([x for x in matrix_media_embedding1])\n",
        "embed_matrix2 = np.array([x for x in matrix_media_embedding2])\n",
        "\n",
        "# Calcula a similaridade do coseno entre as sentenças\n",
        "cos_matrix = cosine_similarity(embed_matrix1,embed_matrix2)\n",
        "\n",
        "# Coloca a similaridade para a primeira sentença\n",
        "df1[\"medida\"] = cos_matrix[0]\n",
        "\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      sentenca    medida\n",
              "0          Bom Dia, professor.  0.870976\n",
              "1    Qual o conteúdo da prova?  0.817182\n",
              "2      Vai cair tudo na prova?  1.000000\n",
              "3  Aguardo uma resposta, João.  0.824109"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4eaeb937-57c3-49ff-8626-e315f31613cc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentenca</th>\n",
              "      <th>medida</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bom Dia, professor.</td>\n",
              "      <td>0.870976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Qual o conteúdo da prova?</td>\n",
              "      <td>0.817182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Vai cair tudo na prova?</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aguardo uma resposta, João.</td>\n",
              "      <td>0.824109</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4eaeb937-57c3-49ff-8626-e315f31613cc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4eaeb937-57c3-49ff-8626-e315f31613cc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4eaeb937-57c3-49ff-8626-e315f31613cc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLL7ukdPosEn"
      },
      "source": [
        "### Mapa de calor calculado com a similaridade cosseno entre todas as sentenças gerados separadamente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "LxUCfzZFs9ls",
        "outputId": "14db8139-ed82-4fe8-8d1a-e0251b9ad429"
      },
      "source": [
        "# Cria o dataframe da lista com as sentenças como nome das colunas\n",
        "df1 = pd.DataFrame(cos_matrix,columns = documento_2)\n",
        "# Indexa pelas sentença do documento_1\n",
        "df1.index = documento_1\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             Aguardo uma resposta, João.  \\\n",
              "Bom Dia, professor.                             0.870976   \n",
              "Qual o conteúdo da prova?                       0.821413   \n",
              "Vai cair tudo na prova?                         0.841723   \n",
              "Aguardo uma resposta, João.                     1.000000   \n",
              "\n",
              "                             Qual o conteúdo da prova?  Bom Dia, professor.  \\\n",
              "Bom Dia, professor.                           0.817182             1.000000   \n",
              "Qual o conteúdo da prova?                     1.000000             0.817182   \n",
              "Vai cair tudo na prova?                       0.893188             0.824109   \n",
              "Aguardo uma resposta, João.                   0.821413             0.870976   \n",
              "\n",
              "                             Vai cair tudo na prova?  \n",
              "Bom Dia, professor.                         0.824109  \n",
              "Qual o conteúdo da prova?                   0.893188  \n",
              "Vai cair tudo na prova?                     1.000000  \n",
              "Aguardo uma resposta, João.                 0.841723  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b74f0dcd-b7a5-4c66-b6fd-b168003dc6d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Aguardo uma resposta, João.</th>\n",
              "      <th>Qual o conteúdo da prova?</th>\n",
              "      <th>Bom Dia, professor.</th>\n",
              "      <th>Vai cair tudo na prova?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Bom Dia, professor.</th>\n",
              "      <td>0.870976</td>\n",
              "      <td>0.817182</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.824109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Qual o conteúdo da prova?</th>\n",
              "      <td>0.821413</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.817182</td>\n",
              "      <td>0.893188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Vai cair tudo na prova?</th>\n",
              "      <td>0.841723</td>\n",
              "      <td>0.893188</td>\n",
              "      <td>0.824109</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Aguardo uma resposta, João.</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.821413</td>\n",
              "      <td>0.870976</td>\n",
              "      <td>0.841723</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b74f0dcd-b7a5-4c66-b6fd-b168003dc6d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b74f0dcd-b7a5-4c66-b6fd-b168003dc6d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b74f0dcd-b7a5-4c66-b6fd-b168003dc6d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "id": "havdK-ZliXAA",
        "outputId": "0c3de69b-bb6b-49c3-b604-b5fdf2a5e091"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tamanho da figura\n",
        "fig, ax = plt.subplots(figsize=(18,12))\n",
        "\n",
        "# Cria o gráfico\n",
        "ax = sns.heatmap(cos_matrix, xticklabels=documento_2, yticklabels=documento_1, cbar_kws={\"label\": \"Medida de similaridade cosseno\"}, annot=True)\n",
        "\n",
        "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, horizontalalignment=\"right\")\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, horizontalalignment=\"right\")\n",
        "\n",
        "# Coloca o título da matriz\n",
        "ax.set_title(\"Similaridade do cosseno entre os embeddings das sentenças gerados separadamente\\n\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x864 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6n3U_D0aE6G"
      },
      "source": [
        "## Gerando embeddings das sentenças simultaneamente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMA19QFtZ4T0"
      },
      "source": [
        "### Calculando a similaridade com a primeira sentença"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCgWmOn9Z4T7"
      },
      "source": [
        "# Import das biblioteca\n",
        "import pandas as pd\n",
        "\n",
        "# Converte o documento em um dataframe\n",
        "df1 = pd.DataFrame(documento_1, columns = [\"sentenca\"])\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documento_original_concatenado = \" \".join(documento_1)\n",
        "\n",
        "df2 = pd.DataFrame(documento_2, columns = [\"sentenca\"])\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documento_permutado_concatenado = \" \".join(documento_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGGGPICufVVm"
      },
      "source": [
        "Gera os embeddings de cada sentença"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlzYvvUrfb-E"
      },
      "source": [
        "# Gerando embeddings dos documentos\n",
        "embedding_documento_original, tokensOriginal = getEmbeddingsConcat4UltimasCamadas(documento_original_concatenado, model, tokenizer)    \n",
        "\n",
        "embedding_documento_permutado, tokensPermutado = getEmbeddingsConcat4UltimasCamadas(documento_permutado_concatenado, model, tokenizer)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ri1po8efht3"
      },
      "source": [
        "Recupera os embeddings de cada sentença"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rsq_U2PFbAl4",
        "outputId": "494b307a-0c1c-4206-c890-eadf06d9867b"
      },
      "source": [
        "print(\"Documento 1  :\", str(documento_1))\n",
        "print(\"Quantidade de sentenças:\",len(documento_1))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_1)\n",
        "\n",
        "# Guarda os embeddings das sentenças\n",
        "matrix_embedding1 = []\n",
        "\n",
        "# Calcula a média dos embeddings das sentenças\n",
        "matrix_media_embedding1 = []\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_1[i]\n",
        "  \n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_original, documento_original_concatenado, Si, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)    \n",
        "\n",
        "    # Guarda os embeddings da sentença\n",
        "    matrix_embedding1.append(embeddingSi)\n",
        "  \n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "\n",
        "    # Adiciona na lista\n",
        "    matrix_media_embedding1.append(mediaEmbeddingSi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento 1  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xh_-y4xfo6g"
      },
      "source": [
        "Recupera os embeddings de cada sentença"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xuOtDjGb11-",
        "outputId": "b25ae157-7366-4ea8-8c09-8c633beb0112"
      },
      "source": [
        "print(\"Documento 2  :\", str(documento_2))\n",
        "print(\"Quantidade de sentenças:\",len(documento_2))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento_2)\n",
        "\n",
        "# Guarda os embeddings das sentenças\n",
        "matrix_embedding2 = []\n",
        "\n",
        "# Calcula a média dos embeddings das sentenças\n",
        "matrix_media_embedding2 = []\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento_2[i]\n",
        "  \n",
        "    # Recupera os embeddings das sentenças no embeddings do documento original    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embedding_documento_permutado, documento_permutado_concatenado, Si, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024>\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)    \n",
        "  \n",
        "    # Guarda os embeddings da sentença\n",
        "    matrix_embedding2.append(embeddingSi)\n",
        "\n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSi=\", mediaEmbeddingSi.shape)\n",
        "\n",
        "    # Adiciona na lista\n",
        "    matrix_media_embedding2.append(mediaEmbeddingSi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento 2  : ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "Quantidade de sentenças: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgrEAtYHfz3i"
      },
      "source": [
        "Calcula a similaridade do cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "vzNzT7gAZ4T8",
        "outputId": "4c8687d5-1b46-45bb-edbb-3ea00d8b8a53"
      },
      "source": [
        "# Importa a biblioteca\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Coloca todos os embeddings de sentença em uma matriz\n",
        "embed_matrix1 = np.array([x.numpy() for x in matrix_media_embedding1])\n",
        "embed_matrix2 = np.array([x.numpy() for x in matrix_media_embedding2])\n",
        "\n",
        "# Calcula a similaridade do coseno entre as sentenças\n",
        "cos_matrix = 1 - cosine_similarity(embed_matrix1,embed_matrix2)\n",
        "\n",
        "# Coloca a similaridade para a primeira sentença\n",
        "df1[\"medida\"] = cos_matrix[0]\n",
        "\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      sentenca    medida\n",
              "0          Bom Dia, professor.  0.473121\n",
              "1    Qual o conteúdo da prova?  0.308333\n",
              "2      Vai cair tudo na prova?  0.012831\n",
              "3  Aguardo uma resposta, João.  0.309963"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-21b687b0-1eb0-4186-838c-056ea11a195e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentenca</th>\n",
              "      <th>medida</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bom Dia, professor.</td>\n",
              "      <td>0.473121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Qual o conteúdo da prova?</td>\n",
              "      <td>0.308333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Vai cair tudo na prova?</td>\n",
              "      <td>0.012831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aguardo uma resposta, João.</td>\n",
              "      <td>0.309963</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21b687b0-1eb0-4186-838c-056ea11a195e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-21b687b0-1eb0-4186-838c-056ea11a195e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-21b687b0-1eb0-4186-838c-056ea11a195e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw_hhpWEZ4T8"
      },
      "source": [
        "### Mapa de calor calculado com a similaridade cosseno entre todas as sentenças gerados separadamente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "RUGpvD-8Z4T8",
        "outputId": "fda92f1a-8ec5-4db2-a76d-b9711d025add"
      },
      "source": [
        "# Cria o dataframe da lista com as sentenças como nome das colunas\n",
        "df1 = pd.DataFrame(cos_matrix,columns = documento_2)\n",
        "# Indexa pela sentença\n",
        "df1.index = documento_1\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             Aguardo uma resposta, João.  \\\n",
              "Bom Dia, professor.                             0.473121   \n",
              "Qual o conteúdo da prova?                       0.404391   \n",
              "Vai cair tudo na prova?                         0.409566   \n",
              "Aguardo uma resposta, João.                     0.036629   \n",
              "\n",
              "                             Qual o conteúdo da prova?  Bom Dia, professor.  \\\n",
              "Bom Dia, professor.                           0.308333             0.012831   \n",
              "Qual o conteúdo da prova?                     0.008158             0.312444   \n",
              "Vai cair tudo na prova?                       0.133707             0.318125   \n",
              "Aguardo uma resposta, João.                   0.415785             0.490673   \n",
              "\n",
              "                             Vai cair tudo na prova?  \n",
              "Bom Dia, professor.                         0.309963  \n",
              "Qual o conteúdo da prova?                   0.122058  \n",
              "Vai cair tudo na prova?                     0.019349  \n",
              "Aguardo uma resposta, João.                 0.410157  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dc824ded-1215-4356-8af9-45ffd3462ab5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Aguardo uma resposta, João.</th>\n",
              "      <th>Qual o conteúdo da prova?</th>\n",
              "      <th>Bom Dia, professor.</th>\n",
              "      <th>Vai cair tudo na prova?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Bom Dia, professor.</th>\n",
              "      <td>0.473121</td>\n",
              "      <td>0.308333</td>\n",
              "      <td>0.012831</td>\n",
              "      <td>0.309963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Qual o conteúdo da prova?</th>\n",
              "      <td>0.404391</td>\n",
              "      <td>0.008158</td>\n",
              "      <td>0.312444</td>\n",
              "      <td>0.122058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Vai cair tudo na prova?</th>\n",
              "      <td>0.409566</td>\n",
              "      <td>0.133707</td>\n",
              "      <td>0.318125</td>\n",
              "      <td>0.019349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Aguardo uma resposta, João.</th>\n",
              "      <td>0.036629</td>\n",
              "      <td>0.415785</td>\n",
              "      <td>0.490673</td>\n",
              "      <td>0.410157</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc824ded-1215-4356-8af9-45ffd3462ab5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dc824ded-1215-4356-8af9-45ffd3462ab5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dc824ded-1215-4356-8af9-45ffd3462ab5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "id": "rVqzhgg1Z4T8",
        "outputId": "c1c3004c-11a7-4330-c07f-60846cf04a6e"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tamanho da figura\n",
        "fig, ax = plt.subplots(figsize=(18,12))\n",
        "\n",
        "# Cria o gráfico\n",
        "ax = sns.heatmap(cos_matrix, xticklabels=documento_2, yticklabels=documento_1, cbar_kws={\"label\": \"Medida de similaridade do cosseno\"}, annot=True)\n",
        "\n",
        "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, horizontalalignment=\"right\")\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, horizontalalignment=\"right\")\n",
        "\n",
        "# Coloca o título da matriz\n",
        "ax.set_title(\"Similaridade do cosseno entre os embeddings das sentenças gerados simultaneamente\\n\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x864 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}