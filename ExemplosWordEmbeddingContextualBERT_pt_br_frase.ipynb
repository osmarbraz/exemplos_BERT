{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExemplosWordEmbeddingContextualBERT_pt_br_frase.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osmarbraz/exemplos_BERT/blob/main/ExemplosWordEmbeddingContextualBERT_pt_br_frase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78HE8FLsKN9Q"
      },
      "source": [
        "#Exemplo de Word Embeddings(pt-br) Contextual usando BERT Transformers by HuggingFace\n",
        "\n",
        "## **A execução pode ser feita através do menu Ambiente de Execução opção Executar tudo.**\n",
        "\n",
        "Exemplos de uso de **Word Embeddings Contextuais** para **desambiguação** de documentos originais e permutados. No final do notebook estão os exemplos com os documentos:\n",
        "\n",
        "*   documento original e permutado\n",
        "\n",
        "**Link biblioteca Huggingface:**\n",
        "https://github.com/huggingface/transformers\n",
        "\n",
        "\n",
        "**Artigo original BERT Jacob Devlin:**\n",
        "https://arxiv.org/pdf/1506.06724.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyxb5Px3p1-e"
      },
      "source": [
        "## 0 - Preparação do ambiente\n",
        "Preparação do ambiente para execução do exemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAPVtRXQqDim"
      },
      "source": [
        "###Tratamento de logs\n",
        "\n",
        "Método para tratamento dos logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcopxbGZqDip"
      },
      "source": [
        "#biblioteca de logging\n",
        "import logging\n",
        "\n",
        "#formatando a mensagem de logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GjYtXcMnSAe"
      },
      "source": [
        "### Identificando o ambiente Colab\n",
        "\n",
        "Cria uma variável para identificar que o notebook está sendo executado no Google Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMiH0E3OnRa1"
      },
      "source": [
        "#se estiver executando no Google Colaboratory\n",
        "import sys\n",
        "#retorna true ou false se estiver no Google Colaboratory\n",
        "IN_COLAB = 'google.colab' in sys.modules"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqa-7WXBAw8q"
      },
      "source": [
        "## 1 - Instalação BERT da Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCdqJCtQN52l"
      },
      "source": [
        "Instala a interface pytorch para o BERT by Hugging Face. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RfUN_KolV-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e1a8356-7b80-4316-f6f4-24aa37f243a3"
      },
      "source": [
        "# Instala a última versão da biblioteca\n",
        "#!pip install transformers\n",
        "\n",
        "# Instala uma versão específica da biblioteca\n",
        "!pip install -U transformers==4.5.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: transformers==4.5.1 in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (0.0.45)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (0.10.2)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.1) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.1) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.1) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQj2wmKDpkrH"
      },
      "source": [
        "## 2 - Download do arquivo do PyTorch Checkpoint\n",
        "\n",
        "Lista de modelos da comunidade:\n",
        "* https://huggingface.co/models\n",
        "\n",
        "Português(https://github.com/neuralmind-ai/portuguese-bert):  \n",
        "* **'neuralmind/bert-base-portuguese-cased'**\n",
        "* **'neuralmind/bert-large-portuguese-cased'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajrTjZzapkrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d9aeb4-de66-4ce3-9659-7ff40c77e4fa"
      },
      "source": [
        "# Importando as bibliotecas\n",
        "import os\n",
        "\n",
        "# Variável para setar o arquivo\n",
        "URL_MODELO = None\n",
        "\n",
        "# Comente uma das urls para carregar modelos de tamanhos diferentes(base/large)\n",
        "# URL_MODELO do arquivo do modelo tensorflow\n",
        "# arquivo menor(base) 1.1 Gbytes\n",
        "# URL_MODELO = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip\"\n",
        "\n",
        "# arquivo grande(large) 3.5 Gbytes\n",
        "URL_MODELO = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-large-portuguese-cased/bert-large-portuguese-cased_pytorch_checkpoint.zip\"\n",
        "\n",
        "# Se a variável foi setada\n",
        "if URL_MODELO:\n",
        "\n",
        "    # Diretório descompactação\n",
        "    DIRETORIO_MODELO = '/content/modelo'\n",
        "\n",
        "    # Recupera o nome do arquivo do modelo da URL_MODELO\n",
        "    arquivo = URL_MODELO.split(\"/\")[-1]\n",
        "\n",
        "    # Nome do arquivo do vocabulário\n",
        "    arquivo_vocab = \"vocab.txt\"\n",
        "\n",
        "    # Caminho do arquivo na URL_MODELO\n",
        "    caminho = URL_MODELO[0:len(URL_MODELO)-len(arquivo)]\n",
        "\n",
        "    # Verifica se a pasta de descompactação existe no pasta corrente\n",
        "    if not os.path.exists(DIRETORIO_MODELO):\n",
        "   \n",
        "        # Baixa o arquivo do modelo\n",
        "        !wget $URL_MODELO\n",
        "    \n",
        "        # Descompacta o arquivo na pasta de descompactação\n",
        "        !unzip -o $arquivo -d $DIRETORIO_MODELO\n",
        "\n",
        "        # Baixa o arquivo do vocabulário\n",
        "        # O vocabulário não está no arquivo compactado acima, mesma url mas arquivo diferente\n",
        "        URL_MODELO_VOCAB = caminho + arquivo_vocab\n",
        "        !wget $URL_MODELO_VOCAB\n",
        "    \n",
        "        # Coloca o arquivo do vocabulário no diretório de descompactação\n",
        "        !mv $arquivo_vocab $DIRETORIO_MODELO\n",
        "            \n",
        "        # Move o arquivo para pasta de descompactação\n",
        "        !mv $arquivo $DIRETORIO_MODELO\n",
        "       \n",
        "        print('Pasta do ' + DIRETORIO_MODELO + ' pronta!')\n",
        "    else:\n",
        "      print('Pasta do ' + DIRETORIO_MODELO + ' já existe!')\n",
        "\n",
        "    # Lista a pasta corrente\n",
        "    !ls -la $DIRETORIO_MODELO\n",
        "else:\n",
        "    print('Variável URL_MODELO não setada!')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pasta do /content/modelo já existe!\n",
            "total 2525908\n",
            "drwxr-xr-x 2 root root       4096 May  6 14:32 .\n",
            "drwxr-xr-x 1 root root       4096 May  6 14:32 ..\n",
            "-rw-r--r-- 1 root root 1244275810 Jan 22  2020 bert-large-portuguese-cased_pytorch_checkpoint.zip\n",
            "-rw-rw-r-- 1 root root        874 Jan 12  2020 config.json\n",
            "-rw-rw-r-- 1 root root 1342014951 Jan 12  2020 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root     209528 Jan 21  2020 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcpd9t9PpkrX"
      },
      "source": [
        "## 3 - Carregando o Tokenizador BERT\n",
        "\n",
        "O tokenizador utiliza WordPiece, veja em [artigo original](https://arxiv.org/pdf/1609.08144.pdf).\n",
        "\n",
        "Carregando o tokenizador da pasta '/content/modelo/' do diretório padrão se variável `URL_MODELO` setada.\n",
        "\n",
        "**Caso contrário carrega da comunidade**\n",
        "\n",
        "Por default(`do_lower_case=True`) todas as letras são colocadas para minúsculas. Para ignorar a conversão para minúsculo use o parâmetro `do_lower_case=False`. Esta opção também considera as letras acentuadas(ãçéí...), que são necessárias a língua portuguesa.\n",
        "\n",
        "O parâmetro `do_lower_case` interfere na quantidade tokens a ser gerado apartir de um texto. Quando igual a `False` reduz a quantidade de tokens gerados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8cKVs4fpkrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f6705a-62c8-4b9c-f190-8fd775fc4433"
      },
      "source": [
        "# Importando as bibliotecas do tokenizador\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Se a variável URL_MODELO foi setada\n",
        "if URL_MODELO:\n",
        "    # Carregando o Tokenizador\n",
        "    print('Carrgando o tokenizador BERT do diretório ' + DIRETORIO_MODELO + '...')\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(DIRETORIO_MODELO, \n",
        "                                              do_lower_case=False)    \n",
        "else:\n",
        "    # Carregando o Tokenizador da comunidade\n",
        "    print('Carregando o tokenizador da comunidade...')\n",
        "    \n",
        "    #tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)\n",
        "    tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased', \n",
        "                                              do_lower_case=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Carrgando o tokenizador BERT do diretório /content/modelo...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m__On2g1a--K"
      },
      "source": [
        "## 4 - Carregando o Modelo BERT(BertModel)\n",
        "\n",
        "Se a variável `URL_MODELO` estiver setada carrega o modelo do diretório `content/modelo`.\n",
        "\n",
        "Caso contrário carrega da comunidade.\n",
        "\n",
        "Carregando o modelo da pasta '/content/modelo/' do diretório padrão.\n",
        "\n",
        "A implementação do huggingface pytorch inclui um conjunto de interfaces projetadas para uma variedade de tarefas de PNL. Embora essas interfaces sejam todas construídas sobre um modelo treinado de BERT, cada uma possui diferentes camadas superiores e tipos de saída projetados para acomodar suas tarefas específicas de PNL.\n",
        "\n",
        "A documentação para estas pode ser encontrada em [aqui](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html).\n",
        "\n",
        "Por default o modelo está em modo avaliação ou seja `model.eval()`.\n",
        "\n",
        "-----------------------\n",
        "\n",
        "Durante a avaliação do modelo, este retorna um número de diferentes objetos com base em como é configurado na chamada do método `from_pretrained`. \n",
        "\n",
        "Quando definimos `output_hidden_states = True` na chamada do método `from_pretrained`, retorno do modelo possui no terceiro item os estados ocultos(**hidden_states**) de todas as camadas.  Veja a documentação para mais detalhes: https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "\n",
        "Quando **`output_hidden_states = True`** model retorna:\n",
        "- outputs[0] = last_hidden_state;\n",
        "- outputs[1] = pooler_output; \n",
        "- outputs[2] = hidden_states.\n",
        "\n",
        "Quando **`output_hidden_states = False`** ou não especificado model retorna:\n",
        "- outputs[0] = last_hidden_state;\n",
        "- outputs[1] = pooler_output.\n",
        "\n",
        "\n",
        "**ATENÇÃO**: O parâmetro ´**output_hidden_states = True**´ habilita gerar as camadas ocultas do modelo. Caso contrário somente a última camada é mantida. Este parâmetro otimiza a memória mas não os resultados.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRV6l_I-qg9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94e96c8a-ca75-4771-c774-d9463a903d0c"
      },
      "source": [
        "# Importando as bibliotecas do Modelo\n",
        "from transformers import BertModel\n",
        "\n",
        "# Se a variável URL_MODELO1 foi setada\n",
        "if URL_MODELO:\n",
        "    # Carregando o Tokenizador\n",
        "    print('Carregando o modelo BERT do diretório ' + DIRETORIO_MODELO + '...')\n",
        "\n",
        "    model = BertModel.from_pretrained(DIRETORIO_MODELO, \n",
        "                                      output_hidden_states = True)    \n",
        "else:\n",
        "    # Carregando o Tokenizador da comunidade\n",
        "    print('Carregando o modelo BERT da comunidade ...')\n",
        "\n",
        "    model = BertModel.from_pretrained('neuralmind/bert-large-portuguese-cased', \n",
        "                                       output_hidden_states = True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Carregando o modelo BERT do diretório /content/modelo...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU3wHzNUmmBP"
      },
      "source": [
        "## 5 - Funções auxiliares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s42mgtmSZ8MR"
      },
      "source": [
        "### getEmbeddingsCamadas\n",
        "\n",
        "Funções que recuperam os embeddings das camadas:\n",
        "- Primeira camada;\n",
        "- Penúltima camada;\n",
        "- Ùltima camada;\n",
        "- Soma das 4 últimas camadas;\n",
        "- Concatenação das 4 últimas camadas;\n",
        "- Soma de todas as camadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgo3EBTRZ9-3"
      },
      "source": [
        "def getEmbeddingPrimeiraCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][0]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingPenultimaCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][-2]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingUltimaCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "     \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][-1]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado    \n",
        "\n",
        "def getEmbeddingSoma4UltimasCamadas(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  embeddingCamadas = output[2][-4:]\n",
        "  # Saída: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "\n",
        "  # Usa o método `stack` para criar uma nova dimensão no tensor \n",
        "  # com a concateção dos tensores dos embeddings.        \n",
        "  #Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultadoStack = torch.stack(embeddingCamadas, dim=0)\n",
        "  # Saída: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  \n",
        "  # Realiza a soma dos embeddings de todos os tokens para as camadas\n",
        "  # Entrada: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  resultado = torch.sum(resultadoStack, dim=0)\n",
        "  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingConcat4UltimasCamadas(output):  \n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Cria uma lista com os tensores a serem concatenados\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  # Lista com os tensores a serem concatenados\n",
        "  listaConcat = []\n",
        "  # Percorre os 4 últimos\n",
        "  for i in [-1,-2,-3,-4]:\n",
        "      # Concatena da lista\n",
        "      listaConcat.append(output[2][i])\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  # Realiza a concatenação dos embeddings de todos as camadas\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = torch.cat(listaConcat, dim=-1)\n",
        "  # Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "    \n",
        "  return resultado   \n",
        "\n",
        "def getEmbeddingSomaTodasAsCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "   \n",
        "  # Retorna todas as camadas descontando a primeira(0)\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  embeddingCamadas = output[2][1:]\n",
        "  # Saída: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  # Usa o método `stack` para criar uma nova dimensão no tensor \n",
        "  # com a concateção dos tensores dos embeddings.        \n",
        "  #Entrada: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultadoStack = torch.stack(embeddingCamadas, dim=0)\n",
        "  # Saída: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "    \n",
        "  # Realiza a soma dos embeddings de todos os tokens para as camadas\n",
        "  # Entrada: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  resultado = torch.sum(resultadoStack, dim=0)\n",
        "  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "    \n",
        "  return resultado"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWqMsrb-ew5T"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm98RoojJcqP"
      },
      "source": [
        "# Import das bibliotecas\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7nx_eZ8hSlr"
      },
      "source": [
        "### getEmbeddingsVisual\n",
        "\n",
        "Função para gerar as coordenadas de plotagem a partir das sentenças de embeddings.\n",
        "\n",
        "Existe uma função para os tipos de camadas utilizadas:\n",
        "- Ùltima camada;\n",
        "- Soma das 4 últimas camadas;\n",
        "- Concatenação das 4 últimas camadas;\n",
        "- Soma de todas as camadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLdbOT8-g43V"
      },
      "source": [
        "def getEmbeddingsVisualUltimaCamada(texto, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    texto_marcado = \"[CLS] \" + texto + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    texto_tokenizado = tokenizador.tokenize(texto_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(texto_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à frase \"1\".\n",
        "    mascara_atencao = [1] * len(texto_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingUltimaCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, texto_tokenizado"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAf9lJJ2hZbt"
      },
      "source": [
        "def getEmbeddingsVisualSoma4UltimasCamadas(texto, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    texto_marcado = \"[CLS] \" + texto + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    texto_tokenizado = tokenizador.tokenize(texto_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(texto_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à frase \"1\".\n",
        "    mascara_atencao = [1] * len(texto_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSoma4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, texto_tokenizado"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XpwSN1ghpnz"
      },
      "source": [
        "def getEmbeddingsVisualConcat4UltimasCamadas(texto, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    texto_marcado = \"[CLS] \" + texto + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    texto_tokenizado = tokenizador.tokenize(texto_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(texto_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à frase \"1\".\n",
        "    mascara_atencao = [1] * len(texto_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingConcat4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, texto_tokenizado"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3KU1EFrnSPK"
      },
      "source": [
        "def getEmbeddingsVisualSomaTodasAsCamadas(texto, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    texto_marcado = \"[CLS] \" + texto + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    texto_tokenizado = tokenizador.tokenize(texto_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(texto_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à frase \"1\".\n",
        "    mascara_atencao = [1] * len(texto_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSomaTodasAsCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, texto_tokenizado"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-3QzDMwmfiJ"
      },
      "source": [
        "## 6 - Exemplo frases de documento original e permutado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlUOqzcYFinG"
      },
      "source": [
        "### Funções auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvWIBFTLJ7z9"
      },
      "source": [
        "def getDocumentoTokenizado(documento, tokenizador):\n",
        "\n",
        "    # Adiciona os tokens especiais.\n",
        "    documentoMarcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Documento tokenizado\n",
        "    documentoTokenizado = tokenizador.tokenize(documentoMarcado)\n",
        "\n",
        "    return documentoTokenizado"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abS44M4yvFxf"
      },
      "source": [
        "# Localiza os índices de início e fim de uma sublista em uma lista\n",
        "def encontrarIndiceSubLista(lista, sublista):\n",
        "    # Recupera o tamanho da lista \n",
        "    h = len(lista)\n",
        "    # Recupera o tamanho da sublista\n",
        "    n = len(sublista)    \n",
        "    skip = {sublista[i]: n - i - 1 for i in range(n - 1)}\n",
        "    i = n - 1\n",
        "    while i < h:\n",
        "        for j in range(n):\n",
        "            if lista[i - j] != sublista[-j - 1]:\n",
        "                i += skip.get(lista[i], n)\n",
        "                break\n",
        "        else:\n",
        "            indiceInicio = i - n + 1\n",
        "            indiceFim = indiceInicio + len(sublista)-1\n",
        "            return indiceInicio, indiceFim\n",
        "    return -1, -1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSQs3O5QpJSj"
      },
      "source": [
        "def getEmbeddingFraseDocumentoEmbedding(embeddingDocumento, documento, frase, tokenizador):\n",
        "  # Tokeniza o documento\n",
        "  documentoTokenizado = getDocumentoTokenizado(documento, tokenizador)  \n",
        "  #print(documentoTokenizado)\n",
        "\n",
        "  # Tokeniza a frase\n",
        "  fraseTokenizada = getDocumentoTokenizado(frase, tokenizador)\n",
        "  \n",
        "  # Remove os tokens de início e fim da frase\n",
        "  fraseTokenizada.remove('[CLS]')\n",
        "  fraseTokenizada.remove('[SEP]')  \n",
        "  #print(fraseTokenizada)\n",
        "  #print(len(fraseTokenizada))\n",
        "  \n",
        "  # Localiza os índices dos tokens da frase no documento\n",
        "  inicio, fim = encontrarIndiceSubLista(documentoTokenizado,fraseTokenizada)\n",
        "  #print(inicio,fim) \n",
        " \n",
        "  # Recupera os embeddings dos tokens da frase a partir dos embeddings do documento\n",
        "  embeddingFrase = embeddingDocumento[inicio:fim+1]\n",
        "  #print(\"embeddingFrase=\", embeddingFrase.shape)\n",
        "  \n",
        "  # Retorna o embedding da frase no documento\n",
        "  return embeddingFrase"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTmrN_IRmfiO"
      },
      "source": [
        "### Documento Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYKIVpzTmfiP",
        "outputId": "534d5859-1f07-4238-bc62-4db734af4a85"
      },
      "source": [
        "# Define um sentença de exemplo com diversos significados da palavra  \"pilha\"\n",
        "documento_original = [\"Bom Dia, professor.\", \n",
        "             \"Qual o conteúdo da prova?\", \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as frases do documento\n",
        "documento_texto_original = ' '.join(documento_original)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + documento_texto_original + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print('{:>3} {:<12} {:>6,}'.format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLsHMNz9mfiQ"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pXg2A6zmfiR",
        "outputId": "0190e298-9d63-4706-ba7d-f5780b64bfd8"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à frase \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM5GUtVNmfiR"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ePiuflemfiS"
      },
      "source": [
        "# Importa a bibliteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qhHUUoAmfiS"
      },
      "source": [
        "Gera os embeddings para o documento original. Guarda somente a última camada da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o970gzwhmfiS"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnXcn_dvmfiT"
      },
      "source": [
        "Recupera a saída"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSiGmlpjmfiT",
        "outputId": "3d56c565-d86e-484a-b19a-8a56b03e34ac"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFDtOmN_mfiV"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM4Aosw4mfiV",
        "outputId": "96c850ae-4bda-4dd1-cc92-2e144aa57ea0"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "token_embeddings_original = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento original tem o formato:\", token_embeddings_original.size())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor de tokens de embedding do documento original tem o formato: torch.Size([26, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T68Aje2tmfiW"
      },
      "source": [
        "Confirmando vetores dependentes do contexto\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeNaW--hmfiW",
        "outputId": "40e50d24-79ef-4701-b930-faf3eddb09e4"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVDBdrHWmfiX"
      },
      "source": [
        "Exibe os embenddings das frases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkZrVaVFmfiX",
        "outputId": "f4104819-c7be-4d58-85bd-9f055b63e9c0"
      },
      "source": [
        "# Índice das frases a serem comparadas\n",
        "frase1Original = documento_original[0]\n",
        "frase2Original = documento_original[1]\n",
        "frase3Original = documento_original[2]\n",
        "frase4Original = documento_original[3]\n",
        "\n",
        "embeddingFrase1Original = getEmbeddingFraseDocumentoEmbedding(token_embeddings_original, documento_texto_original, frase1Original, tokenizer)\n",
        "embeddingFrase2Original = getEmbeddingFraseDocumentoEmbedding(token_embeddings_original, documento_texto_original, frase2Original, tokenizer)\n",
        "embeddingFrase3Original = getEmbeddingFraseDocumentoEmbedding(token_embeddings_original, documento_texto_original, frase3Original, tokenizer)\n",
        "embeddingFrase4Original = getEmbeddingFraseDocumentoEmbedding(token_embeddings_original, documento_texto_original, frase4Original, tokenizer)\n",
        "\n",
        "print('Os primeiros 4 valores de cada frase do documento original.')\n",
        "\n",
        "print('\\nFrase 1:', frase1Original,'-', str(embeddingFrase1Original[:4]))\n",
        "print('Soma embedding Frase1:', frase1Original,'-', str(torch.sum(embeddingFrase1Original[:4])))\n",
        "\n",
        "print('\\nFrase 2:', frase2Original,'-', str(embeddingFrase2Original[:4]))\n",
        "print('Soma embedding Frase2:', frase2Original,'-', str(torch.sum(embeddingFrase2Original[:4])))\n",
        "\n",
        "print('\\nFrase 3:', frase3Original,'-', str(embeddingFrase3Original[:4]))\n",
        "print('Soma embedding Frase3:', frase3Original,'-', str(torch.sum(embeddingFrase3Original[:4])))\n",
        "\n",
        "print('\\nFrase 4:', frase4Original,'-', str(embeddingFrase4Original[:4]))\n",
        "print('Soma embedding Frase4:', frase4Original,'-', str(torch.sum(embeddingFrase4Original[:4])))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Os primeiros 4 valores de cada frase do documento original.\n",
            "\n",
            "Frase 1: Bom Dia, professor. - tensor([[-0.5098, -0.2877,  0.0873,  ...,  0.5436, -0.9302,  0.4668],\n",
            "        [ 0.6078, -0.8869,  0.3736,  ..., -0.3517, -1.2140, -0.3077],\n",
            "        [ 0.9075, -1.1233, -0.0093,  ...,  0.4433, -0.4633, -0.0113],\n",
            "        [ 0.2499, -0.4717, -0.1217,  ...,  0.7079, -0.2300,  0.4911]])\n",
            "Soma embedding Frase1: Bom Dia, professor. - tensor(-113.4554)\n",
            "\n",
            "Frase 2: Qual o conteúdo da prova? - tensor([[-0.3987, -0.9450,  0.1785,  ...,  0.7189, -0.6772, -0.1452],\n",
            "        [ 0.2067, -0.2705,  0.7145,  ...,  0.3047, -0.2718,  0.7577],\n",
            "        [ 0.2355,  0.2686,  0.5669,  ...,  1.0817,  0.5614,  0.3750],\n",
            "        [ 0.5264, -0.4600,  0.4810,  ..., -0.5559, -0.2941,  0.0378]])\n",
            "Soma embedding Frase2: Qual o conteúdo da prova? - tensor(-115.8800)\n",
            "\n",
            "Frase 3: Vai cair tudo na prova? - tensor([[ 0.5178,  0.0863,  0.7394,  ..., -0.5765, -0.6208, -0.2230],\n",
            "        [ 0.1617,  1.1516, -0.0350,  ...,  0.1730,  0.2104, -0.0207],\n",
            "        [ 0.9251,  0.5806,  0.2491,  ...,  0.1687,  0.0772, -0.1173],\n",
            "        [ 0.5782,  1.3571, -0.5161,  ..., -0.0942,  0.0404, -0.0390]])\n",
            "Soma embedding Frase3: Vai cair tudo na prova? - tensor(-110.5299)\n",
            "\n",
            "Frase 4: Aguardo uma resposta, João. - tensor([[ 0.4339, -0.7168,  0.1173,  ...,  0.0768, -1.2036,  0.3863],\n",
            "        [-0.6806, -1.0194, -0.0765,  ..., -0.1768, -0.6326,  0.2895],\n",
            "        [ 1.1384, -0.8541,  0.8992,  ...,  0.2798, -0.0381,  0.5061],\n",
            "        [ 0.9206, -0.9862,  0.1347,  ...,  0.4182, -1.0164,  0.4713]])\n",
            "Soma embedding Frase4: Aguardo uma resposta, João. - tensor(-116.1903)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exqsxerrmfiY"
      },
      "source": [
        "Examinando os embeddings do documento original\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5py_A7lVmfiY",
        "outputId": "fabda2b5-87bb-4af2-e162-f5437f656541"
      },
      "source": [
        "# Índice das frases a serem comparadas\n",
        "frase1Original = documento_original[0]\n",
        "frase2Original = documento_original[1]\n",
        "frase3Original = documento_original[2]\n",
        "frase4Original = documento_original[3]\n",
        "\n",
        "print(\"Documento Original:\", documento_original)\n",
        "\n",
        "# Localiza os índices dos tokens da frase no documento\n",
        "frase1TokenizadaOriginal = tokenizer.tokenize(frase1Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,frase1TokenizadaOriginal)\n",
        "embeddingFrase1Original = getEmbeddingFraseDocumentoEmbedding(token_embeddings_original, documento_texto_original, frase1Original, tokenizer)\n",
        "print('\\nFrase 1 Original=\\'', frase1Original, '\\'')\n",
        "print('    Frase tokenizada:', frase1TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingFrase1Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingFrase1Original))\n",
        "\n",
        "# Localiza os índices dos tokens da frase no documento\n",
        "frase2TokenizadaOriginal = tokenizer.tokenize(frase2Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,frase2TokenizadaOriginal)\n",
        "embeddingFrase2Original = getEmbeddingFraseDocumentoEmbedding(token_embeddings_original, documento_texto_original, frase2Original, tokenizer)\n",
        "print('\\nFrase 2 Original=\\'', frase2Original, '\\'')\n",
        "print('    Frase tokenizada:', frase2TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingFrase2Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingFrase2Original))\n",
        "\n",
        "# Localiza os índices dos tokens da frase no documento\n",
        "frase3TokenizadaOriginal = tokenizer.tokenize(frase3Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,frase3TokenizadaOriginal)\n",
        "embeddingFrase3Original = getEmbeddingFraseDocumentoEmbedding(token_embeddings_original, documento_texto_original, frase3Original, tokenizer)\n",
        "print('\\nFrase 3 Original=\\'', frase3Original, '\\'')\n",
        "print('    Frase tokenizada:', frase3TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingFrase3Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingFrase3Original))\n",
        "\n",
        "# Localiza os índices dos tokens da frase no documento\n",
        "frase4TokenizadaOriginal = tokenizer.tokenize(frase4Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,frase4TokenizadaOriginal)\n",
        "embeddingFrase4Original = getEmbeddingFraseDocumentoEmbedding(token_embeddings_original, documento_texto_original, frase4Original, tokenizer)\n",
        "print('\\nFrase 4 Original=\\'', frase4Original, '\\'')\n",
        "print('    Frase tokenizada:', frase4TokenizadaOriginal)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingFrase4Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingFrase4Original))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Original: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Frase 1 Original=' Bom Dia, professor. '\n",
            "    Frase tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 1024])\n",
            "    Soma embeddings:  -141.45\n",
            "\n",
            "Frase 2 Original=' Qual o conteúdo da prova? '\n",
            "    Frase tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -173.58\n",
            "\n",
            "Frase 3 Original=' Vai cair tudo na prova? '\n",
            "    Frase tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -167.58\n",
            "\n",
            "Frase 4 Original=' Aguardo uma resposta, João. '\n",
            "    Frase tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -200.72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hg9eKyjEfE5"
      },
      "source": [
        "### Documento Permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqHzON3PCa49",
        "outputId": "8b2fd5c2-bada-493f-d707-ab5f7b1ff4b0"
      },
      "source": [
        "# Define um sentença de exemplo com diversos significados da palavra  \"pilha\"\n",
        "documento_permutado = [\"Aguardo uma resposta, João.\",\n",
        "             \"Qual o conteúdo da prova?\", \n",
        "             \"Bom Dia, professor.\",\n",
        "             \"Vai cair tudo na prova?\"]\n",
        "\n",
        "# Concatena as frases do documento\n",
        "documento_texto_permutado = ' '.join(documento_permutado)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_permutado = \"[CLS] \" + documento_texto_permutado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_permutado = tokenizer.tokenize(documento_marcado_permutado)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_permutado = tokenizer.convert_tokens_to_ids(documento_tokenizado_permutado)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_permutado, documento_tokens_indexados_permutado):\n",
        "    print('{:>3} {:<12} {:>6,}'.format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Agu           8,125\n",
            "  2 ##ardo        2,222\n",
            "  3 uma             230\n",
            "  4 resposta      4,299\n",
            "  5 ,               117\n",
            "  6 João          1,453\n",
            "  7 .               119\n",
            "  8 Qual         13,082\n",
            "  9 o               146\n",
            " 10 conteúdo      5,015\n",
            " 11 da              180\n",
            " 12 prova         2,310\n",
            " 13 ?               136\n",
            " 14 Bom           8,399\n",
            " 15 Dia           3,616\n",
            " 16 ,               117\n",
            " 17 professor     2,917\n",
            " 18 .               119\n",
            " 19 Vai          20,805\n",
            " 20 cair          9,322\n",
            " 21 tudo          2,745\n",
            " 22 na              229\n",
            " 23 prova         2,310\n",
            " 24 ?               136\n",
            " 25 [SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5Snv8-ACy47",
        "outputId": "28ba670d-9189-4dcc-e12c-28339d349c52"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à frase \"1\".\n",
        "mascara_atencao_permutado = [1] * len(documento_tokenizado_permutado)\n",
        "\n",
        "print (mascara_atencao_permutado)\n",
        "print (len(mascara_atencao_permutado))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMFR5tSiCy48"
      },
      "source": [
        "# Importa a bibliteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_permutado = torch.as_tensor([documento_tokens_indexados_permutado])\n",
        "mascara_atencao_tensores_permutado = torch.as_tensor([mascara_atencao_permutado])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je2zyykXCy49"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_permutado, mascara_atencao_tensores_permutado)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z09FmGtnCy49",
        "outputId": "b94e6de9-5844-42fd-e4e3-ebdc3fab8616"
      },
      "source": [
        "# Recupera a última e única camada da saída\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "print (\"O vetor da última camada oculta tem o formato:\", last_hidden_states.size())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor da última camada oculta tem o formato: torch.Size([1, 26, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkk3Ix9kC93C",
        "outputId": "171e74ea-86b4-4f6a-950b-179b3c283b0d"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "token_embeddings_permutado = torch.squeeze(last_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento permutado tem o formato:\", token_embeddings_permutado.size())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O vetor de tokens de embedding do documento permutado tem o formato: torch.Size([26, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIsMSKxNIUg9"
      },
      "source": [
        "Exibe os embenddings das frases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkHr7wEFIUhA",
        "outputId": "7f74fd44-95d3-4ee2-ed1b-8e07ef9814d6"
      },
      "source": [
        "# Índice das frases a serem comparadas\n",
        "frase1Permutado = documento_permutado[0]\n",
        "frase2Permutado = documento_permutado[1]\n",
        "frase3Permutado = documento_permutado[2]\n",
        "frase4Permutado = documento_permutado[3]\n",
        "\n",
        "embeddingFrase1Permutado = getEmbeddingFraseDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, frase1Permutado, tokenizer)\n",
        "embeddingFrase2Permutado = getEmbeddingFraseDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, frase2Permutado, tokenizer)\n",
        "embeddingFrase3Permutado = getEmbeddingFraseDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, frase3Permutado, tokenizer)\n",
        "embeddingFrase4Permutado = getEmbeddingFraseDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, frase4Permutado, tokenizer)\n",
        "\n",
        "print('Os primeiros 4 valores de cada frase do documento permutado.')\n",
        "\n",
        "print('\\nFrase 1:', frase1Permutado,'-', str(embeddingFrase1Permutado[:4]))\n",
        "print('Soma embedding Frase1:', frase1Original,'-', str(torch.sum(embeddingFrase1Original[:4])))\n",
        "\n",
        "print('\\nFrase 2:', frase2Permutado,'-', str(embeddingFrase2Permutado[:4]))\n",
        "print('Soma embedding Frase2:', frase2Permutado,'-', str(torch.sum(embeddingFrase2Permutado[:4])))\n",
        "\n",
        "print('\\nFrase 3:', frase3Permutado,'-', str(embeddingFrase3Permutado[:4]))\n",
        "print('Soma embedding Frase3:', frase3Permutado,'-', str(torch.sum(embeddingFrase3Original[:4])))\n",
        "\n",
        "print('\\nFrase 4:', frase4Permutado,'-', str(embeddingFrase4Permutado[:4]))\n",
        "print('Soma embedding Frase4:', frase4Permutado,'-', str(torch.sum(embeddingFrase4Permutado[:4])))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Os primeiros 4 valores de cada frase do documento permutado.\n",
            "\n",
            "Frase 1: Aguardo uma resposta, João. - tensor([[ 0.4152, -0.7838, -0.0364,  ...,  0.2686, -1.3504,  0.4707],\n",
            "        [-0.6012, -1.1043, -0.2660,  ..., -0.1943, -0.8112,  0.2180],\n",
            "        [ 1.2319, -1.1194,  0.9353,  ...,  0.2953, -0.1973,  0.5069],\n",
            "        [ 0.8861, -0.9666,  0.1831,  ...,  0.4595, -1.1045,  0.5615]])\n",
            "Soma embedding Frase1: Bom Dia, professor. - tensor(-113.4554)\n",
            "\n",
            "Frase 2: Qual o conteúdo da prova? - tensor([[-0.2957, -1.0942, -0.0024,  ...,  0.5875, -0.7164,  0.0104],\n",
            "        [ 0.2193, -0.4914,  0.7233,  ...,  0.3551, -0.3698,  0.8577],\n",
            "        [ 0.1377,  0.3856,  0.6060,  ...,  1.2662,  0.4965,  0.5211],\n",
            "        [ 0.5271, -0.4796,  0.4135,  ..., -0.5303, -0.3934,  0.1571]])\n",
            "Soma embedding Frase2: Qual o conteúdo da prova? - tensor(-115.7949)\n",
            "\n",
            "Frase 3: Bom Dia, professor. - tensor([[-0.2387, -0.2844,  0.3606,  ...,  0.6976, -0.9028,  0.3216],\n",
            "        [ 0.8188, -0.5643,  0.6965,  ..., -0.1439, -0.7018, -0.1933],\n",
            "        [ 0.9349, -1.0496,  0.1645,  ...,  0.4782, -0.3837, -0.2850],\n",
            "        [ 0.0104, -0.4793,  0.0501,  ...,  0.7341, -0.3687,  0.4562]])\n",
            "Soma embedding Frase3: Bom Dia, professor. - tensor(-110.5299)\n",
            "\n",
            "Frase 4: Vai cair tudo na prova? - tensor([[ 7.8763e-01, -1.1689e-01,  7.6317e-01,  ..., -4.3840e-01,\n",
            "         -6.5442e-01, -6.2305e-02],\n",
            "        [ 2.1062e-01,  1.0353e+00,  2.5936e-02,  ...,  7.1820e-02,\n",
            "          1.2368e-01,  1.1588e-03],\n",
            "        [ 7.5943e-01,  7.9969e-01,  2.4871e-01,  ...,  1.8792e-01,\n",
            "          9.4662e-02, -9.1650e-02],\n",
            "        [ 6.7000e-01,  1.3208e+00, -5.6431e-01,  ..., -6.3588e-03,\n",
            "          1.6016e-01, -1.4379e-01]])\n",
            "Soma embedding Frase4: Vai cair tudo na prova? - tensor(-110.7604)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MINDqF2LDA9z",
        "outputId": "a9f0b09d-999a-4cdf-c54c-0ddd95551ade"
      },
      "source": [
        "# Índice das frases a serem comparadas\n",
        "frase1Permutado = documento_permutado[0]\n",
        "frase2Permutado = documento_permutado[1]\n",
        "frase3Permutado = documento_permutado[2]\n",
        "frase4Permutado = documento_permutado[3]\n",
        "\n",
        "print(\"Documento Permutado:\", documento_permutado)\n",
        "\n",
        "# Localiza os índices dos tokens da frase no documento\n",
        "frase1TokenizadaPermutado = tokenizer.tokenize(frase1Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,frase1TokenizadaPermutado)\n",
        "embeddingFrase1Permutado = getEmbeddingFraseDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, frase1Permutado, tokenizer)\n",
        "print('\\nFrase 1 Permutada=\\'', frase1Permutado, '\\'')\n",
        "print('    Frase tokenizada:', frase1TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingFrase1Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingFrase1Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da frase no documento\n",
        "frase2TokenizadaPermutado = tokenizer.tokenize(frase2Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,frase2TokenizadaPermutado)\n",
        "embeddingFrase2Permutado = getEmbeddingFraseDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, frase2Permutado, tokenizer)\n",
        "print('\\nFrase 2 Permutada=\\'', frase2Permutado, '\\'')\n",
        "print('    Frase tokenizada:', frase2TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingFrase2Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingFrase2Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da frase no documento\n",
        "frase3TokenizadaPermutado = tokenizer.tokenize(frase3Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,frase3TokenizadaPermutado)\n",
        "embeddingFrase3Permutado = getEmbeddingFraseDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, frase3Permutado, tokenizer)\n",
        "print('\\nFrase 3 Permutada=\\'', frase3Permutado, '\\'')\n",
        "print('    Frase tokenizada:', frase3TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingFrase3Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingFrase3Permutado))\n",
        "\n",
        "# Localiza os índices dos tokens da frase no documento\n",
        "frase4TokenizadaPermutado = tokenizer.tokenize(frase4Permutado)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_permutado,frase4TokenizadaPermutado)\n",
        "embeddingFrase4Permutado = getEmbeddingFraseDocumentoEmbedding(token_embeddings_permutado, documento_texto_permutado, frase4Permutado, tokenizer)\n",
        "print('\\nFrase 4 Permutada=\\'', frase4Permutado, '\\'')\n",
        "print('    Frase tokenizada:', frase4TokenizadaPermutado)\n",
        "print('    => inicio em', inicio , 'e término em', fim)\n",
        "print('    Formato modelo :', embeddingFrase4Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingFrase4Permutado))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documento Permutado: ['Aguardo uma resposta, João.', 'Qual o conteúdo da prova?', 'Bom Dia, professor.', 'Vai cair tudo na prova?']\n",
            "\n",
            "Frase 1 Permutada=' Aguardo uma resposta, João. '\n",
            "    Frase tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 1 e término em 7\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -199.90\n",
            "\n",
            "Frase 2 Permutada=' Qual o conteúdo da prova? '\n",
            "    Frase tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 8 e término em 13\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -173.06\n",
            "\n",
            "Frase 3 Permutada=' Bom Dia, professor. '\n",
            "    Frase tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 14 e término em 18\n",
            "    Formato modelo : torch.Size([5, 1024])\n",
            "    Soma embeddings:  -141.81\n",
            "\n",
            "Frase 4 Permutada=' Vai cair tudo na prova? '\n",
            "    Frase tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 19 e término em 24\n",
            "    Formato modelo : torch.Size([6, 1024])\n",
            "    Soma embeddings:  -167.47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UILLnj7KvHi"
      },
      "source": [
        "### Comparando as frases\n",
        "\n",
        "A mesma frase apresenta embeddings com valores diferentes, pois se encontram em locais diferentes do documento. A soma de todos os embeddings demonstra isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eyEbV-7Kz6r",
        "outputId": "2bb6db72-40da-4cd2-de85-913216557876"
      },
      "source": [
        "print('\\nFrase 4 Original=\\'', frase4Original, '\\'')\n",
        "print('    Frase tokenizada:', frase4TokenizadaOriginal)\n",
        "print('    Formato modelo :', embeddingFrase4Original.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingFrase4Original))\n",
        "print('    Os 4 primeiros embeddings:', str(embeddingFrase4Original[:4]))\n",
        "\n",
        "print('\\nFrase 1 Permutada=\\'', frase1Permutado, '\\'')\n",
        "print('    Frase tokenizada:', frase1TokenizadaPermutado)\n",
        "print('    Formato modelo :', embeddingFrase1Permutado.shape)\n",
        "print('    Soma embeddings:  %.2f' % torch.sum(embeddingFrase1Permutado))\n",
        "print('    Os 4 primeiros embeddings:', str(embeddingFrase1Permutado[:4]))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Frase 4 Original=' Aguardo uma resposta, João. '\n",
            "    Frase tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -200.72\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.4339, -0.7168,  0.1173,  ...,  0.0768, -1.2036,  0.3863],\n",
            "        [-0.6806, -1.0194, -0.0765,  ..., -0.1768, -0.6326,  0.2895],\n",
            "        [ 1.1384, -0.8541,  0.8992,  ...,  0.2798, -0.0381,  0.5061],\n",
            "        [ 0.9206, -0.9862,  0.1347,  ...,  0.4182, -1.0164,  0.4713]])\n",
            "\n",
            "Frase 1 Permutada=' Aguardo uma resposta, João. '\n",
            "    Frase tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    Formato modelo : torch.Size([7, 1024])\n",
            "    Soma embeddings:  -199.90\n",
            "    Os 4 primeiros embeddings: tensor([[ 0.4152, -0.7838, -0.0364,  ...,  0.2686, -1.3504,  0.4707],\n",
            "        [-0.6012, -1.1043, -0.2660,  ..., -0.1943, -0.8112,  0.2180],\n",
            "        [ 1.2319, -1.1194,  0.9353,  ...,  0.2953, -0.1973,  0.5069],\n",
            "        [ 0.8861, -0.9666,  0.1831,  ...,  0.4595, -1.1045,  0.5615]])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}