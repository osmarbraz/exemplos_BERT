{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExemplosPrevisaoProximaPalavraBERT_pt_br.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osmarbraz/exemplos_BERT/blob/main/ExemplosPrevisaoProximaPalavraBERT_pt_br.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78HE8FLsKN9Q"
      },
      "source": [
        "#Exemplo de Previsão da Próxima Palavra(pt-br) usando BERT Transformers by HuggingFace\n",
        "\n",
        "## **A execução pode ser feita através do menu Ambiente de Execução opção Executar tudo.**\n",
        "\n",
        "Exemplos de **Previsão da Próxima Palavra(pt-br)** usando **BERT** em uma sentença pelo mascaramento(\"[MASK]\") de palavras.\n",
        "\n",
        "**Link biblioteca Huggingface:**\n",
        "https://github.com/huggingface/transformers\n",
        "\n",
        "\n",
        "**Artigo original BERT Jacob Devlin:**\n",
        "https://arxiv.org/pdf/1506.06724.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyxb5Px3p1-e"
      },
      "source": [
        "## 0 - Preparação do ambiente\n",
        "Preparação do ambiente para execução do exemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAPVtRXQqDim"
      },
      "source": [
        "###Tratamento de logs\n",
        "\n",
        "Método para tratamento dos logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcopxbGZqDip"
      },
      "source": [
        "# Biblioteca de logging\n",
        "import logging\n",
        "\n",
        "# Formatando a mensagem de logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GjYtXcMnSAe"
      },
      "source": [
        "### Identificando o ambiente Colab\n",
        "\n",
        "Cria uma variável para identificar que o notebook está sendo executado no Google Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMiH0E3OnRa1"
      },
      "source": [
        "# Se estiver executando no Google Colaboratory\n",
        "import sys\n",
        "\n",
        "# Retorna true ou false se estiver no Google Colaboratory\n",
        "IN_COLAB = 'google.colab' in sys.modules"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RufkKnojlwzu"
      },
      "source": [
        "### Instalação do spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0LeiOTx0Dlk"
      },
      "source": [
        "https://spacy.io/\n",
        "\n",
        "Modelos do spaCy para português:\n",
        "https://spacy.io/models/pt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2Fvx0TVRQUw",
        "outputId": "087f1840-31d5-4dfe-d4db-0bc351f6c310"
      },
      "source": [
        "# Instala o spacy\n",
        "!pip install -U spacy==2.3.5"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy==2.3.5\n",
            "  Downloading spacy-2.3.5-cp37-cp37m-manylinux2014_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (2.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (4.62.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (3.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.0.6)\n",
            "Collecting thinc<7.5.0,>=7.4.1\n",
            "  Downloading thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 27.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (3.10.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2.10)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed spacy-2.3.5 thinc-7.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35GwcgkOlWi3"
      },
      "source": [
        "Realiza o download e carrega os modelos necessários a biblioteca\n",
        "\n",
        "https://spacy.io/models/pt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4LqE5kTwDYm"
      },
      "source": [
        "# Definição do nome do arquivo do modelo\n",
        "#ARQUIVOMODELO = 'pt_core_news_sm'\n",
        "#ARQUIVOMODELO = 'pt_core_news_md'\n",
        "ARQUIVOMODELO = 'pt_core_news_lg'\n",
        "\n",
        "# Definição da versão da spaCy\n",
        "#VERSAOSPACY = '-3.0.0a0'\n",
        "VERSAOSPACY = '-2.3.0'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ2KB3UCp-ws"
      },
      "source": [
        "#Baixa automaticamente o arquivo do modelo.\n",
        "#!python -m spacy download {ARQUIVOMODELO}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASk5iFeUp9LE",
        "outputId": "3a4fd052-9071-4d40-b88f-8674a21ba973"
      },
      "source": [
        "# Realiza o download do arquivo do modelo para o diretório corrente\n",
        "!wget https://github.com/explosion/spacy-models/releases/download/{ARQUIVOMODELO}{VERSAOSPACY}/{ARQUIVOMODELO}{VERSAOSPACY}.tar.gz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-10 20:27:15--  https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-2.3.0/pt_core_news_lg-2.3.0.tar.gz\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/84940268/a899e480-ab07-11ea-831b-b5aa9cc04510?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211210%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211210T202716Z&X-Amz-Expires=300&X-Amz-Signature=e30e05fad651619a09876f78151e2063d8a1458213339ff27eba39eb401d6d40&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Dpt_core_news_lg-2.3.0.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-12-10 20:27:16--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/84940268/a899e480-ab07-11ea-831b-b5aa9cc04510?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211210%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211210T202716Z&X-Amz-Expires=300&X-Amz-Signature=e30e05fad651619a09876f78151e2063d8a1458213339ff27eba39eb401d6d40&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Dpt_core_news_lg-2.3.0.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 576599832 (550M) [application/octet-stream]\n",
            "Saving to: ‘pt_core_news_lg-2.3.0.tar.gz’\n",
            "\n",
            "pt_core_news_lg-2.3 100%[===================>] 549.89M  30.9MB/s    in 23s     \n",
            "\n",
            "2021-12-10 20:27:40 (23.5 MB/s) - ‘pt_core_news_lg-2.3.0.tar.gz’ saved [576599832/576599832]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu_LkF7Nfm8_"
      },
      "source": [
        "Descompacta o arquivo do modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9fCQQJGeVEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02926a51-1e24-48ce-9e88-24eaf5a79ada"
      },
      "source": [
        "# Descompacta o arquivo do modelo\n",
        "!tar -xvf  /content/{ARQUIVOMODELO}{VERSAOSPACY}.tar.gz"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pt_core_news_lg-2.3.0/\n",
            "pt_core_news_lg-2.3.0/PKG-INFO\n",
            "pt_core_news_lg-2.3.0/setup.py\n",
            "pt_core_news_lg-2.3.0/setup.cfg\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/dependency_links.txt\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/PKG-INFO\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/SOURCES.txt\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/requires.txt\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/top_level.txt\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/not-zip-safe\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/__init__.py\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/parser/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/parser/cfg\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/parser/moves\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/parser/model\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/ner/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/ner/cfg\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/ner/moves\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/ner/model\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tokenizer\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/lookups.bin\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/vectors\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/key2row\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/lookups_extra.bin\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/strings.json\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/accuracy.json\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tagger/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tagger/cfg\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tagger/tag_map\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tagger/model\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/meta.json\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/meta.json\n",
            "pt_core_news_lg-2.3.0/MANIFEST.in\n",
            "pt_core_news_lg-2.3.0/meta.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovOx-3Wb-JJW"
      },
      "source": [
        "# Coloca a pasta do modelo descompactado em uma pasta de nome mais simples\n",
        "!mv /content/{ARQUIVOMODELO}{VERSAOSPACY}/{ARQUIVOMODELO}/{ARQUIVOMODELO}{VERSAOSPACY} /content/{ARQUIVOMODELO}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STHT2c89qvwK"
      },
      "source": [
        "Carrega o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbELnrpgA4T1"
      },
      "source": [
        "# Importando as bibliotecas\n",
        "import spacy\n",
        "\n",
        "CAMINHOMODELO = \"/content/\" + ARQUIVOMODELO\n",
        "\n",
        "#nlp = spacy.load(CAMINHOMODELO)\n",
        "# Necessário 'tagger' para encontrar os substantivos\n",
        "nlp = spacy.load(CAMINHOMODELO, disable=['tokenizer', 'lemmatizer', 'ner', 'parser', 'textcat', 'custom'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFTTdqxKQ1Ay"
      },
      "source": [
        "Recupera os stopwords do spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBInu7ayQ31J"
      },
      "source": [
        "# Recupera as stop words\n",
        "spacy_stopwords = nlp.Defaults.stop_words"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_EYNu-_RX7k"
      },
      "source": [
        "Lista dos stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUSaUJEWRbnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd6cc0a-b96a-4174-feef-e45f51604e3c"
      },
      "source": [
        "print(\"Quantidade de stopwords:\", len(spacy_stopwords))\n",
        "\n",
        "print(spacy_stopwords)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de stopwords: 413\n",
            "{'conselho', 'eu', 'bem', 'vezes', 'área', 'ela', 'quanto', 'meu', 'ver', 'tem', 'breve', 'forma', 'boa', 'estou', 'umas', 'último', 'com', 'duas', 'tente', 'estado', 'algumas', 'sexto', 'vários', 'terceiro', 'cento', 'parte', 'tudo', 'porquanto', 'essa', 'às', 'disso', 'as', 'vão', 'saber', 'entre', 'podia', 'pôde', 'cinco', 'maiorias', 'ponto', 'fazem', 'desde', 'tenho', 'logo', 'lá', 'deste', 'por', 'isso', 'talvez', 'lugar', 'tal', 'fará', 'ambas', 'primeiro', 'antes', 'já', 'cá', 'nas', 'sempre', 'demais', 'desse', 'tuas', 'tempo', 'novo', 'podem', 'quieto', 'vem', 'dizer', 'sem', 'essas', 'todos', 'tão', 'bastante', 'ir', 'um', 'até', 'quinta', 'dezanove', 'na', 'depois', 'está', 'naquele', 'aqueles', 'temos', 'vossos', 'fazemos', 'perto', 'tentaram', 'bom', 'vindo', 'teu', 'ali', 'sobre', 'enquanto', 'aqui', 'põem', 'grande', 'nuns', 'oitava', 'fazia', 'uns', 'ele', 'sou', 'sabe', 'primeira', 'tua', 'nessa', 'tanta', 'sim', 'é', 'isto', 'inicio', 'inclusive', 'pouco', 'treze', 'também', 'minha', 'seus', 'teve', 'mil', 'você', 'dar', 'porquê', 'vêm', 'apenas', 'todo', 'ora', 'quarta', 'estás', 'quais', 'fim', 'à', 'nesse', 'como', 'catorze', 'suas', 'querem', 'meio', 'fomos', 'conhecido', 'têm', 'sexta', 'vais', 'dá', 'pelo', 'puderam', 'nesta', 'que', 'tanto', 'tiveste', 'pelas', 'final', 'pois', 'põe', 'tipo', 'oitavo', 'novas', 'da', 'posição', 'nos', 'alguns', 'aquele', 'doze', 'lado', 'usar', 'te', 'somente', 'das', 'corrente', 'seis', 'povo', 'veja', 'quarto', 'cima', 'faço', 'favor', 'através', 'agora', 'porque', 'dão', 'estivestes', 'toda', 'quem', 'aquilo', 'naquela', 'quatro', 'mal', 'fazeis', 'este', 'dezoito', 'aquelas', 'nem', 'pouca', 'vez', 'estes', 'daquele', 'usa', 'dentro', 'local', 'grupo', 'nada', 'ter', 'pelos', 'quando', 'possível', 'portanto', 'geral', 'nunca', 'baixo', 'obrigada', 'certeza', 'adeus', 'tarde', 'ademais', 'segundo', 'poderá', 'eventual', 'para', 'sétimo', 'esta', 'nosso', 'seria', 'estão', 'apoia', 'diante', 'menos', 'os', 'de', 'vinte', 'tendes', 'diz', 'estará', 'esses', 'estivemos', 'muito', 'tais', 'apoio', 'dez', 'exemplo', 'cuja', 'estava', 'neste', 'números', 'iniciar', 'quinze', 'acerca', 'mês', 'dizem', 'terceira', 'outra', 'faz', 'dezasseis', 'meus', 'nove', 'cada', 'outros', 'nossos', 'zero', 'sua', 'atrás', 'comprida', 'irá', 'tentei', 'foram', 'contra', 'des', 'quer', 'então', 'meses', 'onze', 'és', 'num', 'estive', 'possivelmente', 'contudo', 'sete', 'tive', 'nenhuma', 'desta', 'esse', 'nossas', 'após', 'se', 'pode', 'quero', 'vosso', 'outras', 'fez', 'porém', 'dois', 'mas', 'sistema', 'vocês', 'número', 'tivestes', 'longe', 'vens', 'mesmo', 'ambos', 'conhecida', 'aquela', 'no', 'questão', 'próxima', 'seu', 'devem', 'sétima', 'ser', 'posso', 'quinto', 'máximo', 'segunda', 'cujo', 'estiveste', 'era', 'teus', 'nossa', 'obrigado', 'estiveram', 'for', 'maior', 'maioria', 'mais', 'do', 'parece', 'valor', 'vos', 'falta', 'apontar', 'direita', 'tens', 'embora', 'próprio', 'certamente', 'deve', 'caminho', 'minhas', 'foi', 'só', 'aos', 'menor', 'sois', 'onde', 'custa', 'não', 'oito', 'fazes', 'partir', 'dessa', 'cedo', 'três', 'ou', 'quê', 'novos', 'fazer', 'foste', 'lhe', 'deverá', 'uma', 'vossas', 'pegar', 'coisa', 'esteve', 'momento', 'fostes', 'comprido', 'ontem', 'vós', 'pela', 'qual', 'em', 'próximo', 'dos', 'tu', 'grandes', 'algo', 'somos', 'assim', 'ligado', 'fui', 'nível', 'nós', 'daquela', 'elas', 'sei', 'tivemos', 'todas', 'vinda', 'fora', 'debaixo', 'vai', 'relação', 'estas', 'ao', 'tentar', 'tiveram', 'pontos', 'sob', 'qualquer', 'estar', 'vossa', 'numa', 'ainda', 'eles', 'aí', 'muitos', 'são', 'me', 'poder', 'quieta', 'além', 'dezassete', 'nova'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqa-7WXBAw8q"
      },
      "source": [
        "### Instalação do BERT da Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCdqJCtQN52l"
      },
      "source": [
        "Instala a interface pytorch para o BERT by Hugging Face. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RfUN_KolV-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b1effb2-b874-430e-acad-434975daec9c"
      },
      "source": [
        "# Instala a última versão da biblioteca\n",
        "#!pip install transformers\n",
        "\n",
        "# Instala uma versão específica da biblioteca\n",
        "!pip install -U transformers==4.5.1"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.5.1\n",
            "  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |▎                               | 20 kB 24.6 MB/s eta 0:00:01\r\u001b[K     |▌                               | 30 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |▋                               | 40 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |▉                               | 51 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█                               | 61 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 71 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 81 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 92 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 102 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 112 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 122 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 133 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 143 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 153 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 163 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 174 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 184 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 194 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 204 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 215 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 225 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 235 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 245 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 256 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 266 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 276 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 286 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 296 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 307 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 317 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 327 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 337 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 348 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 358 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 368 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 378 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 389 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 399 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 409 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 419 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 430 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 440 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 450 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 460 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 471 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 481 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 491 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 501 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 512 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 522 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 532 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 542 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 552 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 563 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 573 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 583 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 593 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 604 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 614 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 624 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 634 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 645 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 655 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 665 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 675 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 686 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 696 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 706 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 716 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 727 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 737 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 747 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 757 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 768 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 778 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 788 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 798 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 808 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 819 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 829 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 839 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 849 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 860 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 870 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 880 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 890 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 901 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 911 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 921 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 931 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 942 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 952 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 962 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 972 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 983 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 993 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.6 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.6 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.6 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.6 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.6 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.6 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.6 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.6 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.6 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.6 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.7 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.7 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.7 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.7 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.7 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.7 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.7 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.8 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.8 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.8 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.8 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.8 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.8 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.8 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.8 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.8 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.8 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.9 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.9 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.9 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.9 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.9 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.9 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.9 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.9 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.1 MB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 23.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 58.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2019.12.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.1) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.1) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.1) (3.0.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQj2wmKDpkrH"
      },
      "source": [
        "## 1 - Download do arquivo do PyTorch Checkpoint\n",
        "\n",
        "Lista de modelos da comunidade:\n",
        "* https://huggingface.co/models\n",
        "\n",
        "Português(https://github.com/neuralmind-ai/portuguese-bert):  \n",
        "* **'neuralmind/bert-base-portuguese-cased'**\n",
        "* **'neuralmind/bert-large-portuguese-cased'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajrTjZzapkrK",
        "outputId": "5dca7b39-8742-457b-a083-5f528b853ea4"
      },
      "source": [
        "# Importando as bibliotecas\n",
        "import os\n",
        "\n",
        "# Variável para setar o arquivo\n",
        "URL_MODELO = None\n",
        "\n",
        "# Comente uma das urls para carregar modelos de tamanhos diferentes(base/large)\n",
        "# URL_MODELO do arquivo do modelo tensorflow\n",
        "# arquivo menor(base) 1.1 Gbytes\n",
        "#URL_MODELO = 'https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip'\n",
        "\n",
        "# arquivo grande(large) 3.5 Gbytes\n",
        "URL_MODELO = 'https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-large-portuguese-cased/bert-large-portuguese-cased_pytorch_checkpoint.zip'\n",
        "\n",
        "# Se a variável foi setada\n",
        "if URL_MODELO:\n",
        "\n",
        "    # Diretório descompactação\n",
        "    DIRETORIO_MODELO = '/content/modelo'\n",
        "\n",
        "    # Recupera o nome do arquivo do modelo da URL_MODELO\n",
        "    arquivo = URL_MODELO.split('/')[-1]\n",
        "\n",
        "    # Nome do arquivo do vocabulário\n",
        "    arquivo_vocab = 'vocab.txt'\n",
        "\n",
        "    # Caminho do arquivo na URL_MODELO\n",
        "    caminho = URL_MODELO[0:len(URL_MODELO)-len(arquivo)]\n",
        "\n",
        "    # Verifica se a pasta de descompactação existe na pasta corrente\n",
        "    if os.path.exists(DIRETORIO_MODELO):\n",
        "      print('Apagando diretório existente do modelo!')\n",
        "      # Apaga a pasta e os arquivos existentes\n",
        "      !rm -rf $DIRETORIO_MODELO      \n",
        "    \n",
        "    # Baixa o arquivo do modelo\n",
        "    !wget $URL_MODELO\n",
        "    # Descompacta o arquivo na pasta de descompactação\n",
        "    !unzip -o $arquivo -d $DIRETORIO_MODELO\n",
        "\n",
        "    # Baixa o arquivo do vocabulário\n",
        "    # O vocabulário não está no arquivo compactado acima, mesma url mas arquivo diferente\n",
        "    URL_MODELO_VOCAB = caminho + arquivo_vocab\n",
        "    !wget $URL_MODELO_VOCAB\n",
        "    \n",
        "    # Coloca o arquivo do vocabulário no diretório de descompactação\n",
        "    !mv $arquivo_vocab $DIRETORIO_MODELO\n",
        "            \n",
        "    # Move o arquivo para pasta de descompactação\n",
        "    !mv $arquivo $DIRETORIO_MODELO\n",
        "       \n",
        "    print('Pasta do ' + DIRETORIO_MODELO + ' pronta!')\n",
        "    \n",
        "    # Lista a pasta corrente\n",
        "    !ls -la $DIRETORIO_MODELO\n",
        "else:\n",
        "    print('Variável URL_MODELO não setada!')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-10 20:27:59--  https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-large-portuguese-cased/bert-large-portuguese-cased_pytorch_checkpoint.zip\n",
            "Resolving neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)... 52.219.176.170\n",
            "Connecting to neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)|52.219.176.170|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1244275810 (1.2G) [application/zip]\n",
            "Saving to: ‘bert-large-portuguese-cased_pytorch_checkpoint.zip’\n",
            "\n",
            "bert-large-portugue 100%[===================>]   1.16G  45.3MB/s    in 27s     \n",
            "\n",
            "2021-12-10 20:28:27 (44.0 MB/s) - ‘bert-large-portuguese-cased_pytorch_checkpoint.zip’ saved [1244275810/1244275810]\n",
            "\n",
            "Archive:  bert-large-portuguese-cased_pytorch_checkpoint.zip\n",
            "  inflating: /content/modelo/config.json  \n",
            "  inflating: /content/modelo/pytorch_model.bin  \n",
            "--2021-12-10 20:28:42--  https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-large-portuguese-cased/vocab.txt\n",
            "Resolving neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)... 52.219.97.210\n",
            "Connecting to neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)|52.219.97.210|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 209528 (205K) [text/plain]\n",
            "Saving to: ‘vocab.txt’\n",
            "\n",
            "vocab.txt           100%[===================>] 204.62K   799KB/s    in 0.3s    \n",
            "\n",
            "2021-12-10 20:28:43 (799 KB/s) - ‘vocab.txt’ saved [209528/209528]\n",
            "\n",
            "Pasta do /content/modelo pronta!\n",
            "total 2525908\n",
            "drwxr-xr-x 2 root root       4096 Dec 10 20:28 .\n",
            "drwxr-xr-x 1 root root       4096 Dec 10 20:28 ..\n",
            "-rw-r--r-- 1 root root 1244275810 Jan 22  2020 bert-large-portuguese-cased_pytorch_checkpoint.zip\n",
            "-rw-rw-r-- 1 root root        874 Jan 12  2020 config.json\n",
            "-rw-rw-r-- 1 root root 1342014951 Jan 12  2020 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root     209528 Jan 21  2020 vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcpd9t9PpkrX"
      },
      "source": [
        "## 2 - Carregando o Tokenizador BERT\n",
        "\n",
        "O tokenizador utiliza WordPiece, veja em [artigo original](https://arxiv.org/pdf/1609.08144.pdf).\n",
        "\n",
        "Carregando o tokenizador da pasta '/content/modelo/' do diretório padrão se variável `URL_MODELO` setada.\n",
        "\n",
        "**Caso contrário carrega da comunidade**\n",
        "\n",
        "Por default(`do_lower_case=True`) todas as letras são colocadas para minúsculas. Para ignorar a conversão para minúsculo use o parâmetro `do_lower_case=False`. Esta opção também considera as letras acentuadas(ãçéí...), que são necessárias a língua portuguesa.\n",
        "\n",
        "O parâmetro `do_lower_case` interfere na quantidade tokens a ser gerado a partir de um texto. Quando igual a `False` reduz a quantidade de tokens gerados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8cKVs4fpkrY",
        "outputId": "69796970-7653-4d3b-850a-e2d879fc2cd5"
      },
      "source": [
        "# Importando as bibliotecas do tokenizador\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Se a variável URL_MODELO foi setada\n",
        "if DIRETORIO_MODELO:\n",
        "    # Carregando o Tokenizador\n",
        "    print('Carrgando o tokenizador BERT do diretório ' + DIRETORIO_MODELO + '...')\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(DIRETORIO_MODELO, \n",
        "                                              do_lower_case=False)    \n",
        "else:\n",
        "    # Carregando o Tokenizador da comunidade\n",
        "    print('Carregando o tokenizador da comunidade...')\n",
        "    \n",
        "    #tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)\n",
        "    tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased', do_lower_case=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carrgando o tokenizador BERT do diretório /content/modelo...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m__On2g1a--K"
      },
      "source": [
        "## 3 - Carregando o Modelo BERT(BertForMaskedLM)\n",
        "\n",
        "Se a variável `URL_MODELO` estiver setada carrega o modelo do diretório `content/modelo`.\n",
        "\n",
        "Caso contrário carrega da comunidade.\n",
        "\n",
        "Carregando o modelo da pasta '/content/modelo/' do diretório padrão.\n",
        "\n",
        "A implementação do huggingface pytorch inclui um conjunto de interfaces projetadas para uma variedade de tarefas de PNL. Embora essas interfaces sejam todas construídas sobre um modelo treinado de BERT, cada uma possui diferentes camadas superiores e tipos de saída projetados para acomodar suas tarefas específicas de PNL.\n",
        "\n",
        "A documentação para estas pode ser encontrada em [aqui](https://huggingface.co/transformers/model_doc/bert.html#bertformaskedlm).\n",
        "\n",
        "Por default o modelo está em modo avaliação ou seja `model.eval()`.\n",
        "\n",
        "-----------------------\n",
        "\n",
        "Durante a avaliação do modelo, este retorna um número de diferentes objetos com base em como é configurado na chamada do método `from_pretrained`. \n",
        "\n",
        "Quando definimos `output_hidden_states = True` na chamada do método `from_pretrained`, retorno do modelo possui no terceiro item os estados ocultos(**hidden_states**) de todas as camadas.  Veja a documentação para mais detalhes: https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "\n",
        "Quando **`output_hidden_states = True`** model retorna:\n",
        "- outputs[0] = last_hidden_state;\n",
        "- outputs[1] = pooler_output; \n",
        "- outputs[2] = hidden_states.\n",
        "\n",
        "Quando **`output_hidden_states = False`** ou não especificado model retorna:\n",
        "- outputs[0] = last_hidden_state;\n",
        "- outputs[1] = pooler_output.\n",
        "\n",
        "\n",
        "**ATENÇÃO**: O parâmetro ´**output_hidden_states = True**´ habilita gerar as camadas ocultas do modelo. Caso contrário somente a última camada é mantida. Este parâmetro otimiza a memória mas não os resultados.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRV6l_I-qg9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f90fb724-8bf8-487c-9adf-7ac914d8c65d"
      },
      "source": [
        "# Importando as bibliotecas do Modelo\n",
        "from transformers import BertForMaskedLM\n",
        "\n",
        "# Se a variável URL_MODELO1 foi setada\n",
        "if URL_MODELO:\n",
        "    # Carregando o modelo\n",
        "    print('Carregando o modelo BERT do diretório ' + DIRETORIO_MODELO + '...')\n",
        "\n",
        "    model = BertForMaskedLM.from_pretrained(DIRETORIO_MODELO, \n",
        "                                      output_attentions = False,\n",
        "                                      output_hidden_states = True)    \n",
        "else:\n",
        "    # Carregando o modelo da comunidade\n",
        "    print('Carregando o modelo BERT da comunidade ...')\n",
        "\n",
        "    model = BertForMaskedLM.from_pretrained('neuralmind/bert-large-portuguese-cased', \n",
        "                                      output_attentions = False,\n",
        "                                      output_hidden_states = True)## 5 - Funções auxiliares"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando o modelo BERT do diretório /content/modelo...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/modelo were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCQRC9fHAZQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb7a2711-af60-4971-8094-9aa90911ce86"
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(29794, 1024, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 1024)\n",
              "      (token_type_embeddings): Embedding(2, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (12): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (13): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (14): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (15): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (16): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (17): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (18): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (19): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (20): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (21): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (22): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (23): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=1024, out_features=29794, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CItBju3uAa8q"
      },
      "source": [
        "# model.to('cuda')  # Se tiver gpu"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKicQ_TH_WxK"
      },
      "source": [
        "## 4 - Funções auxiliares\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqYOs576NPQ"
      },
      "source": [
        "# Importando a biblioteca\n",
        "import torch\n",
        "\n",
        "def previsaoPalavraSentenca(texto, top_k=5):\n",
        "    '''\n",
        "    Retorna uma lista com as k previsões para a palavra mascarada no texto.\n",
        "    '''\n",
        "\n",
        "    # Adiciona os tokens especiais ao texto\n",
        "    texto_marcado = \"[CLS] \" + texto + \"[SEP]\"\n",
        "    #print(\"texto_marcado:\", texto_marcado)\n",
        "\n",
        "    # Divide as palavras em tokens\n",
        "    texto_tokenizado = tokenizer.tokenize(texto_marcado)    \n",
        "    #print(\"texto_tokenizado:\", texto_tokenizado)\n",
        "\n",
        "    # Retorna o índice da mascara de atenção\n",
        "    mascara_atencao_indice = texto_tokenizado.index(\"[MASK]\")\n",
        "    #print(\"mascara_atencao_indice:\", mascara_atencao_indice)\n",
        "\n",
        "    # Mapeia os tokens em seus índices do vocabulário\n",
        "    tokens_indexados = tokenizer.convert_tokens_to_ids(texto_tokenizado)\n",
        "    #print(\"tokens_indexados:\", tokens_indexados)\n",
        "    \n",
        "    # Converte as entradas de lista para tensores do torch\n",
        "    tokens_tensores = torch.tensor([tokens_indexados])\n",
        "    \n",
        "    # Realiza a predição dos tokens\n",
        "    with torch.no_grad():\n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = model(tokens_tensores)\n",
        "\n",
        "    # Recupera a predição com os embeddings da última camada oculta    \n",
        "    predicao = outputs[0]\n",
        "    \n",
        "    # Normaliza os pesos das predições nos embeddings e calcula sua probabilidade\n",
        "    probabilidades = torch.nn.functional.softmax(predicao[0, mascara_atencao_indice], dim=-1)    \n",
        "    # Retorna os k maiores elementos de determinado tensor de entrada ao longo de uma determinada dimensão de forma ordenada descrescentemente.\n",
        "    top_k_pesos, top_k_indices = torch.topk(probabilidades, top_k, sorted=True)\n",
        "\n",
        "    # Mostra as predições\n",
        "    print(\"Frase:\", texto )\n",
        "    for i, indicePredicao in enumerate(top_k_indices):\n",
        "        # Mapeia os índices do vocabulário para os seus tokens\n",
        "        token_previsto = tokenizer.convert_ids_to_tokens([indicePredicao])[0]\n",
        "        token_peso = top_k_pesos[i]\n",
        "\n",
        "        print((i+1), \"[MASK]: \", token_previsto, \" | peso:\", float(token_peso))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru02mC0Estsb"
      },
      "source": [
        "## 5 - Exemplo MLM previsão da próxima palavra utilizando BERT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZkpDPZF6K0x"
      },
      "source": [
        "### Exemplo 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwWHiSy91cji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14bf3b20-5031-46d8-dbad-14224d6195e2"
      },
      "source": [
        "sentenca = 'O carro bateu no [MASK].'\n",
        "\n",
        "previsaoPalavraSentenca(sentenca, top_k=10)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frase: O carro bateu no [MASK].\n",
            "1 [MASK]:  muro  | peso: 0.597743034362793\n",
            "2 [MASK]:  caminhão  | peso: 0.06205667182803154\n",
            "3 [MASK]:  chão  | peso: 0.0495670922100544\n",
            "4 [MASK]:  outro  | peso: 0.0391114167869091\n",
            "5 [MASK]:  solo  | peso: 0.034394290298223495\n",
            "6 [MASK]:  ônibus  | peso: 0.01980435661971569\n",
            "7 [MASK]:  portão  | peso: 0.013402305543422699\n",
            "8 [MASK]:  buraco  | peso: 0.010201003402471542\n",
            "9 [MASK]:  carro  | peso: 0.009121101349592209\n",
            "10 [MASK]:  rio  | peso: 0.00718745356425643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-dkydqHVBB4",
        "outputId": "9bf34c70-9fbb-4fbc-fa1c-2cbcaa153323"
      },
      "source": [
        "sentenca = 'O dia está [MASK].'\n",
        "\n",
        "previsaoPalavraSentenca(sentenca, top_k=10)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frase: O dia está [MASK].\n",
            "1 [MASK]:  claro  | peso: 0.18800124526023865\n",
            "2 [MASK]:  quente  | peso: 0.10005087405443192\n",
            "3 [MASK]:  chegando  | peso: 0.06581124663352966\n",
            "4 [MASK]:  escuro  | peso: 0.05243193358182907\n",
            "5 [MASK]:  terminando  | peso: 0.03215831145644188\n",
            "6 [MASK]:  bonito  | peso: 0.031797997653484344\n",
            "7 [MASK]:  frio  | peso: 0.029412493109703064\n",
            "8 [MASK]:  bom  | peso: 0.0276333000510931\n",
            "9 [MASK]:  longo  | peso: 0.022950707003474236\n",
            "10 [MASK]:  triste  | peso: 0.019353417679667473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IBIDVR9_iaB"
      },
      "source": [
        "### Exemplo 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kdo1kx3-C4D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9c7ea55-f462-411c-d9a8-d6c71bb3d034"
      },
      "source": [
        "sentenca = 'O que é uma pilha e como [MASK] seu elemento?'\n",
        "\n",
        "previsaoPalavraSentenca(sentenca, top_k=10)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frase: O que é uma pilha e como [MASK] seu elemento?\n",
            "1 [MASK]:  identificar  | peso: 0.2645019590854645\n",
            "2 [MASK]:  funciona  | peso: 0.11154681444168091\n",
            "3 [MASK]:  localizar  | peso: 0.04558868333697319\n",
            "4 [MASK]:  é  | peso: 0.04059113189578056\n",
            "5 [MASK]:  encontrar  | peso: 0.034488629549741745\n",
            "6 [MASK]:  classificar  | peso: 0.02985268458724022\n",
            "7 [MASK]:  calcular  | peso: 0.029209716245532036\n",
            "8 [MASK]:  medir  | peso: 0.02668553777039051\n",
            "9 [MASK]:  utilizar  | peso: 0.025590619072318077\n",
            "10 [MASK]:  usar  | peso: 0.021644597873091698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPDz-9aT_k9Q"
      },
      "source": [
        "### Exemplo 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-5TNrWZ-6Gg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44c10365-9a75-433a-9985-e58d6460a166"
      },
      "source": [
        "sentenca = 'O que é uma [MASK] e como enfileirar seu elemento?'\n",
        "\n",
        "previsaoPalavraSentenca(sentenca, top_k=10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frase: O que é uma [MASK] e como enfileirar seu elemento?\n",
            "1 [MASK]:  árvore  | peso: 0.1691964864730835\n",
            "2 [MASK]:  casa  | peso: 0.04466995969414711\n",
            "3 [MASK]:  pilha  | peso: 0.04050327092409134\n",
            "4 [MASK]:  planta  | peso: 0.038771532475948334\n",
            "5 [MASK]:  coleção  | peso: 0.03736716881394386\n",
            "6 [MASK]:  biblioteca  | peso: 0.0324641689658165\n",
            "7 [MASK]:  flor  | peso: 0.025039255619049072\n",
            "8 [MASK]:  mesa  | peso: 0.02046835422515869\n",
            "9 [MASK]:  fonte  | peso: 0.018971988931298256\n",
            "10 [MASK]:  caixa  | peso: 0.018774881958961487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GylSCDQIVO7m"
      },
      "source": [
        "### Exemplo 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXgHNxJvVKK-",
        "outputId": "877e4208-fbc9-4709-ca7f-30a37672900a"
      },
      "source": [
        "sentenca = 'O que é uma [MASK] e como empilhar seu elemento?'\n",
        "\n",
        "previsaoPalavraSentenca(sentenca, top_k=10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frase: O que é uma [MASK] e como empilhar seu elemento?\n",
            "1 [MASK]:  árvore  | peso: 0.13572701811790466\n",
            "2 [MASK]:  pilha  | peso: 0.10913962870836258\n",
            "3 [MASK]:  caixa  | peso: 0.035586193203926086\n",
            "4 [MASK]:  coleção  | peso: 0.03333856910467148\n",
            "5 [MASK]:  pirâmide  | peso: 0.02637934312224388\n",
            "6 [MASK]:  biblioteca  | peso: 0.0259123295545578\n",
            "7 [MASK]:  casa  | peso: 0.0231474582105875\n",
            "8 [MASK]:  estrutura  | peso: 0.022385787218809128\n",
            "9 [MASK]:  planta  | peso: 0.020019162446260452\n",
            "10 [MASK]:  mina  | peso: 0.017395537346601486\n"
          ]
        }
      ]
    }
  ]
}