{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExemplosContextoGlobalBERT_pt_br.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ae24d2d3869c44168703fdcf572d1b17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_931a2d7641264104824f6deb6d1e86ff",
              "IPY_MODEL_5535a4c4fe7a4c5a9b3476147051ccf2",
              "IPY_MODEL_17829a77c1cc4adcb26efa95641eba31"
            ],
            "layout": "IPY_MODEL_080262663ffd4b67bcacdb4b464e8713"
          }
        },
        "931a2d7641264104824f6deb6d1e86ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a56a5916fa64b21957059154dee4659",
            "placeholder": "​",
            "style": "IPY_MODEL_ae9146b94efd4ef8a09cb9731733bbdf",
            "value": "Downloading: 100%"
          }
        },
        "5535a4c4fe7a4c5a9b3476147051ccf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb78372760914a14b2988bff17edb89d",
            "max": 209528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a710dd1b77094798945a4b76b1f6f985",
            "value": 209528
          }
        },
        "17829a77c1cc4adcb26efa95641eba31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09f2cf1a31e8458e9108db3954453a06",
            "placeholder": "​",
            "style": "IPY_MODEL_e966b34ef716488990310dbe779a9ff8",
            "value": " 210k/210k [00:00&lt;00:00, 3.02MB/s]"
          }
        },
        "080262663ffd4b67bcacdb4b464e8713": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a56a5916fa64b21957059154dee4659": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae9146b94efd4ef8a09cb9731733bbdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb78372760914a14b2988bff17edb89d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a710dd1b77094798945a4b76b1f6f985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09f2cf1a31e8458e9108db3954453a06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e966b34ef716488990310dbe779a9ff8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b86afb78fe841a994f1440a9446a940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8fa3e3c0963d4888ac7329352a76d695",
              "IPY_MODEL_eb690a64a4104148a98cdd52cd01b906",
              "IPY_MODEL_c1b60c065fb242df879d95fa12e44a9b"
            ],
            "layout": "IPY_MODEL_ff8ecbcea62f43a1883454af1a5761b9"
          }
        },
        "8fa3e3c0963d4888ac7329352a76d695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6fbdacdf350433e83c321ec7f0952e3",
            "placeholder": "​",
            "style": "IPY_MODEL_40f3b9eb43b84189bfb746759e9f2382",
            "value": "Downloading: 100%"
          }
        },
        "eb690a64a4104148a98cdd52cd01b906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d282a191a364362bf018a61d9b145e9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80ad98c41a0f400c836a8b6c83d17630",
            "value": 2
          }
        },
        "c1b60c065fb242df879d95fa12e44a9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_019fb6e4500b4418960db01e6a6181d1",
            "placeholder": "​",
            "style": "IPY_MODEL_368a6590259940ee931420c6cf6f7db7",
            "value": " 2.00/2.00 [00:00&lt;00:00, 47.8B/s]"
          }
        },
        "ff8ecbcea62f43a1883454af1a5761b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6fbdacdf350433e83c321ec7f0952e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40f3b9eb43b84189bfb746759e9f2382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d282a191a364362bf018a61d9b145e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80ad98c41a0f400c836a8b6c83d17630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "019fb6e4500b4418960db01e6a6181d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "368a6590259940ee931420c6cf6f7db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0864470cc7f44fc081f7da0253255579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6e9bc6491574edf9bb2c33a9164a6e8",
              "IPY_MODEL_24da427783324e9e8b48cc3fac88f7d3",
              "IPY_MODEL_1c9dd242c33c44e696bfd8dab207d2bc"
            ],
            "layout": "IPY_MODEL_35202acd11834508ba93dd5034ff0326"
          }
        },
        "c6e9bc6491574edf9bb2c33a9164a6e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a9352d592734c70a21c784474ff7c4b",
            "placeholder": "​",
            "style": "IPY_MODEL_ab54fac1c6034f1e95e81f9eaa856c80",
            "value": "Downloading: 100%"
          }
        },
        "24da427783324e9e8b48cc3fac88f7d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_991d7db3b7e34e0380e10172d4c384cf",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a3a4d91df114f9e97d6f3fc4b0a8ec7",
            "value": 112
          }
        },
        "1c9dd242c33c44e696bfd8dab207d2bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22bce9c9c09c4dd9b8a7874796770f19",
            "placeholder": "​",
            "style": "IPY_MODEL_17b68795b28442abb411f3012e12f8d5",
            "value": " 112/112 [00:00&lt;00:00, 3.07kB/s]"
          }
        },
        "35202acd11834508ba93dd5034ff0326": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a9352d592734c70a21c784474ff7c4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab54fac1c6034f1e95e81f9eaa856c80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "991d7db3b7e34e0380e10172d4c384cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a3a4d91df114f9e97d6f3fc4b0a8ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22bce9c9c09c4dd9b8a7874796770f19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17b68795b28442abb411f3012e12f8d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba9ada1ba2b8465e95ef86255df87a24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_427cacc5b7ac4888a66f4e12b1a49536",
              "IPY_MODEL_eaf0149a6bf34726b71381c309fd251a",
              "IPY_MODEL_e95ff4099657488e9d827e9470e97ca1"
            ],
            "layout": "IPY_MODEL_e506c49760f94d0f853507de5007b149"
          }
        },
        "427cacc5b7ac4888a66f4e12b1a49536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c096a814d6e24b4eb2ffbcd70bb207d7",
            "placeholder": "​",
            "style": "IPY_MODEL_db45d9c02528458dbc8e0f4d7ec0b8ff",
            "value": "Downloading: 100%"
          }
        },
        "eaf0149a6bf34726b71381c309fd251a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b69b9801f0a4407d9bba9c7304099207",
            "max": 155,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4389100548734c9e9124fe3d4d0843bc",
            "value": 155
          }
        },
        "e95ff4099657488e9d827e9470e97ca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8d87ebededf4ac6ae54d5a7128d1b58",
            "placeholder": "​",
            "style": "IPY_MODEL_cef097ecb0d24215a6a03101df7fa044",
            "value": " 155/155 [00:00&lt;00:00, 3.42kB/s]"
          }
        },
        "e506c49760f94d0f853507de5007b149": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c096a814d6e24b4eb2ffbcd70bb207d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db45d9c02528458dbc8e0f4d7ec0b8ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b69b9801f0a4407d9bba9c7304099207": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4389100548734c9e9124fe3d4d0843bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8d87ebededf4ac6ae54d5a7128d1b58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cef097ecb0d24215a6a03101df7fa044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25fbb7b4ccf44f71b748ac23eb48b2a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a996cdb1c0c7481a86b619f71d4a3bfa",
              "IPY_MODEL_0453373635a24e93a7173ee89b24a211",
              "IPY_MODEL_51109da084d84c8886adfc1eca2a6b0f"
            ],
            "layout": "IPY_MODEL_f52a93dab2c647d68e64c63da70d3db2"
          }
        },
        "a996cdb1c0c7481a86b619f71d4a3bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7e9df119fc0452ea8ea3cc17a4b7dc5",
            "placeholder": "​",
            "style": "IPY_MODEL_70b5ec253e0f4e65a764926f709131a6",
            "value": "Downloading: 100%"
          }
        },
        "0453373635a24e93a7173ee89b24a211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_550e42f094ad4e50a08a2c0752b47b83",
            "max": 648,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_adcec6f71cc047a4a372f65ae254973f",
            "value": 648
          }
        },
        "51109da084d84c8886adfc1eca2a6b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f60f739e5852490e960a309b46ebd71a",
            "placeholder": "​",
            "style": "IPY_MODEL_924478c918644df69a1fd097dbd34b23",
            "value": " 648/648 [00:00&lt;00:00, 10.1kB/s]"
          }
        },
        "f52a93dab2c647d68e64c63da70d3db2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7e9df119fc0452ea8ea3cc17a4b7dc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70b5ec253e0f4e65a764926f709131a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "550e42f094ad4e50a08a2c0752b47b83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adcec6f71cc047a4a372f65ae254973f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f60f739e5852490e960a309b46ebd71a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "924478c918644df69a1fd097dbd34b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c98698b0aadb41ba92ee9c6ad7b2bdba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67b039ff66734717a825d7fbb3650075",
              "IPY_MODEL_cccd7d650d8343dbbec6c9b0c5be2bcf",
              "IPY_MODEL_b325f94bc9cd4b4e9e30a6c55e40f529"
            ],
            "layout": "IPY_MODEL_05041e24ad454fce81d7144993ed2531"
          }
        },
        "67b039ff66734717a825d7fbb3650075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f57650a366f74b378df3fc0e403f79bf",
            "placeholder": "​",
            "style": "IPY_MODEL_742720860f9d44dcb1d6f0401ad58c28",
            "value": "Downloading: 100%"
          }
        },
        "cccd7d650d8343dbbec6c9b0c5be2bcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c01a67c74b2a4cd4bee65edac09a0ded",
            "max": 1342014951,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55fe97ec28904d0e8af8d71ad2272099",
            "value": 1342014951
          }
        },
        "b325f94bc9cd4b4e9e30a6c55e40f529": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dd3a9cd60ba4700b7788d321b0a69e6",
            "placeholder": "​",
            "style": "IPY_MODEL_79d9af2c652742aeae185e4cb2d723b5",
            "value": " 1.34G/1.34G [00:37&lt;00:00, 48.1MB/s]"
          }
        },
        "05041e24ad454fce81d7144993ed2531": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f57650a366f74b378df3fc0e403f79bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "742720860f9d44dcb1d6f0401ad58c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c01a67c74b2a4cd4bee65edac09a0ded": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55fe97ec28904d0e8af8d71ad2272099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6dd3a9cd60ba4700b7788d321b0a69e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79d9af2c652742aeae185e4cb2d723b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osmarbraz/exemplos_BERT/blob/main/ExemplosContextoGlobalBERT_pt_br.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78HE8FLsKN9Q"
      },
      "source": [
        "#Exemplo de Contexto global do documento Global(pt-br) usando BERT Transformers by HuggingFace\n",
        "\n",
        "# **A execução pode ser feita através do menu Ambiente de Execução opção Executar tudo.**\n",
        "\n",
        "Exemplos de **documento Global(pt-br)** de documentos usando **BERT**.\n",
        "\n",
        "\n",
        "**Link biblioteca Huggingface:**\n",
        "https://github.com/huggingface/transformers\n",
        "\n",
        "\n",
        "**Artigo original BERT Jacob Devlin:**\n",
        "https://arxiv.org/pdf/1506.06724.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyxb5Px3p1-e"
      },
      "source": [
        "# 0 - Preparação do ambiente\n",
        "Preparação do ambiente para execução do exemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAPVtRXQqDim"
      },
      "source": [
        "##Tratamento de logs\n",
        "\n",
        "Método para tratamento dos logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcopxbGZqDip"
      },
      "source": [
        "# Biblioteca de logging\n",
        "import logging\n",
        "\n",
        "# Formatando a mensagem de logging\n",
        "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GjYtXcMnSAe"
      },
      "source": [
        "## Identificando o ambiente Colab\n",
        "\n",
        "Cria uma variável para identificar que o notebook está sendo executado no Google Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMiH0E3OnRa1"
      },
      "source": [
        "# Se estiver executando no Google Colaboratory\n",
        "import sys\n",
        "\n",
        "# Retorna true ou false se estiver no Google Colaboratory\n",
        "IN_COLAB = \"google.colab\" in sys.modules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RufkKnojlwzu"
      },
      "source": [
        "## Instalação do spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0LeiOTx0Dlk"
      },
      "source": [
        "https://spacy.io/\n",
        "\n",
        "Modelos do spaCy para português:\n",
        "https://spacy.io/models/pt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2Fvx0TVRQUw",
        "outputId": "0642441a-37a3-4118-ff3b-beb209a9ada4"
      },
      "source": [
        "# Instala o spacy\n",
        "!pip install -U spacy==2.3.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==2.3.5\n",
            "  Downloading spacy-2.3.5-cp37-cp37m-manylinux2014_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 30.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (2.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (0.7.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.21.6)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Collecting srsly<1.1.0,>=1.0.2\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 42.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (3.0.6)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (0.9.1)\n",
            "Collecting thinc<7.5.0,>=7.4.1\n",
            "  Downloading thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 57.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2022.6.15)\n",
            "Installing collected packages: srsly, plac, catalogue, thinc, spacy\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.3\n",
            "    Uninstalling srsly-2.4.3:\n",
            "      Successfully uninstalled srsly-2.4.3\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.7\n",
            "    Uninstalling catalogue-2.0.7:\n",
            "      Successfully uninstalled catalogue-2.0.7\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.0.17\n",
            "    Uninstalling thinc-8.0.17:\n",
            "      Successfully uninstalled thinc-8.0.17\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.3.1\n",
            "    Uninstalling spacy-3.3.1:\n",
            "      Successfully uninstalled spacy-3.3.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.3.0 requires spacy<3.4.0,>=3.3.0.dev0, but you have spacy 2.3.5 which is incompatible.\u001b[0m\n",
            "Successfully installed catalogue-1.0.0 plac-1.1.3 spacy-2.3.5 srsly-1.0.5 thinc-7.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35GwcgkOlWi3"
      },
      "source": [
        "Realiza o download e carrega os modelos necessários a biblioteca\n",
        "\n",
        "https://spacy.io/models/pt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4LqE5kTwDYm"
      },
      "source": [
        "# Definição do nome do arquivo do modelo\n",
        "#ARQUIVOMODELO = \"pt_core_news_sm\"\n",
        "#ARQUIVOMODELO = \"pt_core_news_md\"\n",
        "ARQUIVOMODELO = \"pt_core_news_lg\"\n",
        "\n",
        "# Definição da versão da spaCy\n",
        "#VERSAOSPACY = \"-3.0.0a0\"\n",
        "VERSAOSPACY = \"-2.3.0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ2KB3UCp-ws"
      },
      "source": [
        "#Baixa automaticamente o arquivo do modelo.\n",
        "#!python -m spacy download {ARQUIVOMODELO}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASk5iFeUp9LE",
        "outputId": "79b3498c-0f8c-4e06-af2e-26284e4c11c9"
      },
      "source": [
        "# Realiza o download do arquivo do modelo para o diretório corrente\n",
        "!wget https://github.com/explosion/spacy-models/releases/download/{ARQUIVOMODELO}{VERSAOSPACY}/{ARQUIVOMODELO}{VERSAOSPACY}.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-27 13:10:02--  https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-2.3.0/pt_core_news_lg-2.3.0.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/84940268/a899e480-ab07-11ea-831b-b5aa9cc04510?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220627%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220627T131002Z&X-Amz-Expires=300&X-Amz-Signature=3fdd75ced0c3e51450ff4adb8836375e3440ad0655d03c6ad10fc5ee2b0dd7d4&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Dpt_core_news_lg-2.3.0.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-06-27 13:10:02--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/84940268/a899e480-ab07-11ea-831b-b5aa9cc04510?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220627%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220627T131002Z&X-Amz-Expires=300&X-Amz-Signature=3fdd75ced0c3e51450ff4adb8836375e3440ad0655d03c6ad10fc5ee2b0dd7d4&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Dpt_core_news_lg-2.3.0.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 576599832 (550M) [application/octet-stream]\n",
            "Saving to: ‘pt_core_news_lg-2.3.0.tar.gz’\n",
            "\n",
            "pt_core_news_lg-2.3 100%[===================>] 549.89M  97.2MB/s    in 5.3s    \n",
            "\n",
            "2022-06-27 13:10:08 (104 MB/s) - ‘pt_core_news_lg-2.3.0.tar.gz’ saved [576599832/576599832]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu_LkF7Nfm8_"
      },
      "source": [
        "Descompacta o arquivo do modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9fCQQJGeVEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f506e5c-9e9e-49d3-b792-f876a92e2e9c"
      },
      "source": [
        "# Descompacta o arquivo do modelo\n",
        "!tar -xvf  /content/{ARQUIVOMODELO}{VERSAOSPACY}.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pt_core_news_lg-2.3.0/\n",
            "pt_core_news_lg-2.3.0/PKG-INFO\n",
            "pt_core_news_lg-2.3.0/setup.py\n",
            "pt_core_news_lg-2.3.0/setup.cfg\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/dependency_links.txt\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/PKG-INFO\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/SOURCES.txt\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/requires.txt\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/top_level.txt\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg.egg-info/not-zip-safe\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/__init__.py\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/parser/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/parser/cfg\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/parser/moves\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/parser/model\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/ner/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/ner/cfg\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/ner/moves\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/ner/model\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tokenizer\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/lookups.bin\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/vectors\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/key2row\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/lookups_extra.bin\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/vocab/strings.json\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/accuracy.json\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tagger/\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tagger/cfg\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tagger/tag_map\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/tagger/model\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/pt_core_news_lg-2.3.0/meta.json\n",
            "pt_core_news_lg-2.3.0/pt_core_news_lg/meta.json\n",
            "pt_core_news_lg-2.3.0/MANIFEST.in\n",
            "pt_core_news_lg-2.3.0/meta.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovOx-3Wb-JJW"
      },
      "source": [
        "# Coloca a pasta do modelo descompactado em uma pasta de nome mais simples\n",
        "!mv /content/{ARQUIVOMODELO}{VERSAOSPACY}/{ARQUIVOMODELO}/{ARQUIVOMODELO}{VERSAOSPACY} /content/{ARQUIVOMODELO}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STHT2c89qvwK"
      },
      "source": [
        "Carrega o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbELnrpgA4T1"
      },
      "source": [
        "import spacy\n",
        "\n",
        "CAMINHOMODELO = \"/content/\" + ARQUIVOMODELO\n",
        "\n",
        "#nlp = spacy.load(CAMINHOMODELO)\n",
        "# Necessário \"tagger\" para encontrar os substantivos\n",
        "nlp = spacy.load(CAMINHOMODELO, disable=[\"tokenizer\", \"lemmatizer\", \"ner\", \"parser\", \"textcat\", \"custom\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFTTdqxKQ1Ay"
      },
      "source": [
        "Recupera os stopwords do spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBInu7ayQ31J"
      },
      "source": [
        "# Recupera as stop words\n",
        "spacy_stopwords = nlp.Defaults.stop_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_EYNu-_RX7k"
      },
      "source": [
        "Lista dos stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUSaUJEWRbnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3acfdf86-4ca2-4983-f2b2-ce811fb4d8f4"
      },
      "source": [
        "print(\"Quantidade de stopwords:\", len(spacy_stopwords))\n",
        "\n",
        "print(spacy_stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de stopwords: 413\n",
            "{'podem', 'uma', 'comprido', 'minhas', 'estava', 'boa', 'nova', 'somente', 'tivemos', 'pois', 'inicio', 'estado', 'tiveste', 'nove', 'irá', 'bom', 'esteve', 'fez', 'como', 'maioria', 'sem', 'nível', 'foram', 'podia', 'és', 'zero', 'das', 'conhecido', 'sei', 'aí', 'dá', 'lhe', 'nossa', 'tais', 'puderam', 'fazem', 'ambos', 'ao', 'teu', 'está', 'sobre', 'após', 'suas', 'embora', 'tente', 'estive', 'somos', 'vários', 'sim', 'ainda', 'cedo', 'agora', 'estiveste', 'relação', 'sexto', 'também', 'diante', 'tenho', 'sou', 'então', 'teus', 'mês', 'primeira', 'pela', 'naquela', 'ligado', 'custa', 'qualquer', 'dos', 'catorze', 'diz', 'tiveram', 'lado', 'cuja', 'só', 'favor', 'mal', 'até', 'corrente', 'fomos', 'quem', 'elas', 'deve', 'tentaram', 'quanto', 'aos', 'questão', 'ponto', 'dezasseis', 'dezoito', 'demais', 'devem', 'nada', 'tentei', 'deste', 'porquê', 'mil', 'do', 'conhecida', 'sexta', 'estás', 'primeiro', 'vinda', 'pelos', 'todos', 'já', 'portanto', 'põe', 'tempo', 'terceiro', 'quê', 'dois', 'estou', 'cada', 'se', 'vós', 'onde', 'próxima', 'número', 'depois', 'fim', 'poder', 'ali', 'tua', 'quieta', 'de', 'sétimo', 'dez', 'segunda', 'vocês', 'cinco', 'bastante', 'lá', 'maior', 'vossos', 'tens', 'daquele', 'nessa', 'apoia', 'ora', 'sob', 'querem', 'ela', 'usa', 'faço', 'tuas', 'outra', 'quer', 'disso', 'foi', 'temos', 'usar', 'ambas', 'outros', 'contudo', 'oitava', 'parece', 'eles', 'pode', 'cá', 'desse', 'desde', 'faz', 'dar', 'ele', 'daquela', 'vão', 'algumas', 'desta', 'deverá', 'quarta', 'três', 'segundo', 'possivelmente', 'lugar', 'neste', 'veja', 'porque', 'dezassete', 'meses', 'fui', 'sempre', 'põem', 'fazeis', 'coisa', 'estão', 'dão', 'momento', 'dentro', 'na', 'antes', 'des', 'seus', 'enquanto', 'aquilo', 'todas', 'ver', 'local', 'pouca', 'máximo', 'números', 'me', 'com', 'grandes', 'apenas', 'eventual', 'vais', 'sua', 'para', 'nuns', 'da', 'pelas', 'ontem', 'ou', 'pouco', 'quero', 'sabe', 'pôde', 'falta', 'quinto', 'onze', 'tudo', 'alguns', 'vos', 'nos', 'quais', 'atrás', 'geral', 'fazia', 'obrigada', 'apoio', 'último', 'vindo', 'menos', 'meio', 'nossos', 'treze', 'sete', 'mais', 'qual', 'final', 'no', 'ser', 'te', 'acerca', 'pelo', 'tive', 'certamente', 'novo', 'próprio', 'novas', 'toda', 'nosso', 'foste', 'dezanove', 'estiveram', 'oitavo', 'você', 'dessa', 'parte', 'nas', 'através', 'saber', 'vossa', 'debaixo', 'vêm', 'que', 'um', 'estar', 'dizer', 'quinta', 'as', 'exemplo', 'maiorias', 'tentar', 'eu', 'numa', 'breve', 'conselho', 'posso', 'vai', 'além', 'contra', 'estes', 'sistema', 'têm', 'forma', 'vossas', 'entre', 'nesta', 'ademais', 'tão', 'tal', 'seu', 'doze', 'dizem', 'terceira', 'naquele', 'umas', 'grupo', 'perto', 'nenhuma', 'possível', 'tanto', 'vem', 'vinte', 'novos', 'todo', 'não', 'nós', 'este', 'sétima', 'grande', 'nesse', 'tu', 'aquela', 'ir', 'estará', 'tanta', 'algo', 'pegar', 'bem', 'caminho', 'posição', 'valor', 'os', 'estivemos', 'quinze', 'área', 'cujo', 'inclusive', 'isso', 'tivestes', 'nem', 'aqui', 'poderá', 'pontos', 'porquanto', 'comprida', 'iniciar', 'cima', 'esses', 'talvez', 'aquelas', 'muitos', 'vez', 'esta', 'quarto', 'certeza', 'num', 'seis', 'seria', 'fará', 'mesmo', 'direita', 'mas', 'estivestes', 'teve', 'por', 'aqueles', 'povo', 'duas', 'muito', 'minha', 'quatro', 'oito', 'partir', 'uns', 'baixo', 'à', 'quieto', 'sois', 'longe', 'isto', 'cento', 'adeus', 'logo', 'menor', 'outras', 'em', 'vens', 'essas', 'porém', 'fazer', 'essa', 'era', 'tendes', 'são', 'meu', 'vosso', 'próximo', 'é', 'tarde', 'meus', 'aquele', 'esse', 'vezes', 'fazemos', 'nossas', 'fazes', 'for', 'às', 'tipo', 'assim', 'ter', 'nunca', 'quando', 'tem', 'obrigado', 'estas', 'fostes', 'apontar', 'fora'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqa-7WXBAw8q"
      },
      "source": [
        "## Instalação do BERT da Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCdqJCtQN52l"
      },
      "source": [
        "Instala a interface pytorch para o BERT by Hugging Face. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RfUN_KolV-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0874580a-72ef-4788-8283-6a707a2bb163"
      },
      "source": [
        "# Instala a última versão da biblioteca\n",
        "#!pip install transformers\n",
        "\n",
        "# Instala uma versão específica da biblioteca\n",
        "!pip install -U transformers==4.5.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.5.1\n",
            "  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 27.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.11.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2022.6.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 54.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.1) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.1) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.1) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=570ea55a9cd229708caef86cd210f0325d86b83e14a5404be04876f74a89d8da\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQj2wmKDpkrH"
      },
      "source": [
        "# 2 - Download do arquivo do PyTorch Checkpoint\n",
        "\n",
        "Lista de modelos da comunidade:\n",
        "* https://huggingface.co/models\n",
        "\n",
        "Português(https://github.com/neuralmind-ai/portuguese-bert):  \n",
        "* **\"neuralmind/bert-base-portuguese-cased\"**\n",
        "* **\"neuralmind/bert-large-portuguese-cased\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajrTjZzapkrK",
        "outputId": "5bc053f3-6d15-48a5-f89b-fe887ffe97a6"
      },
      "source": [
        "# Importando as bibliotecas\n",
        "import os\n",
        "\n",
        "# Variável para setar o arquivo\n",
        "URL_MODELO = None\n",
        "\n",
        "# Comente uma das urls para carregar modelos de tamanhos diferentes(base/large)\n",
        "# URL_MODELO do arquivo do modelo tensorflow\n",
        "# arquivo menor(base) 1.1 Gbytes\n",
        "#URL_MODELO = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip\"\n",
        "\n",
        "# arquivo grande(large) 3.5 Gbytes\n",
        "#URL_MODELO = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-large-portuguese-cased/bert-large-portuguese-cased_pytorch_checkpoint.zip\"\n",
        "\n",
        "# Se a variável foi setada\n",
        "if URL_MODELO:\n",
        "\n",
        "    # Diretório descompactação\n",
        "    DIRETORIO_MODELO = \"/content/modelo\"\n",
        "\n",
        "    # Recupera o nome do arquivo do modelo da URL_MODELO\n",
        "    arquivo = URL_MODELO.split(\"/\")[-1]\n",
        "\n",
        "    # Nome do arquivo do vocabulário\n",
        "    arquivo_vocab = \"vocab.txt\"\n",
        "\n",
        "    # Caminho do arquivo na URL_MODELO\n",
        "    caminho = URL_MODELO[0:len(URL_MODELO)-len(arquivo)]\n",
        "\n",
        "    # Verifica se a pasta de descompactação existe na pasta corrente\n",
        "    if os.path.exists(DIRETORIO_MODELO):\n",
        "      print(\"Apagando diretório existente do modelo!\")\n",
        "      # Apaga a pasta e os arquivos existentes\n",
        "      !rm -rf $DIRETORIO_MODELO      \n",
        "    \n",
        "    # Baixa o arquivo do modelo\n",
        "    !wget $URL_MODELO\n",
        "    # Descompacta o arquivo na pasta de descompactação\n",
        "    !unzip -o $arquivo -d $DIRETORIO_MODELO\n",
        "\n",
        "    # Baixa o arquivo do vocabulário\n",
        "    # O vocabulário não está no arquivo compactado acima, mesma url mas arquivo diferente\n",
        "    URL_MODELO_VOCAB = caminho + arquivo_vocab\n",
        "    !wget $URL_MODELO_VOCAB\n",
        "    \n",
        "    # Coloca o arquivo do vocabulário no diretório de descompactação\n",
        "    !mv $arquivo_vocab $DIRETORIO_MODELO\n",
        "            \n",
        "    # Move o arquivo para pasta de descompactação\n",
        "    !mv $arquivo $DIRETORIO_MODELO\n",
        "       \n",
        "    print(\"Pasta do \" + DIRETORIO_MODELO + \" pronta!\")\n",
        "    \n",
        "    # Lista a pasta corrente\n",
        "    !ls -la $DIRETORIO_MODELO\n",
        "else:\n",
        "    DIRETORIO_MODELO = None\n",
        "    print(\"Variável URL_MODELO não setada!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variável URL_MODELO não setada!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcpd9t9PpkrX"
      },
      "source": [
        "# 3 - Carregando o Tokenizador BERT\n",
        "\n",
        "O tokenizador utiliza WordPiece, veja em [artigo original](https://arxiv.org/pdf/1609.08144.pdf).\n",
        "\n",
        "Carregando o tokenizador da pasta \"/content/modelo/\" do diretório padrão se variável `URL_MODELO` setada.\n",
        "\n",
        "**Caso contrário carrega da comunidade**\n",
        "\n",
        "Por default(`do_lower_case=True`) todas as letras são colocadas para minúsculas. Para ignorar a conversão para minúsculo use o parâmetro `do_lower_case=False`. Esta opção também considera as letras acentuadas(ãçéí...), que são necessárias a língua portuguesa.\n",
        "\n",
        "O parâmetro `do_lower_case` interfere na quantidade tokens a ser gerado a partir de um documento. Quando igual a `False` reduz a quantidade de tokens gerados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "ae24d2d3869c44168703fdcf572d1b17",
            "931a2d7641264104824f6deb6d1e86ff",
            "5535a4c4fe7a4c5a9b3476147051ccf2",
            "17829a77c1cc4adcb26efa95641eba31",
            "080262663ffd4b67bcacdb4b464e8713",
            "3a56a5916fa64b21957059154dee4659",
            "ae9146b94efd4ef8a09cb9731733bbdf",
            "eb78372760914a14b2988bff17edb89d",
            "a710dd1b77094798945a4b76b1f6f985",
            "09f2cf1a31e8458e9108db3954453a06",
            "e966b34ef716488990310dbe779a9ff8",
            "1b86afb78fe841a994f1440a9446a940",
            "8fa3e3c0963d4888ac7329352a76d695",
            "eb690a64a4104148a98cdd52cd01b906",
            "c1b60c065fb242df879d95fa12e44a9b",
            "ff8ecbcea62f43a1883454af1a5761b9",
            "d6fbdacdf350433e83c321ec7f0952e3",
            "40f3b9eb43b84189bfb746759e9f2382",
            "0d282a191a364362bf018a61d9b145e9",
            "80ad98c41a0f400c836a8b6c83d17630",
            "019fb6e4500b4418960db01e6a6181d1",
            "368a6590259940ee931420c6cf6f7db7",
            "0864470cc7f44fc081f7da0253255579",
            "c6e9bc6491574edf9bb2c33a9164a6e8",
            "24da427783324e9e8b48cc3fac88f7d3",
            "1c9dd242c33c44e696bfd8dab207d2bc",
            "35202acd11834508ba93dd5034ff0326",
            "8a9352d592734c70a21c784474ff7c4b",
            "ab54fac1c6034f1e95e81f9eaa856c80",
            "991d7db3b7e34e0380e10172d4c384cf",
            "0a3a4d91df114f9e97d6f3fc4b0a8ec7",
            "22bce9c9c09c4dd9b8a7874796770f19",
            "17b68795b28442abb411f3012e12f8d5",
            "ba9ada1ba2b8465e95ef86255df87a24",
            "427cacc5b7ac4888a66f4e12b1a49536",
            "eaf0149a6bf34726b71381c309fd251a",
            "e95ff4099657488e9d827e9470e97ca1",
            "e506c49760f94d0f853507de5007b149",
            "c096a814d6e24b4eb2ffbcd70bb207d7",
            "db45d9c02528458dbc8e0f4d7ec0b8ff",
            "b69b9801f0a4407d9bba9c7304099207",
            "4389100548734c9e9124fe3d4d0843bc",
            "c8d87ebededf4ac6ae54d5a7128d1b58",
            "cef097ecb0d24215a6a03101df7fa044"
          ]
        },
        "id": "Z8cKVs4fpkrY",
        "outputId": "5d74e91e-20ad-4c7b-c083-8aa654317c53"
      },
      "source": [
        "# Importando as bibliotecas do tokenizador\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Se a variável URL_MODELO foi setada\n",
        "if DIRETORIO_MODELO:\n",
        "    # Carregando o Tokenizador\n",
        "    print(\"Carrgando o tokenizador BERT do diretório \" + DIRETORIO_MODELO + \"...\")\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(DIRETORIO_MODELO, \n",
        "                                              do_lower_case=False)    \n",
        "else:\n",
        "    # Carregando o Tokenizador da comunidade\n",
        "    print(\"Carregando o tokenizador da comunidade...\")\n",
        "    \n",
        "    #tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", do_lower_case=False)\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-large-portuguese-cased\", do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando o tokenizador da comunidade...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/210k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae24d2d3869c44168703fdcf572d1b17"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b86afb78fe841a994f1440a9446a940"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0864470cc7f44fc081f7da0253255579"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/155 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba9ada1ba2b8465e95ef86255df87a24"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m__On2g1a--K"
      },
      "source": [
        "# 4 - Carregando o Modelo BERT(BertModel)\n",
        "\n",
        "Se a variável `URL_MODELO` estiver setada carrega o modelo do diretório `content/modelo`.\n",
        "\n",
        "Caso contrário carrega da comunidade.\n",
        "\n",
        "Carregando o modelo da pasta \"/content/modelo/\" do diretório padrão.\n",
        "\n",
        "A implementação do huggingface pytorch inclui um conjunto de interfaces projetadas para uma variedade de tarefas de PNL. Embora essas interfaces sejam todas construídas sobre um modelo treinado de BERT, cada uma possui diferentes camadas superiores e tipos de saída projetados para acomodar suas tarefas específicas de PNL.\n",
        "\n",
        "A documentação para estas pode ser encontrada em [aqui](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html).\n",
        "\n",
        "Por default o modelo está em modo avaliação ou seja `model.eval()`.\n",
        "\n",
        "-----------------------\n",
        "\n",
        "Durante a avaliação do modelo, este retorna um número de diferentes objetos com base em como é configurado na chamada do método `from_pretrained`. \n",
        "\n",
        "Quando definimos `output_hidden_states = True` na chamada do método `from_pretrained`, retorno do modelo possui no terceiro item os estados ocultos(**hidden_states**) de todas as camadas.  Veja a documentação para mais detalhes: https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "\n",
        "Quando **`output_hidden_states = True`** model retorna:\n",
        "- outputs[0] = last_hidden_state;\n",
        "- outputs[1] = pooler_output; \n",
        "- outputs[2] = hidden_states.\n",
        "\n",
        "Quando **`output_hidden_states = False`** ou não especificado model retorna:\n",
        "- outputs[0] = last_hidden_state;\n",
        "- outputs[1] = pooler_output.\n",
        "\n",
        "\n",
        "**ATENÇÃO**: O parâmetro ´**output_hidden_states = True**´ habilita gerar as camadas ocultas do modelo. Caso contrário somente a última camada é mantida. Este parâmetro otimiza a memória mas não os resultados.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "25fbb7b4ccf44f71b748ac23eb48b2a9",
            "a996cdb1c0c7481a86b619f71d4a3bfa",
            "0453373635a24e93a7173ee89b24a211",
            "51109da084d84c8886adfc1eca2a6b0f",
            "f52a93dab2c647d68e64c63da70d3db2",
            "c7e9df119fc0452ea8ea3cc17a4b7dc5",
            "70b5ec253e0f4e65a764926f709131a6",
            "550e42f094ad4e50a08a2c0752b47b83",
            "adcec6f71cc047a4a372f65ae254973f",
            "f60f739e5852490e960a309b46ebd71a",
            "924478c918644df69a1fd097dbd34b23",
            "c98698b0aadb41ba92ee9c6ad7b2bdba",
            "67b039ff66734717a825d7fbb3650075",
            "cccd7d650d8343dbbec6c9b0c5be2bcf",
            "b325f94bc9cd4b4e9e30a6c55e40f529",
            "05041e24ad454fce81d7144993ed2531",
            "f57650a366f74b378df3fc0e403f79bf",
            "742720860f9d44dcb1d6f0401ad58c28",
            "c01a67c74b2a4cd4bee65edac09a0ded",
            "55fe97ec28904d0e8af8d71ad2272099",
            "6dd3a9cd60ba4700b7788d321b0a69e6",
            "79d9af2c652742aeae185e4cb2d723b5"
          ]
        },
        "id": "zRV6l_I-qg9s",
        "outputId": "55a4c41d-24ee-4711-8ddd-25db36b1fe7b"
      },
      "source": [
        "# Importando as bibliotecas do Modelo\n",
        "from transformers import BertModel\n",
        "\n",
        "# Se a variável URL_MODELO1 foi setada\n",
        "if URL_MODELO:\n",
        "    # Carregando o Tokenizador\n",
        "    print(\"Carregando o modelo BERT do diretório \" + DIRETORIO_MODELO + \"...\")\n",
        "\n",
        "    model = BertModel.from_pretrained(DIRETORIO_MODELO, \n",
        "                                      output_attentions = False,\n",
        "                                      output_hidden_states = True)    \n",
        "else:\n",
        "    # Carregando o Tokenizador da comunidade\n",
        "    print(\"Carregando o modelo BERT da comunidade ...\")\n",
        "\n",
        "    model = BertModel.from_pretrained(\"neuralmind/bert-large-portuguese-cased\", \n",
        "                                      output_attentions = False,\n",
        "                                      output_hidden_states = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando o modelo BERT da comunidade ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/648 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25fbb7b4ccf44f71b748ac23eb48b2a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c98698b0aadb41ba92ee9c6ad7b2bdba"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU3wHzNUmmBP"
      },
      "source": [
        "# 5 - Funções auxiliares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWqMsrb-ew5T"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm98RoojJcqP"
      },
      "source": [
        "# Import das bibliotecas\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xaeX0oTVQ5t"
      },
      "source": [
        "##removeStopWords\n",
        "\n",
        "Remove as stopwords de um documento ou senteça."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIaQ9bzBVQ5t"
      },
      "source": [
        "def removeStopWord(documento, stopwords):\n",
        "  # Remoção das stop words do documento\n",
        "  documentoSemStopwords = [palavra for palavra in documento.split() if palavra.lower() not in stopwords]\n",
        "\n",
        "  # Concatena o documento sem os stopwords\n",
        "  documentoLimpo = \" \".join(documentoSemStopwords)\n",
        "\n",
        "  # Retorna o documento\n",
        "  return documentoLimpo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7NAe8ogCf1y"
      },
      "source": [
        "## retornaRelevante\n",
        "\n",
        "Retorna somente os palavras do documento ou sentença do tipo especificado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNNfykypChn-"
      },
      "source": [
        "def retornaRelevante(documento, tipoRelevante=\"NOUN\"):\n",
        "  \n",
        "  # Realiza o parsing no spacy\n",
        "  doc = nlp(documento)\n",
        "\n",
        "  # Retorna a lista das palavras relevantes\n",
        "  documentoComSubstantivos = []\n",
        "  for token in doc:\n",
        "    #print(\"token:\", token.pos_)\n",
        "    if token.pos_ == tipoRelevante:\n",
        "      documentoComSubstantivos.append(token.text)\n",
        "  #documentoComSubstantivos = [token.text for token in doc if token.pos_ == tipoRelevante]\n",
        "\n",
        "  # Concatena o documento com os substantivos\n",
        "  documentoConcatenado = \" \".join(documentoComSubstantivos)\n",
        "\n",
        "  # Retorna o documento\n",
        "  return documentoConcatenado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s42mgtmSZ8MR"
      },
      "source": [
        "## getEmbeddingsCamadas\n",
        "\n",
        "Funções que recuperam os embeddings das camadas:\n",
        "- Primeira camada;\n",
        "- Penúltima camada;\n",
        "- Ùltima camada;\n",
        "- Soma das 4 últimas camadas;\n",
        "- Concatenação das 4 últimas camadas;\n",
        "- Soma de todas as camadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgo3EBTRZ9-3"
      },
      "source": [
        "def getEmbeddingPrimeiraCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][0]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingPenultimaCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][-2]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingUltimaCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "     \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = output[2][-1]\n",
        "  # Saída: (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  return resultado    \n",
        "\n",
        "def getEmbeddingSoma4UltimasCamadas(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Retorna todas a primeira(-1) camada\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  embeddingCamadas = output[2][-4:]\n",
        "  # Saída: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "\n",
        "  # Usa o método `stack` para criar uma nova dimensão no tensor \n",
        "  # com a concateção dos tensores dos embeddings.        \n",
        "  #Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultadoStack = torch.stack(embeddingCamadas, dim=0)\n",
        "  # Saída: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  \n",
        "  # Realiza a soma dos embeddings de todos os tokens para as camadas\n",
        "  # Entrada: <4> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  resultado = torch.sum(resultadoStack, dim=0)\n",
        "  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  \n",
        "  return resultado\n",
        "\n",
        "def getEmbeddingConcat4UltimasCamadas(output):  \n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "      \n",
        "  # Cria uma lista com os tensores a serem concatenados\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  # Lista com os tensores a serem concatenados\n",
        "  listaConcat = []\n",
        "  # Percorre os 4 últimos\n",
        "  for i in [-1,-2,-3,-4]:\n",
        "      # Concatena da lista\n",
        "      listaConcat.append(output[2][i])\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  # Realiza a concatenação dos embeddings de todos as camadas\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultado = torch.cat(listaConcat, dim=-1)\n",
        "  # Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "    \n",
        "  return resultado   \n",
        "\n",
        "def getEmbeddingSomaTodasAsCamada(output):\n",
        "  # outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "  # hidden_states é uma lista python, e cada elemento um tensor pytorch no formado <lote> x <qtde_tokens> x <768 ou 1024>.\n",
        "   \n",
        "  # Retorna todas as camadas descontando a primeira(0)\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  embeddingCamadas = output[2][1:]\n",
        "  # Saída: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  \n",
        "  # Usa o método `stack` para criar uma nova dimensão no tensor \n",
        "  # com a concateção dos tensores dos embeddings.        \n",
        "  #Entrada: List das camadas(12 ou 24) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  resultadoStack = torch.stack(embeddingCamadas, dim=0)\n",
        "  # Saída: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "    \n",
        "  # Realiza a soma dos embeddings de todos os tokens para as camadas\n",
        "  # Entrada: <12 ou 24> x <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "  resultado = torch.sum(resultadoStack, dim=0)\n",
        "  # Saida: <1(lote)> x <qtde_tokens> x <768 ou 1024>\n",
        "    \n",
        "  return resultado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7nx_eZ8hSlr"
      },
      "source": [
        "## getEmbeddingsVisual\n",
        "\n",
        "Função para gerar as coordenadas de plotagem a partir das sentenças de embeddings.\n",
        "\n",
        "Existe uma função para os tipos de camadas utilizadas:\n",
        "- Ùltima camada;\n",
        "- Soma das 4 últimas camadas;\n",
        "- Concatenação das 4 últimas camadas;\n",
        "- Soma de todas as camadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLdbOT8-g43V"
      },
      "source": [
        "def getEmbeddingsVisualUltimaCamada(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingUltimaCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, documento_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAf9lJJ2hZbt"
      },
      "source": [
        "def getEmbeddingsVisualSoma4UltimasCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSoma4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, documento_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XpwSN1ghpnz"
      },
      "source": [
        "def getEmbeddingsVisualConcat4UltimasCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingConcat4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, documento_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3KU1EFrnSPK"
      },
      "source": [
        "def getEmbeddingsVisualSomaTodasAsCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSomaTodasAsCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    # Recupera os embeddings dos tokens como um vetor\n",
        "    embeddings = token_embeddings.numpy()\n",
        "\n",
        "    # Converte para um array\n",
        "    W = np.array(embeddings)\n",
        "    # Transforma em um array\n",
        "    B = np.array([embeddings[0], embeddings[-1]])\n",
        "    # Invertee B.T\n",
        "    Bi = np.linalg.pinv(B.T)\n",
        "\n",
        "    #Projeta a palavra no espaço\n",
        "    Wp = np.matmul(Bi,W.T)\n",
        "\n",
        "    return Wp, documento_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8MjE0utzlZT"
      },
      "source": [
        "## getEmbeddings\n",
        "\n",
        "Função para gerar os embeddings de sentenças.\n",
        "\n",
        "Existe uma função para os tipos de camadas utilizadas:\n",
        "- Ùltima camada;\n",
        "- Soma das 4 últimas camadas;\n",
        "- Concatenação das 4 últimas camadas;\n",
        "- Soma de todas as camadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QcqOuwS067Q"
      },
      "source": [
        "def getEmbeddingsUltimaCamada(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingUltimaCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        " \n",
        "    return token_embeddings, documento_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK1wDGBl067Y"
      },
      "source": [
        "def getEmbeddingsSoma4UltimasCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSoma4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "   \n",
        "    return token_embeddings, documento_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hym19Hxr067Y"
      },
      "source": [
        "def getEmbeddingsConcat4UltimasCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingConcat4UltimasCamadas(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    return token_embeddings, documento_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-PLZiUR067Z"
      },
      "source": [
        "def getEmbeddingsSomaTodasAsCamadas(documento, modelo, tokenizador):\n",
        "    \n",
        "    # Adiciona os tokens especiais\n",
        "    documento_marcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Divide a sentença em tokens\n",
        "    documento_tokenizado = tokenizador.tokenize(documento_marcado)\n",
        "\n",
        "    # Mapeia as strings dos tokens em seus índices do vocabuário    \n",
        "    tokens_indexados = tokenizador.convert_tokens_to_ids(documento_tokenizado)\n",
        "    \n",
        "    # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "    mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "    # Converte a entrada em tensores\n",
        "    tokens_tensores = torch.as_tensor([tokens_indexados])\n",
        "    mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "    \n",
        "    # Prediz os atributos dos estados ocultos para cada camada\n",
        "    with torch.no_grad():        \n",
        "        # Retorno de model quando ´output_hidden_states=True´ é setado:  \n",
        "        #outputs[0] = last_hidden_state, outputs[1] = pooler_output, outputs[2] = hidden_states\n",
        "        outputs = modelo(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "    # Camada embedding    \n",
        "    camada = getEmbeddingSomaTodasAsCamada(outputs)\n",
        "\n",
        "    # Remove a dimensão 1, o lote \"batches\".\n",
        "    token_embeddings = torch.squeeze(camada, dim=0)\n",
        "\n",
        "    return token_embeddings, documento_tokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFd1rse11DpZ"
      },
      "source": [
        "## getDocumentoTokenizado \n",
        "Retorna o documento tokenizado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvWIBFTLJ7z9"
      },
      "source": [
        "def getDocumentoTokenizado(documento, tokenizador):\n",
        "\n",
        "    # Adiciona os tokens especiais.\n",
        "    documentoMarcado = \"[CLS] \" + documento + \" [SEP]\"\n",
        "\n",
        "    # Documento tokenizado\n",
        "    documentoTokenizado = tokenizador.tokenize(documentoMarcado)\n",
        "\n",
        "    return documentoTokenizado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wvgXwN81RCz"
      },
      "source": [
        "## encontrarIndiceSubLista \n",
        "\n",
        "Retorna os índices de início e fim da sublista na lista"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abS44M4yvFxf"
      },
      "source": [
        "# Localiza os índices de início e fim de uma sublista em uma lista\n",
        "def encontrarIndiceSubLista(lista, sublista):\n",
        "    # Recupera o tamanho da lista \n",
        "    h = len(lista)\n",
        "    # Recupera o tamanho da sublista\n",
        "    n = len(sublista)    \n",
        "    skip = {sublista[i]: n - i - 1 for i in range(n - 1)}\n",
        "    i = n - 1\n",
        "    while i < h:\n",
        "        for j in range(n):\n",
        "            if lista[i - j] != sublista[-j - 1]:\n",
        "                i += skip.get(lista[i], n)\n",
        "                break\n",
        "        else:\n",
        "            indiceInicio = i - n + 1\n",
        "            indiceFim = indiceInicio + len(sublista)-1\n",
        "            return indiceInicio, indiceFim\n",
        "    return -1, -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_pnIh1h1Z_J"
      },
      "source": [
        "## getEmbeddingSentencaEmbeddingDocumentoComTodasPalavras\n",
        "\n",
        "Retorna os embeddings de uma sentença com todas as palavras a partir dos embeddings do documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSQs3O5QpJSj"
      },
      "source": [
        "def getEmbeddingSentencaEmbeddingDocumentoComTodasPalavras(embeddingDocumento, documento, sentenca, tokenizador):\n",
        "  \n",
        "  # Tokeniza o documento\n",
        "  documentoTokenizado = getDocumentoTokenizado(documento, tokenizador)  \n",
        "  #print(documentoTokenizado)\n",
        "\n",
        "  # Tokeniza a sentença\n",
        "  sentencaTokenizada = getDocumentoTokenizado(sentenca, tokenizador)\n",
        "  \n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentencaTokenizada.remove(\"[CLS]\")\n",
        "  sentencaTokenizada.remove(\"[SEP]\")  \n",
        "  #print(sentencaTokenizada)\n",
        "  #print(len(sentencaTokenizada))\n",
        "  \n",
        "  # Localiza os índices dos tokens da sentença no documento\n",
        "  inicio, fim = encontrarIndiceSubLista(documentoTokenizado,sentencaTokenizada)\n",
        "  #print(\"Sentença inicia em:\", inicio, \"até\", fim) \n",
        " \n",
        "  # Recupera os embeddings dos tokens da sentença a partir dos embeddings do documento\n",
        "  embeddingSentenca = embeddingDocumento[inicio:fim+1]\n",
        "  #print(\"embeddingSentenca=\", embeddingSentenca.shape)\n",
        "  \n",
        "  # Retorna o embedding da sentença do documento\n",
        "  return embeddingSentenca"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd9xmB9jwZZN"
      },
      "source": [
        "## getEmbeddingSentencaEmbeddingDocumentoSemStopWord\n",
        "\n",
        "Retorna os embeddings de uma sentença sem stopwords a partir dos embeddings do documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5XVsCsdwZZP"
      },
      "source": [
        "def getEmbeddingSentencaEmbeddingDocumentoSemStopWord(embeddingDocumento, documento, sentenca, tokenizador):\n",
        "  \n",
        "  # Tokeniza o documento\n",
        "  documentoTokenizado = getDocumentoTokenizado(documento, tokenizador)  \n",
        "  #print(documentoTokenizado)\n",
        "  \n",
        "  # Remove as stopword da sentença\n",
        "  sentencaSemStopWord = removeStopWord(sentenca, spacy_stopwords)\n",
        "\n",
        "  # Tokeniza a sentença sem stopword\n",
        "  sentencaTokenizadaSemStopWord = getDocumentoTokenizado(sentencaSemStopWord, tokenizador)\n",
        "\n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentencaTokenizadaSemStopWord.remove(\"[CLS]\")\n",
        "  sentencaTokenizadaSemStopWord.remove(\"[SEP]\")  \n",
        "  #print(sentencaTokenizadaSemStopWord)\n",
        "  #print(len(sentencaTokenizadaSemStopWord))\n",
        "\n",
        "  # Tokeniza a sentença\n",
        "  sentencaTokenizada = getDocumentoTokenizado(sentenca, tokenizador)\n",
        "  \n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentencaTokenizada.remove(\"[CLS]\")\n",
        "  sentencaTokenizada.remove(\"[SEP]\")  \n",
        "  #print(sentencaTokenizada)\n",
        "  #print(len(sentencaTokenizada))\n",
        "\n",
        "  # Localiza os índices dos tokens da sentença no documento\n",
        "  inicio, fim = encontrarIndiceSubLista(documentoTokenizado,sentencaTokenizada)\n",
        "  #print(\"Sentença inicia em:\", inicio, \"até\", fim) \n",
        "   \n",
        "  # Recupera os embeddings dos tokens da sentença a partir dos embeddings do documento\n",
        "  embeddingSentenca = embeddingDocumento[inicio:fim+1]\n",
        "  #print(\"embeddingSentenca=\", embeddingSentenca.shape)\n",
        "\n",
        "  # Lista com os tensores selecionados\n",
        "  listaTokensSelecionados = []\n",
        "  # Localizar os embeddings dos tokens da sentença tokenizada sem stop word na sentença \n",
        "  # Procura somente no intervalo da sentença\n",
        "  for i, tokenSentenca in enumerate(sentencaTokenizada):\n",
        "    for tokenSentencaSemStopWord in sentencaTokenizadaSemStopWord: \n",
        "      if tokenSentenca == tokenSentencaSemStopWord:        \n",
        "        listaTokensSelecionados.append(embeddingSentenca[i:i+1])\n",
        "  \n",
        "  # Concatena os vetores da lista pela dimensão 0\n",
        "  embeddingSentencaSemStopWord = torch.cat(listaTokensSelecionados, dim=0)\n",
        "  #print(\"embeddingSentencaSemStopWord:\",embeddingSentencaSemStopWord.shape)\n",
        "\n",
        "  # Retorna o embedding da sentença do documento\n",
        "  return embeddingSentencaSemStopWord"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgW4gfEzh34J"
      },
      "source": [
        "## getEmbeddingSentencaEmbeddingDocumentoSomenteRelevante\n",
        "\n",
        "Retorna os embeddings de uma sentença somente com as palavras relevantes a partir dos embeddings do documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHbQJhzSh34T"
      },
      "source": [
        "def getEmbeddingSentencaEmbeddingDocumentoSomenteRelevante(embeddingDocumento, documento, sentenca, tokenizador, tipoRelevante=\"NOUN\"):\n",
        "  \n",
        "  # Tokeniza o documento\n",
        "  documentoTokenizado = getDocumentoTokenizado(documento, tokenizador)  \n",
        "  #print(documentoTokenizado)\n",
        "  \n",
        "  # Retorna as palavras relevantes da sentença do tipo especificado\n",
        "  sentencaSomenteRelevante = retornaRelevante(sentenca,tipoRelevante)\n",
        "\n",
        "  # Tokeniza a sentença \n",
        "  sentencaTokenizadaSomenteRelevante = getDocumentoTokenizado(sentencaSomenteRelevante, tokenizador)\n",
        "\n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentencaTokenizadaSomenteRelevante.remove(\"[CLS]\")\n",
        "  sentencaTokenizadaSomenteRelevante.remove(\"[SEP]\")  \n",
        "  #print(sentencaTokenizadaSomenteRelevante)\n",
        "  #print(len(sentencaTokenizadaSomenteRelevante))\n",
        "\n",
        "  # Tokeniza a sentença\n",
        "  sentencaTokenizada = getDocumentoTokenizado(sentenca, tokenizador)\n",
        "  \n",
        "  # Remove os tokens de início e fim da sentença\n",
        "  sentencaTokenizada.remove(\"[CLS]\")\n",
        "  sentencaTokenizada.remove(\"[SEP]\")  \n",
        "  #print(sentencaTokenizada)\n",
        "  #print(len(sentencaTokenizada))\n",
        "\n",
        "  # Localiza os índices dos tokens da sentença no documento\n",
        "  inicio, fim = encontrarIndiceSubLista(documentoTokenizado,sentencaTokenizada)\n",
        "  #print(\"Sentença inicia em:\", inicio, \"até\", fim) \n",
        "   \n",
        "  # Recupera os embeddings dos tokens da sentença a partir dos embeddings do documento\n",
        "  embeddingSentenca = embeddingDocumento[inicio:fim+1]\n",
        "  #print(\"embeddingSentenca=\", embeddingSentenca.shape)\n",
        "\n",
        "  # Lista com os tensores selecionados\n",
        "  listaTokensSelecionados = []\n",
        "  # Localizar os embeddings dos tokens da sentença tokenizada sem stop word na sentença \n",
        "  # Procura somente no intervalo da sentença\n",
        "  for i, tokenSentenca in enumerate(sentencaTokenizada):\n",
        "    for tokenSentencaSomenteRelevante in sentencaTokenizadaSomenteRelevante: \n",
        "      if tokenSentenca == tokenSentencaSomenteRelevante:        \n",
        "        listaTokensSelecionados.append(embeddingSentenca[i:i+1])\n",
        "  \n",
        "  if len(listaTokensSelecionados) != 0:\n",
        "    # Concatena os vetores da lista pela dimensão 0\n",
        "    embeddingSentencaRelevante = torch.cat(listaTokensSelecionados, dim=0)\n",
        "    #print(\"embeddingSentencComSubstantivo:\",embeddingSentencComSubstantivo.shape)\n",
        "  else:\n",
        "    embeddingSentencaRelevante = None\n",
        "\n",
        "  # Retorna o embedding da sentença do documento\n",
        "  return embeddingSentencaRelevante"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jccxPKRSbBoK"
      },
      "source": [
        "## getEmbeddingSentencaEmbeddingDocumento\n",
        "\n",
        "Retorna os embeddings de uma sentença com ou sem stopwords a partir dos embeddings do documento sem os StopWords.\n",
        "\n",
        "Filtros:\n",
        "- ALL - Sentença com todas as palavras\n",
        "- CLEAN - Sentença sem as stopwords\n",
        "- NOUN - Sentença somente com substantivos\n",
        "- VERB - Sentença somente com verbos\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRPeALoFbCAx"
      },
      "source": [
        "def getEmbeddingSentencaEmbeddingDocumento(embeddingDocumento, documento, sentenca, tokenizador, filtro=\"ALL\"):\n",
        "  if filtro == \"ALL\":\n",
        "    return getEmbeddingSentencaEmbeddingDocumentoComTodasPalavras(embeddingDocumento, documento, sentenca, tokenizador)\n",
        "  else:\n",
        "    if filtro == \"CLEAN\":\n",
        "        return getEmbeddingSentencaEmbeddingDocumentoSemStopWord(embeddingDocumento, documento, sentenca, tokenizador)\n",
        "    else:\n",
        "      if filtro == \"NOUN\":\n",
        "        return getEmbeddingSentencaEmbeddingDocumentoSomenteRelevante(embeddingDocumento, documento, sentenca, tokenizador, tipoRelevante=\"NOUN\")\n",
        "      else:  \n",
        "        if filtro == \"VERB\":\n",
        "          return getEmbeddingSentencaEmbeddingDocumentoSomenteRelevante(embeddingDocumento, documento, sentenca, tokenizador, tipoRelevante=\"VERB\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THFhXGGmIO_r"
      },
      "source": [
        "## getCondocumentoGlobalMeanMean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhW_OiEsIPJI"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "def getCondocumentoGlobalMeanMean(documento, filtro=\"ALL\"):\n",
        "\n",
        "  # Concatena as sentenças do documento em uma string\n",
        "  documentoConcatenado = \" \".join(documento)\n",
        "\n",
        "  # Adiciona os tokens especiais\n",
        "  documento_marcado = \"[CLS] \" + documentoConcatenado + \" [SEP]\"\n",
        "\n",
        "  # Divide a sentença em tokens\n",
        "  documento_tokenizado = tokenizer.tokenize(documento_marcado)\n",
        "\n",
        "  # Mapeia os tokens em seus índices do vocabulário\n",
        "  documento_tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n",
        "\n",
        "  # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "  mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "  # Converte as entradas de listas para tensores do torch\n",
        "  tokens_tensores = torch.as_tensor([documento_tokens_indexados])\n",
        "  mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "\n",
        "  # Prediz os atributos dos estados ocultos para cada camada\n",
        "  with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "  # Cria uma lista com os tensores a serem concatenados\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  # Lista com os tensores a serem concatenados\n",
        "  listaConcat = []\n",
        "  # Percorre os 4 últimos\n",
        "  for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "  # Realiza a concatenação dos embeddings de todos as camadas\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "  # Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "\n",
        "  # Remove a dimensão 1, o lote \"batches\".\n",
        "  #O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "  embeddingDocumento = torch.squeeze(concat4_hidden_states, dim=0)  \n",
        "\n",
        "  # Quantidade de sentenças no documento\n",
        "  n = len(documento)\n",
        "\n",
        "  soma = 0\n",
        "  conta = 0\n",
        "\n",
        "  # Percorre as sentenças do documento\n",
        "  for i in range(n):\n",
        "      # Seleciona as sentenças do documento  \n",
        "      Si = documento[i]\n",
        "     \n",
        "      # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "      # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "      embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumento, documentoConcatenado, Si, tokenizer, filtro)\n",
        "      # Saída: <qtde_tokens_Si> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "      #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "    \n",
        "      # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "      # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "      mediaEmbeddingSiTokens = torch.mean(embeddingSi, dim=0)    \n",
        "      # Saída: <768 ou 1024>\n",
        "      #print(\"mediaEmbeddingSiTokens=\", mediaEmbeddingSiTokens.shape)\n",
        "\n",
        "      # Calcula a média dos embeddings da sentença.\n",
        "      # Entrada: <768 ou 1024>  \n",
        "      mediaEmbeddingSi = torch.mean(mediaEmbeddingSiTokens, dim=0)    \n",
        "      # Saída: <valor real>\n",
        "              \n",
        "      # Acumula a média\n",
        "      soma = soma + torch.mean(mediaEmbeddingSi)\n",
        "      # Conta as sentenças\n",
        "      conta = conta + 1\n",
        "\n",
        "  # Calcula a média do documento  \n",
        "  condocumento = float(soma)/conta\n",
        "\n",
        "  return condocumento"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbqrcEK3CU3Q"
      },
      "source": [
        "## getCondocumentoGlobalMeanMax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMWVOraHJN0M"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "def getCondocumentoGlobalMeanMax(documento, filtro=\"ALL\"):\n",
        "\n",
        "  # Concatena as sentenças do documento em uma string\n",
        "  documentoConcatenado = \" \".join(documento)\n",
        "\n",
        "  # Adiciona os tokens especiais\n",
        "  documento_marcado = \"[CLS] \" + documentoConcatenado + \" [SEP]\"\n",
        "\n",
        "  # Divide a sentença em tokens\n",
        "  documento_tokenizado = tokenizer.tokenize(documento_marcado)\n",
        "\n",
        "  # Mapeia os tokens em seus índices do vocabulário\n",
        "  documento_tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n",
        "\n",
        "  # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "  mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "  # Converte as entradas de listas para tensores do torch\n",
        "  tokens_tensores = torch.as_tensor([documento_tokens_indexados])\n",
        "  mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "\n",
        "  # Prediz os atributos dos estados ocultos para cada camada\n",
        "  with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "  # Cria uma lista com os tensores a serem concatenados\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  # Lista com os tensores a serem concatenados\n",
        "  listaConcat = []\n",
        "  # Percorre os 4 últimos\n",
        "  for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "  # Realiza a concatenação dos embeddings de todos as camadas\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "  # Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "\n",
        "  # Remove a dimensão 1, o lote \"batches\".\n",
        "  #O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "  embeddingDocumento = torch.squeeze(concat4_hidden_states, dim=0)  \n",
        "\n",
        "  # Quantidade de sentenças no documento\n",
        "  n = len(documento)\n",
        "\n",
        "  soma = 0\n",
        "  conta = 0\n",
        "\n",
        "  # Percorre as sentenças do documento\n",
        "  for i in range(n):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento[i]\n",
        "    \n",
        "    # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumento, documentoConcatenado, Si, tokenizer, filtro)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "   \n",
        "    # Calcula os maiores embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSiTokens = torch.max(embeddingSi, dim=1)    \n",
        "    # Saída: <2><valores x índices>\n",
        "    # Indice 0 retorna os maiores valores de embeddings de cada token, 1 retorna os índices dos maiores\n",
        "    #print(\"maiorEmbeddingSiTokens=\", maiorEmbeddingSiTokens[0].shape)\n",
        "    \n",
        "    # Calcula a média dos embeddings.\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    #mediaEmbeddingSi = torch.mean(maiorEmbeddingSiTokens, dim=0)    \n",
        "    mediaEmbeddingSi = torch.mean(maiorEmbeddingSiTokens[0])\n",
        "    # Saída: <valor real>\n",
        "                 \n",
        "    # Acumula a média\n",
        "    soma = soma + mediaEmbeddingSi\n",
        "    # Conta as sentenças\n",
        "    conta = conta + 1\n",
        "\n",
        "  # Calcula a média do documento  \n",
        "  condocumento = float(soma)/conta\n",
        "  return condocumento"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UKayleUCTwV"
      },
      "source": [
        "## getCondocumentoGlobalMean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8nPYdQ1IrrO"
      },
      "source": [
        "def getCondocumentoGlobalMean(documento, estrategia, filtro):\n",
        "    # Verifica se a sentença do documento está dentro em uma lista\n",
        "    if type(documento) != list:\n",
        "      # Coloca s sentença em uma lista\n",
        "      documento = [documento]\n",
        "\n",
        "    if estrategia == \"MEAN\":\n",
        "        return getCondocumentoGlobalMeanMean(documento, filtro)\n",
        "    else:\n",
        "        return getCondocumentoGlobalMeanMax(documento, filtro)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JplTToZvDLiX"
      },
      "source": [
        "## Similaridade de cosseno entre os embeddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eERVKqh2uk6S"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similaridadeCoseno(embeddings1, embeddings2):\n",
        "    \"\"\"\n",
        "    Similaridade do cosseno dos embeddgins dos textos.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `embeddings1` - Um embedding a ser medido.\n",
        "    `embeddings2` - Um embedding a ser medido.\n",
        "    \"\"\"\n",
        "    \n",
        "    similaridade = cosine(embeddings1, embeddings2)\n",
        "    \n",
        "    return similaridade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av6tZHt6DLiY"
      },
      "source": [
        "## getCondocumentoGlobalSimCosMean\n",
        "\n",
        "Calcula a média aritmética da similaridade do coseno entre a média dos embeddings dos tokens das sentenças de um documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOQ9vWuADLiY"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "def getCondocumentoGlobalSimCosMean(documento, filtro=\"ALL\"):\n",
        "\n",
        "  # Concatena as palavras das sentenças do documento em uma string\n",
        "  documentoConcatenado = \"\"\n",
        "  for sentenca in documento:\n",
        "      sentencaConcatenada = \"\"\n",
        "      for palavra in sentenca:\n",
        "        sentencaConcatenada = sentencaConcatenada + \" \" + palavra.strip(\" \")            \n",
        "      documentoConcatenado = documentoConcatenado + \" \" + sentencaConcatenada.strip(\" \")\n",
        "  \n",
        "  # Remove espaços adicionais antes e depois\n",
        "  documentoConcatenado = documentoConcatenado.strip(\" \")\n",
        "  \n",
        "  # Adiciona os tokens especiais\n",
        "  documento_marcado = \"[CLS] \" + documentoConcatenado + \" [SEP]\"\n",
        "\n",
        "  # Divide a sentença em tokens\n",
        "  documento_tokenizado = tokenizer.tokenize(documento_marcado)\n",
        "\n",
        "  # Mapeia os tokens em seus índices do vocabulário\n",
        "  documento_tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n",
        "\n",
        "  # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "  mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "  # Converte as entradas de listas para tensores do torch\n",
        "  tokens_tensores = torch.as_tensor([documento_tokens_indexados])\n",
        "  mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "\n",
        "  # Prediz os atributos dos estados ocultos para cada camada\n",
        "  with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "  # Cria uma lista com os tensores a serem concatenados\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  # Lista com os tensores a serem concatenados\n",
        "  listaConcat = []\n",
        "  # Percorre os 4 últimos\n",
        "  for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "  # Realiza a concatenação dos embeddings de todos as camadas\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "  # Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "\n",
        "  # Remove a dimensão 1, o lote \"batches\".\n",
        "  #O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "  embeddingDocumento = torch.squeeze(concat4_hidden_states, dim=0)  \n",
        "\n",
        "  # Quantidade de sentenças no documento\n",
        "  n = len(documento)\n",
        "\n",
        "  # Acumuladores\n",
        "  soma = 0\n",
        "  conta = 0\n",
        "\n",
        "  # Percorre as sentenças do documento\n",
        "  for i in range(n):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento[i]\n",
        "\n",
        "    # Quantidade de letras\n",
        "    m = len(Si)    \n",
        "\n",
        "    # Acumuladores sentença\n",
        "    somaSentenca = 0\n",
        "    contaSentenca = 0\n",
        "    mediaSentenca = 0\n",
        "\n",
        "    # Percorre as paladas da sentença\n",
        "    for j in range(m-1):\n",
        "      # Seleciona as palavras da sentenças\n",
        "      Wj = Si[j]\n",
        "      Wk = Si[j+1]\n",
        "      #print(\"Wj:\", Wj, \"Wk:\", Wk)\n",
        "\n",
        "      # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "      # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Wj (palavra j), tokenizador\n",
        "      embeddingWj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumento, documentoConcatenado, Wj, tokenizer, filtro)\n",
        "      # Saída: <qtde_tokens_Wj> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "      #print(\"embeddingWi=\", embeddingWj.shape)\n",
        "   \n",
        "      # Calcula a média dos embeddings para os tokens de Wi, removendo a primeira dimensão.\n",
        "      # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "      mediaEmbeddingWjTokens = torch.mean(embeddingWj, dim=0)    \n",
        "      # Saída: <768 ou 1024>\n",
        "      #print(\"mediaEmbeddingWjTokens=\", mediaEmbeddingWjTokens.shape)\n",
        "\n",
        "      # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "      # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Wj (palavra j), tokenizador\n",
        "      embeddingWk = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumento, documentoConcatenado, Wk, tokenizer, filtro)\n",
        "      # Saída: <qtde_tokens_Wj> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "      #print(\"embeddingWk=\", embeddingWk.shape)\n",
        "   \n",
        "      # Calcula a média dos embeddings para os tokens de Wk, removendo a primeira dimensão.\n",
        "      # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "      mediaEmbeddingWkTokens = torch.mean(embeddingWk, dim=0)    \n",
        "      # Saída: <768 ou 1024>\n",
        "      #print(\"mediaEmbeddingWkTokens=\", mediaEmbeddingWkTokens.shape)\n",
        "\n",
        "      # Similaridade entre os embeddings Wj e Wk\n",
        "      # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "      sim = similaridadeCoseno(mediaEmbeddingWjTokens, mediaEmbeddingWkTokens)\n",
        "      # Saída: Um número real\n",
        "      #print(\"sim:\",sim)\n",
        "                 \n",
        "      # Acumula a similaridade dos tokens da sentença\n",
        "      somaSentenca = somaSentenca + sim\n",
        "\n",
        "      # Conta as palavras da sentença\n",
        "      contaSentenca = contaSentenca + 1\n",
        "\n",
        "    # Calcula a média da sentença  \n",
        "    media = float(somaSentenca)/contaSentenca      \n",
        "        \n",
        "    # Acumula a media\n",
        "    soma = soma + media\n",
        "\n",
        "    # Conta as sentenças\n",
        "    conta = conta + 1        \n",
        "  \n",
        "  # Calcula a média do condocumento com a soma da media das palavras\n",
        "  condocumento = float(soma)/conta\n",
        "  \n",
        "  return condocumento"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF3cPGeZo48p"
      },
      "source": [
        "## getCondocumentoGlobalSimCosMax\n",
        "\n",
        "Calcula a média aritmética da similaridade do coseno entre os maiores valores dos embeddings dos tokens das sentenças de um documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1m_qTolo48q"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "def getCondocumentoGlobalSimCosMax(documento, filtro=\"ALL\"):\n",
        "\n",
        "  # Concatena as palavras das sentenças do documento em uma string\n",
        "  documentoConcatenado = \"\"\n",
        "  for sentenca in documento:\n",
        "      sentencaConcatenada = \"\"\n",
        "      for palavra in sentenca:\n",
        "        sentencaConcatenada = sentencaConcatenada + \" \" + palavra.strip(\" \")            \n",
        "      documentoConcatenado = documentoConcatenado + \" \" + sentencaConcatenada.strip(\" \")\n",
        "  \n",
        "  # Remove espaços adicionais antes e depois\n",
        "  documentoConcatenado = documentoConcatenado.strip(\" \")\n",
        "  \n",
        "  # Adiciona os tokens especiais\n",
        "  documento_marcado = \"[CLS] \" + documentoConcatenado + \" [SEP]\"\n",
        "\n",
        "  # Divide a sentença em tokens\n",
        "  documento_tokenizado = tokenizer.tokenize(documento_marcado)\n",
        "\n",
        "  # Mapeia os tokens em seus índices do vocabulário\n",
        "  documento_tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n",
        "\n",
        "  # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "  mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "  # Converte as entradas de listas para tensores do torch\n",
        "  tokens_tensores = torch.as_tensor([documento_tokens_indexados])\n",
        "  mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "\n",
        "  # Prediz os atributos dos estados ocultos para cada camada\n",
        "  with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "  # Cria uma lista com os tensores a serem concatenados\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  # Lista com os tensores a serem concatenados\n",
        "  listaConcat = []\n",
        "  # Percorre os 4 últimos\n",
        "  for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "  # Realiza a concatenação dos embeddings de todos as camadas\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "  # Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "\n",
        "  # Remove a dimensão 1, o lote \"batches\".\n",
        "  #O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "  embeddingDocumento = torch.squeeze(concat4_hidden_states, dim=0)  \n",
        "\n",
        "  # Quantidade de sentenças no documento\n",
        "  n = len(documento)\n",
        "\n",
        "  # Acumuladores\n",
        "  soma = 0\n",
        "  conta = 0\n",
        "\n",
        "  # Percorre as sentenças do documento\n",
        "  for i in range(n):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento[i]\n",
        "\n",
        "    # Quantidade de letras\n",
        "    m = len(Si)    \n",
        "\n",
        "    # Acumuladores sentença\n",
        "    somaSentenca = 0\n",
        "    contaSentenca = 0\n",
        "    mediaSentenca = 0\n",
        "\n",
        "    # Percorre as paladas da sentença\n",
        "    for j in range(m-1):\n",
        "      # Seleciona as palavras da sentenças\n",
        "      Wj = Si[j]\n",
        "      Wk = Si[j+1]\n",
        "      #print(\"Wj:\", Wj, \"Wk:\", Wk)\n",
        "\n",
        "      # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "      # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Wj (palavra j), tokenizador\n",
        "      embeddingWj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumento, documentoConcatenado, Wj, tokenizer, filtro)\n",
        "      # Saída: <qtde_tokens_Wj> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "      #print(\"embeddingWi=\", embeddingWj.shape)\n",
        "\n",
        "      # Encontra os maiores embeddings para os tokens de Wj, removendo a primeira dimensão.\n",
        "      # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "      maiorEmbeddingWjTokens, linha = torch.max(embeddingWj, dim=1)   \n",
        "      # Saída: <768 ou 1024>\n",
        "      #print(\"maiorEmbeddingWjTokens linha:\", linha)      \n",
        "      #print(\"maiorEmbeddingWjTokens:\", maiorEmbeddingWjTokens)      \n",
        "\n",
        "      # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "      # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Wj (palavra j), tokenizador\n",
        "      embeddingWk = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumento, documentoConcatenado, Wk, tokenizer, filtro)\n",
        "      # Saída: <qtde_tokens_Wj> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "      #print(\"embeddingWk=\", embeddingWk.shape)\n",
        "   \n",
        "      # Encontra os maiores embeddings para os tokens de Wk, removendo a primeira dimensão.\n",
        "      # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "      maiorEmbeddingWkTokens, linha = torch.max(embeddingWk, dim=1)              \n",
        "      # Saída: <768 ou 1024>\n",
        "      #print(\"maiorEmbeddingWjTokens linha:\", linha)      \n",
        "      #print(\"maiorEmbeddingWkTokens:\", maiorEmbeddingWkTokens)\n",
        "      \n",
        "      # Similaridade entre os embeddings Wj e Wk\n",
        "      # Entrada: (<768 ou 1024>) x (<768 ou 1024>)\n",
        "      sim = similaridadeCoseno(maiorEmbeddingWjTokens, maiorEmbeddingWkTokens)\n",
        "      # Saída: Um número real\n",
        "      #print(\"sim:\",sim)\n",
        "                 \n",
        "      # Acumula a similaridade dos tokens da sentença\n",
        "      somaSentenca = somaSentenca + sim\n",
        "\n",
        "      # Conta as palavras da sentença\n",
        "      contaSentenca = contaSentenca + 1\n",
        "\n",
        "    # Calcula a média da sentença  \n",
        "    media = float(somaSentenca)/contaSentenca      \n",
        "        \n",
        "    # Acumula a media\n",
        "    soma = soma + media\n",
        "\n",
        "    # Conta as sentenças\n",
        "    conta = conta + 1        \n",
        "  \n",
        "  # Calcula a média do condocumento com a soma da media das palavras\n",
        "  condocumento = float(soma)/conta\n",
        "  \n",
        "  return condocumento"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pvl1R8kmIJ6"
      },
      "source": [
        "## getCondocumentoGlobalSim\n",
        "\n",
        "Calcula a média aritmética da similaridade do coseno entre os  valores dos embeddings dos tokens das sentenças de um documento usando as estratégias MEAN e MAX."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jssvi-utmIXu"
      },
      "source": [
        "def getCondocumentoGlobalSim(documento, medida, estrategia, filtro):\n",
        "    # Verifica se a sentença do documento está dentro em uma lista\n",
        "    if type(documento) != list:\n",
        "      # Coloca s sentença em uma lista\n",
        "      documento = [documento]\n",
        "\n",
        "    if medida == \"COS\":\n",
        "        if estrategia == \"MEAN\":\n",
        "          return getCondocumentoGlobalSimCosMean(documento, filtro)    \n",
        "        else:\n",
        "          if estrategia == \"MAX\":\n",
        "            return getCondocumentoGlobalSimCosMax(documento, filtro)              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tLfL6BFDLiZ"
      },
      "source": [
        "## Distância euclidiana entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância L2 ou norma L2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKrR5hMNDLiZ"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distanciaEuclidiana(embeddings1, embeddings2):\n",
        "    \"\"\"\n",
        "    Distância euclidiana entre os embeddings dos textos.\n",
        "    Possui outros nomes como distância L2 ou norma L2.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `embeddings1` - Um embedding a ser medido.\n",
        "    `embeddings2` - Um embedding a ser medido.\n",
        "    \"\"\"\n",
        "    \n",
        "    distancia = euclidean(embeddings1, embeddings2)\n",
        "    \n",
        "    return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojaQA2C49UFV"
      },
      "source": [
        "## Distância Manhattan entre os embeddings das sentenças\n",
        "\n",
        "Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "\n",
        "Igual a subtração absoluta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4VBoLbH9UFV"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def distanciaManhattan(embeddings1, embeddings2):\n",
        "    \"\"\"\n",
        "    Distância Manhattan entre os embeddings dos textos \n",
        "    Possui outros nomes como distância Cityblock, distância L1, norma L1 e métrica do táxi.\n",
        "    \n",
        "    Parâmetros:\n",
        "    `embeddings1` - Um embedding a ser medido.\n",
        "    `embeddings2` - Um embedding a ser medido.\n",
        "    \"\"\"\n",
        "    \n",
        "    distancia = cityblock(embeddings1, embeddings2)\n",
        "\n",
        "    return distancia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbOEtwiStGyV"
      },
      "source": [
        "## getCondocumentoGlobalDisMean\n",
        "\n",
        "Calcula a média aritmética da distância euclidiana ou manhattan entre as médias dos valores dos embeddings dos tokens das sentenças de um documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xbLN9BFtGyW"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "def getCondocumentoGlobalDisMean(documento, medida, filtro=\"ALL\"):\n",
        "\n",
        "  # Concatena as palavras das sentenças do documento em uma string\n",
        "  documentoConcatenado = \"\"\n",
        "  for sentenca in documento:\n",
        "      sentencaConcatenada = \"\"\n",
        "      for palavra in sentenca:\n",
        "        sentencaConcatenada = sentencaConcatenada + \" \" + palavra.strip(\" \")            \n",
        "      documentoConcatenado = documentoConcatenado + \" \" + sentencaConcatenada.strip(\" \")\n",
        "  \n",
        "  # Remove espaços adicionais antes e depois\n",
        "  documentoConcatenado = documentoConcatenado.strip(\" \")\n",
        "  \n",
        "  # Adiciona os tokens especiais\n",
        "  documento_marcado = \"[CLS] \" + documentoConcatenado + \" [SEP]\"\n",
        "\n",
        "  # Divide a sentença em tokens\n",
        "  documento_tokenizado = tokenizer.tokenize(documento_marcado)\n",
        "\n",
        "  # Mapeia os tokens em seus índices do vocabulário\n",
        "  documento_tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n",
        "\n",
        "  # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "  mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "  # Converte as entradas de listas para tensores do torch\n",
        "  tokens_tensores = torch.as_tensor([documento_tokens_indexados])\n",
        "  mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "\n",
        "  # Prediz os atributos dos estados ocultos para cada camada\n",
        "  with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "  # Cria uma lista com os tensores a serem concatenados\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  # Lista com os tensores a serem concatenados\n",
        "  listaConcat = []\n",
        "  # Percorre os 4 últimos\n",
        "  for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "  # Realiza a concatenação dos embeddings de todos as camadas\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "  # Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "\n",
        "  # Remove a dimensão 1, o lote \"batches\".\n",
        "  #O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "  embeddingDocumento = torch.squeeze(concat4_hidden_states, dim=0)  \n",
        "\n",
        "  # Quantidade de sentenças no documento\n",
        "  n = len(documento)\n",
        "\n",
        "  # Acumuladores\n",
        "  soma = 0\n",
        "  conta = 0\n",
        "\n",
        "  # Percorre as sentenças do documento\n",
        "  for i in range(n):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento[i]\n",
        "\n",
        "    # Quantidade de letras\n",
        "    m = len(Si)    \n",
        "\n",
        "    # Acumuladores sentença\n",
        "    somaSentenca = 0\n",
        "    contaSentenca = 0\n",
        "    mediaSentenca = 0\n",
        "\n",
        "    # Percorre as paladas da sentença\n",
        "    for j in range(m-1):\n",
        "      # Seleciona as palavras da sentenças\n",
        "      Wj = Si[j]\n",
        "      Wk = Si[j+1]\n",
        "      #print(\"Wj:\", Wj, \"Wk:\", Wk)\n",
        "\n",
        "      # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "      # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Wj (palavra j), tokenizador\n",
        "      embeddingWj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumento, documentoConcatenado, Wj, tokenizer, filtro)\n",
        "      # Saída: <qtde_tokens_Wj> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "      #print(\"embeddingWi=\", embeddingWj.shape)\n",
        "   \n",
        "      # Calcula a média dos embeddings para os tokens de Wi, removendo a primeira dimensão.\n",
        "      # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "      mediaEmbeddingWjTokens = torch.mean(embeddingWj, dim=0)    \n",
        "      # Saída: <768 ou 1024>\n",
        "      #print(\"mediaEmbeddingWjTokens=\", mediaEmbeddingWjTokens.shape)\n",
        "\n",
        "      # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "      # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Wj (palavra j), tokenizador\n",
        "      embeddingWk = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumento, documentoConcatenado, Wk, tokenizer, filtro)\n",
        "      # Saída: <qtde_tokens_Wj> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "      #print(\"embeddingWk=\", embeddingWk.shape)\n",
        "   \n",
        "      # Calcula a média dos embeddings para os tokens de Wk, removendo a primeira dimensão.\n",
        "      # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "      mediaEmbeddingWkTokens = torch.mean(embeddingWk, dim=0)    \n",
        "      # Saída: <768 ou 1024>\n",
        "      #print(\"mediaEmbeddingWkTokens=\", mediaEmbeddingWkTokens.shape)\n",
        "\n",
        "      if medida == \"EUC\":\n",
        "        # Distância euclidiana entre os embeddings Wj e Wk\n",
        "        # Entrada: (<768 ou 1024>) x (<768 ou 1024>)  \n",
        "        dis = distanciaEuclidiana(mediaEmbeddingWjTokens, mediaEmbeddingWkTokens)\n",
        "        # Saída: Um número real\n",
        "        #print(\"dis mean euc:\",dis)\n",
        "      else:\n",
        "        if medida == \"MAN\":\n",
        "          # Distância de manhattan entre os embeddings Wj e Wk\n",
        "          # Entrada: (<768 ou 1024>) x (<768 ou 1024>)  \n",
        "          dis = distanciaManhattan(mediaEmbeddingWjTokens, mediaEmbeddingWkTokens)\n",
        "          # Saída: Um número real\n",
        "          #print(\"dis mean man:\",dis)\n",
        "                 \n",
        "      # Acumula a distancia dos tokens sentença\n",
        "      somaSentenca = somaSentenca + dis\n",
        "\n",
        "      # Conta as palavras da sentença\n",
        "      contaSentenca = contaSentenca + 1\n",
        "\n",
        "    # Calcula a média da sentença  \n",
        "    media = float(somaSentenca)/contaSentenca      \n",
        "        \n",
        "    # Acumula a media\n",
        "    soma = soma + media\n",
        "\n",
        "    # Conta as sentenças\n",
        "    conta = conta + 1        \n",
        "  \n",
        "  # Calcula a média do condocumento com a soma da media das palavras\n",
        "  condocumento = float(soma)/conta\n",
        "  \n",
        "  return condocumento"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3SsZVWauNYd"
      },
      "source": [
        "## getCondocumentoGlobalDisMax\n",
        "\n",
        "Calcula a média aritmética da distância euclidiana ou manhattan entre os maiores valores dos embeddings dos tokens das sentenças de um documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxDeKn6SuNYe"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "def getCondocumentoGlobalDisMax(documento, medida, filtro=\"ALL\"):\n",
        "\n",
        "  # Concatena as palavras das sentenças do documento em uma string\n",
        "  documentoConcatenado = \"\"\n",
        "  for sentenca in documento:\n",
        "      sentencaConcatenada = \"\"\n",
        "      for palavra in sentenca:\n",
        "        sentencaConcatenada = sentencaConcatenada + \" \" + palavra.strip(\" \")            \n",
        "      documentoConcatenado = documentoConcatenado + \" \" + sentencaConcatenada.strip(\" \")\n",
        "  \n",
        "  # Remove espaços adicionais antes e depois\n",
        "  documentoConcatenado = documentoConcatenado.strip(\" \")\n",
        "  \n",
        "  # Adiciona os tokens especiais\n",
        "  documento_marcado = \"[CLS] \" + documentoConcatenado + \" [SEP]\"\n",
        "\n",
        "  # Divide a sentença em tokens\n",
        "  documento_tokenizado = tokenizer.tokenize(documento_marcado)\n",
        "\n",
        "  # Mapeia os tokens em seus índices do vocabulário\n",
        "  documento_tokens_indexados = tokenizer.convert_tokens_to_ids(documento_tokenizado)\n",
        "\n",
        "  # Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "  mascara_atencao = [1] * len(documento_tokenizado)\n",
        "\n",
        "  # Converte as entradas de listas para tensores do torch\n",
        "  tokens_tensores = torch.as_tensor([documento_tokens_indexados])\n",
        "  mascara_atencao_tensores = torch.as_tensor([mascara_atencao])\n",
        "\n",
        "  # Prediz os atributos dos estados ocultos para cada camada\n",
        "  with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores, mascara_atencao_tensores)\n",
        "\n",
        "  # Cria uma lista com os tensores a serem concatenados\n",
        "  # Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  # Lista com os tensores a serem concatenados\n",
        "  listaConcat = []\n",
        "  # Percorre os 4 últimos\n",
        "  for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "  # Realiza a concatenação dos embeddings de todos as camadas\n",
        "  # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "  concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "  # Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "\n",
        "  # Remove a dimensão 1, o lote \"batches\".\n",
        "  #O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "  embeddingDocumento = torch.squeeze(concat4_hidden_states, dim=0)  \n",
        "\n",
        "  # Quantidade de sentenças no documento\n",
        "  n = len(documento)\n",
        "\n",
        "  # Acumuladores\n",
        "  soma = 0\n",
        "  conta = 0\n",
        "\n",
        "  # Percorre as sentenças do documento\n",
        "  for i in range(n):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento[i]\n",
        "\n",
        "    # Quantidade de letras\n",
        "    m = len(Si)    \n",
        "\n",
        "    # Acumuladores sentença\n",
        "    somaSentenca = 0\n",
        "    contaSentenca = 0\n",
        "    mediaSentenca = 0\n",
        "\n",
        "    # Percorre as paladas da sentença\n",
        "    for j in range(m-1):\n",
        "      # Seleciona as palavras da sentenças\n",
        "      Wj = Si[j]\n",
        "      Wk = Si[j+1]\n",
        "      #print(\"Wj:\", Wj, \"Wk:\", Wk)\n",
        "\n",
        "      # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "      # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Wj (palavra j), tokenizador\n",
        "      embeddingWj = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumento, documentoConcatenado, Wj, tokenizer, filtro)\n",
        "      # Saída: <qtde_tokens_Wj> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "      #print(\"embeddingWi=\", embeddingWj.shape)\n",
        "\n",
        "      # Encontra os maiores embeddings para os tokens de Wj, removendo a primeira dimensão.\n",
        "      # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "      maiorEmbeddingWjTokens, linha = torch.max(embeddingWj, dim=1)   \n",
        "      # Saída: <768 ou 1024>\n",
        "      #print(\"maiorEmbeddingWjTokens linha:\", linha)      \n",
        "      #print(\"maiorEmbeddingWjTokens:\", maiorEmbeddingWjTokens)      \n",
        "\n",
        "      # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "      # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Wj (palavra j), tokenizador\n",
        "      embeddingWk = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumento, documentoConcatenado, Wk, tokenizer, filtro)\n",
        "      # Saída: <qtde_tokens_Wj> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "      #print(\"embeddingWk=\", embeddingWk.shape)\n",
        "   \n",
        "      # Encontra os maiores embeddings para os tokens de Wk, removendo a primeira dimensão.\n",
        "      # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "      maiorEmbeddingWkTokens, linha = torch.max(embeddingWk, dim=1)              \n",
        "      # Saída: <768 ou 1024>\n",
        "      #print(\"maiorEmbeddingWjTokens linha:\", linha)      \n",
        "      #print(\"maiorEmbeddingWkTokens:\", maiorEmbeddingWkTokens)\n",
        "      \n",
        "      if medida == \"EUC\":\n",
        "        # Distância euclidiana entre os embeddings Wj e Wk\n",
        "        # Entrada: (<768 ou 1024>) x (<768 ou 1024>)  \n",
        "        dis = distanciaEuclidiana(maiorEmbeddingWjTokens, maiorEmbeddingWkTokens)\n",
        "        # Saída: Um número real\n",
        "        #print(\"dis max euc:\",dis)\n",
        "      else:\n",
        "        if medida == \"MAN\":\n",
        "          # Distância de manhattan entre os embeddings Wj e Wk\n",
        "          # Entrada: (<768 ou 1024>) x (<768 ou 1024>)  \n",
        "          dis = distanciaManhattan(maiorEmbeddingWjTokens, maiorEmbeddingWkTokens)\n",
        "          # Saída: Um número real\n",
        "          #print(\"dis max man:\",dis)\n",
        "                 \n",
        "      # Acumula a similaridade dos tokens da sentença\n",
        "      somaSentenca = somaSentenca + dis\n",
        "\n",
        "      # Conta as palavras da sentença\n",
        "      contaSentenca = contaSentenca + 1\n",
        "\n",
        "    # Calcula a média da sentença  \n",
        "    media = float(somaSentenca)/contaSentenca      \n",
        "        \n",
        "    # Acumula a media\n",
        "    soma = soma + media\n",
        "\n",
        "    # Conta as sentenças\n",
        "    conta = conta + 1        \n",
        "  \n",
        "  # Calcula a média do condocumento com a soma da media das palavras\n",
        "  condocumento = float(soma)/conta\n",
        "  \n",
        "  return condocumento"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jKWtYvOnuv3"
      },
      "source": [
        "## getCondocumentoGlobalDis\n",
        "\n",
        "Calcula a média aritmética de distância euclidiana ou manhattan entre os valores dos embeddings dos tokens das sentenças de um documento usando as estratégias MEAN ou MAX."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r397IjjHs3MT"
      },
      "source": [
        "def getCondocumentoGlobalDis(documento, medida, estrategia, filtro):\n",
        "    # Verifica se a sentença do documento está dentro em uma lista\n",
        "    if type(documento) != list:\n",
        "      # Coloca s sentença em uma lista\n",
        "      documento = [documento]\n",
        "    \n",
        "    if estrategia == \"MEAN\":        \n",
        "      return getCondocumentoGlobalDisMean(documento, medida, filtro)    \n",
        "    else:\n",
        "      if estrategia == \"MAX\":\n",
        "        return getCondocumentoGlobalDisMax(documento, medida, filtro)              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru02mC0Estsb"
      },
      "source": [
        "# 6 - Exemplo documento global utilizando embedding a concatenação das 4 últimas camadas do BERT usando estratégia a MEAN(média dos embeddings)\n",
        "\n",
        "Como estamos utilizando os embeddings concatenado das 4 últimas camadas onde ocorre 768 entenda-se 3072 que é o resultado de 768 por 4 que é a dimensão do MCL BERT de tamanho base. E onde ocorre 1024 entenda-se 4096 que é o resultado de 1024 por 4 que é a dimensão do MCL BERT de tamanho large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1vDmYJTstsg"
      },
      "source": [
        "## Documento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJOUyEpistsg",
        "outputId": "3fb89c12-4ae3-47aa-f2ad-beab7c3a889c"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documentoOriginalConcatenado = \" \".join(documento)\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + documentoOriginalConcatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4HrZqBfstsh"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0tDxh3Mstsh",
        "outputId": "40229e02-d9bb-4976-9461-b307eaef1b1d"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAQf9nM8stsh"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFqFcnx2stsh"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJbHPnoAstsi"
      },
      "source": [
        "Gera os embeddings para o documento. Guarda todas as camadas da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51R6f4Mistsi"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx2_AvR8tbnZ"
      },
      "source": [
        "Recupera a saída e concatena as 4 últimas camada do BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C0KcRUHstsj",
        "outputId": "0a7ab664-859b-4c97-d34c-3ccc830c4ec2"
      },
      "source": [
        "# Cria uma lista com os tensores a serem concatenados\n",
        "# Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "# Lista com os tensores a serem concatenados\n",
        "listaConcat = []\n",
        "# Percorre os 4 últimos\n",
        "for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "# Realiza a concatenação dos embeddings de todos as camadas\n",
        "# Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "# Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "\n",
        "print (\"O vetor da  concatenação das 4 últimas camadas oculta tem o formato:\", concat4_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da  concatenação das 4 últimas camadas oculta tem o formato: torch.Size([1, 26, 4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM4luftrstsk"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f18k_o-stsk",
        "outputId": "23d47ca4-2131-4ae9-ee3c-056e4b2178e3"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoOriginal = torch.squeeze(concat4_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento tem o formato:\", embeddingDocumentoOriginal.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento tem o formato: torch.Size([26, 4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g0E9s2Rstsk"
      },
      "source": [
        "Confirmando vetores dependentes do documento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQGOQF-kstsk",
        "outputId": "1d397c9c-aaf0-4b32-f8b5-55464ac64f6f"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veMZsnAsstsl"
      },
      "source": [
        "Exibe os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9RSPe2Hstsl",
        "outputId": "d76e160a-23a9-4cab-d9b0-987268032101"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento[0]\n",
        "sentenca2Original = documento[1]\n",
        "sentenca3Original = documento[2]\n",
        "sentenca4Original = documento[3]\n",
        "\n",
        "embeddingSentenca1Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca1Original, tokenizer)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca2Original, tokenizer)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca3Original, tokenizer)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca4Original, tokenizer)\n",
        "\n",
        "print(\"Os primeiros 4 valores de cada sentença do documento.\")\n",
        "\n",
        "print(\"\\nSentença 1:\", sentenca1Original,\"-\", str(embeddingSentenca1Original[:4]))\n",
        "print(\"Soma embedding Sentença1:\", sentenca1Original,\"-\", str(torch.sum(embeddingSentenca1Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 2:\", sentenca2Original,\"-\", str(embeddingSentenca2Original[:4]))\n",
        "print(\"Soma embedding Sentença2:\", sentenca2Original,\"-\", str(torch.sum(embeddingSentenca2Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 3:\", sentenca3Original,\"-\", str(embeddingSentenca3Original[:4]))\n",
        "print(\"Soma embedding Sentença3:\", sentenca3Original,\"-\", str(torch.sum(embeddingSentenca3Original[:4])))\n",
        "\n",
        "print(\"\\nSentença 4:\", sentenca4Original,\"-\", str(embeddingSentenca4Original[:4]))\n",
        "print(\"Soma embedding Sentença4:\", sentenca4Original,\"-\", str(torch.sum(embeddingSentenca4Original[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os primeiros 4 valores de cada sentença do documento.\n",
            "\n",
            "Sentença 1: Bom Dia, professor. - tensor([[-0.5098, -0.2877,  0.0873,  ...,  0.9288, -0.0796,  0.3684],\n",
            "        [ 0.6078, -0.8869,  0.3736,  ...,  0.0124,  0.0094, -0.1719],\n",
            "        [ 0.9075, -1.1233, -0.0093,  ...,  0.1359, -0.3067, -0.4078],\n",
            "        [ 0.2499, -0.4717, -0.1217,  ...,  0.4604,  0.0075,  0.1555]])\n",
            "Soma embedding Sentença1: Bom Dia, professor. - tensor(-176.3423)\n",
            "\n",
            "Sentença 2: Qual o conteúdo da prova? - tensor([[-0.3987, -0.9450,  0.1785,  ...,  0.8476, -0.0572,  0.0365],\n",
            "        [ 0.2067, -0.2705,  0.7145,  ...,  0.2113,  0.2899,  0.5135],\n",
            "        [ 0.2355,  0.2686,  0.5669,  ...,  0.7205,  0.2510,  0.0924],\n",
            "        [ 0.5264, -0.4600,  0.4810,  ...,  0.1881,  0.0931,  0.1731]])\n",
            "Soma embedding Sentença2: Qual o conteúdo da prova? - tensor(-165.1487)\n",
            "\n",
            "Sentença 3: Vai cair tudo na prova? - tensor([[ 0.5178,  0.0863,  0.7394,  ..., -0.3301, -0.0115,  0.1168],\n",
            "        [ 0.1617,  1.1516, -0.0350,  ..., -0.2189,  0.1096, -0.3660],\n",
            "        [ 0.9251,  0.5806,  0.2491,  ...,  0.0027,  0.2467, -0.1535],\n",
            "        [ 0.5782,  1.3571, -0.5161,  ...,  0.5285,  0.2553, -0.0197]])\n",
            "Soma embedding Sentença3: Vai cair tudo na prova? - tensor(-177.0387)\n",
            "\n",
            "Sentença 4: Aguardo uma resposta, João. - tensor([[ 0.4339, -0.7168,  0.1173,  ...,  0.3840, -0.0936,  0.1355],\n",
            "        [-0.6806, -1.0194, -0.0765,  ...,  0.2201,  0.2731, -0.0384],\n",
            "        [ 1.1384, -0.8541,  0.8992,  ...,  0.5247,  0.2278,  0.0653],\n",
            "        [ 0.9206, -0.9862,  0.1347,  ...,  0.4393, -0.4792,  0.1743]])\n",
            "Soma embedding Sentença4: Aguardo uma resposta, João. - tensor(-169.8891)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGUkprWbstsl"
      },
      "source": [
        "Examinando os embeddings do documento\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL-c0aqKstsl",
        "outputId": "14cebb3f-0769-42f9-bdeb-accabc4a14b9"
      },
      "source": [
        "# Índice das sentenças a serem comparadas\n",
        "sentenca1Original = documento[0]\n",
        "sentenca2Original = documento[1]\n",
        "sentenca3Original = documento[2]\n",
        "sentenca4Original = documento[3]\n",
        "\n",
        "print(\"Documento:\", documento)\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca1TokenizadaOriginal = tokenizer.tokenize(sentenca1Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca1TokenizadaOriginal)\n",
        "embeddingSentenca1Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca1Original, tokenizer)\n",
        "print(\"\\nSentença 1 Original=\\\"\", sentenca1Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca1TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca1Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca1Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca2TokenizadaOriginal = tokenizer.tokenize(sentenca2Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca2TokenizadaOriginal)\n",
        "embeddingSentenca2Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca2Original, tokenizer)\n",
        "print(\"\\nSentença 2 Original=\\\"\", sentenca2Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca2TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca2Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca2Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca3TokenizadaOriginal = tokenizer.tokenize(sentenca3Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca3TokenizadaOriginal)\n",
        "embeddingSentenca3Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca3Original, tokenizer)\n",
        "print(\"\\nSentença 3 Original=\\\"\", sentenca3Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca3TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca3Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca3Original))\n",
        "\n",
        "# Localiza os índices dos tokens da sentença no documento\n",
        "sentenca4TokenizadaOriginal = tokenizer.tokenize(sentenca4Original)\n",
        "inicio, fim = encontrarIndiceSubLista(documento_tokenizado_original,sentenca4TokenizadaOriginal)\n",
        "embeddingSentenca4Original = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, sentenca4Original, tokenizer)\n",
        "print(\"\\nSentença 4 Original=\\\"\", sentenca4Original, \"\\\"\")\n",
        "print(\"    Sentença tokenizada:\", sentenca4TokenizadaOriginal)\n",
        "print(\"    => inicio em\", inicio , \"e término em\", fim)\n",
        "print(\"    Formato modelo :\", embeddingSentenca4Original.shape)\n",
        "print(\"    Soma embeddings:  %.2f\" % torch.sum(embeddingSentenca4Original))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento: ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "\n",
            "Sentença 1 Original=\" Bom Dia, professor. \"\n",
            "    Sentença tokenizada: ['Bom', 'Dia', ',', 'professor', '.']\n",
            "    => inicio em 1 e término em 5\n",
            "    Formato modelo : torch.Size([5, 4096])\n",
            "    Soma embeddings:  -225.64\n",
            "\n",
            "Sentença 2 Original=\" Qual o conteúdo da prova? \"\n",
            "    Sentença tokenizada: ['Qual', 'o', 'conteúdo', 'da', 'prova', '?']\n",
            "    => inicio em 6 e término em 11\n",
            "    Formato modelo : torch.Size([6, 4096])\n",
            "    Soma embeddings:  -252.06\n",
            "\n",
            "Sentença 3 Original=\" Vai cair tudo na prova? \"\n",
            "    Sentença tokenizada: ['Vai', 'cair', 'tudo', 'na', 'prova', '?']\n",
            "    => inicio em 12 e término em 17\n",
            "    Formato modelo : torch.Size([6, 4096])\n",
            "    Soma embeddings:  -260.65\n",
            "\n",
            "Sentença 4 Original=\" Aguardo uma resposta, João. \"\n",
            "    Sentença tokenizada: ['Agu', '##ardo', 'uma', 'resposta', ',', 'João', '.']\n",
            "    => inicio em 18 e término em 24\n",
            "    Formato modelo : torch.Size([7, 4096])\n",
            "    Soma embeddings:  -288.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELz6la71TdX6"
      },
      "source": [
        "## Mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2xmrGvd9UFT"
      },
      "source": [
        "### Calcula o coerência global pela média aritmética da média dos embeddings tokens de todas as palavras sentenças.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbyuNamR9UFT",
        "outputId": "2d8fb472-3f39-45db-f29a-21c7b463bc82"
      },
      "source": [
        "print(\"Documento  :\", str(documento))\n",
        "print(\"Quantidade de sentenças:\",len(documento))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento)\n",
        "\n",
        "soma = 0\n",
        "conta = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento[i]\n",
        "    \n",
        "    # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "    print(\"embeddingSi=\", embeddingSi.shape)\n",
        "   \n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSiTokens = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSiTokens=\", mediaEmbeddingSiTokens.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings da sentença.\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(mediaEmbeddingSiTokens, dim=0)    \n",
        "    # Saída: <valor real>\n",
        "             \n",
        "    # Acumula a media\n",
        "    soma = soma + torch.mean(mediaEmbeddingSi)\n",
        "    conta = conta + 1\n",
        "\n",
        "print(\"Resultado da soma:\", soma)\n",
        "print(\"Sentenças consideradas:\", conta)\n",
        "media = float(soma)/conta\n",
        "print(\"Média:\", media)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "embeddingSi= torch.Size([5, 4096])\n",
            "embeddingSi= torch.Size([6, 4096])\n",
            "embeddingSi= torch.Size([6, 4096])\n",
            "embeddingSi= torch.Size([7, 4096])\n",
            "Resultado da soma: tensor(-0.0419)\n",
            "Sentenças consideradas: 4\n",
            "Média: -0.0104870880022645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYuviTFR-0Tb"
      },
      "source": [
        "### Calcula o coerência global pela média aritmética da média dos embeddings tokens de substantivos das sentenças.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOpmtE9a-0Tb",
        "outputId": "99b55916-3499-4134-8566-c15ff177b0c8"
      },
      "source": [
        "print(\"Documento  :\", str(documento))\n",
        "print(\"Quantidade de sentenças:\",len(documento))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento)\n",
        "\n",
        "soma = 0\n",
        "conta = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento[i]\n",
        "    \n",
        "    # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "   \n",
        "    # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    mediaEmbeddingSiTokens = torch.mean(embeddingSi, dim=0)    \n",
        "    # Saída: <768 ou 1024>\n",
        "    #print(\"mediaEmbeddingSiTokens=\", mediaEmbeddingSiTokens.shape)\n",
        "\n",
        "    # Calcula a média dos embeddings.\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    mediaEmbeddingSi = torch.mean(mediaEmbeddingSiTokens, dim=0)    \n",
        "    # Saída: <valor real>\n",
        "             \n",
        "    # Acumula a media\n",
        "    soma = soma + torch.mean(mediaEmbeddingSi)\n",
        "    conta = conta + 1\n",
        "\n",
        "print(\"Resultado da soma:\", soma)\n",
        "print(\"Sentenças consideradas:\", conta)\n",
        "media = float(soma)/conta\n",
        "print(\"Média:\", media)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Resultado da soma: tensor(-0.0408)\n",
            "Sentenças consideradas: 4\n",
            "Média: -0.010194318369030952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzhzeJMjbc7k"
      },
      "source": [
        "### Calcula o coerência global pela média aritmética da média dos embeddings tokens das palavras das sentenças sem as stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9svkvkAbc7k",
        "outputId": "c49449c3-72d5-4e92-93c2-22ecf07682ff"
      },
      "source": [
        "print(\"Documento  :\", str(documento))\n",
        "print(\"Quantidade de sentenças:\",len(documento))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento)\n",
        "\n",
        "soma = 0\n",
        "conta = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento[i]\n",
        "    \n",
        "    # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "    #print(\"embeddingSi=\", embeddingSi)\n",
        "    if embeddingSi != None:\n",
        "      # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "      # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "      mediaEmbeddingSiTokens = torch.mean(embeddingSi, dim=0)    \n",
        "      # Saída: <768 ou 1024>\n",
        "      #print(\"mediaEmbeddingSiTokens=\", mediaEmbeddingSiTokens.shape)\n",
        "\n",
        "      # Calcula a média dos embeddings.\n",
        "      # Entrada: <768 ou 1024>  \n",
        "      mediaEmbeddingSi = torch.mean(mediaEmbeddingSiTokens, dim=0)    \n",
        "      # Saída: <valor real>\n",
        "              \n",
        "      # Acumula a media\n",
        "      soma = soma + torch.mean(mediaEmbeddingSi)\n",
        "      conta = conta + 1\n",
        "\n",
        "print(\"Resultado da soma:\", soma)\n",
        "print(\"Sentenças consideradas:\", conta)\n",
        "media = float(soma)/conta\n",
        "print(\"Média:\", media)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Resultado da soma: tensor(-0.0419)\n",
            "Sentenças consideradas: 4\n",
            "Média: -0.010485047474503517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjmxfmW2UIOA"
      },
      "source": [
        "### Calcula o coerência global pela média aritmética da média dos embeddings tokens de verbos das sentenças.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsR6GYSjUIOA",
        "outputId": "affdd6d3-fed5-403f-ca9b-0d24e84c0a56"
      },
      "source": [
        "print(\"Documento  :\", str(documento))\n",
        "print(\"Quantidade de sentenças:\",len(documento))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento)\n",
        "\n",
        "soma = 0\n",
        "conta = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento[i]\n",
        "    \n",
        "    # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer, filtro=\"VERB\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "    #print(\"embeddingSi=\", embeddingSi)\n",
        "    if embeddingSi != None:\n",
        "      # Calcula a média dos embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "      # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "      mediaEmbeddingSiTokens = torch.mean(embeddingSi, dim=0)    \n",
        "      # Saída: <768 ou 1024>\n",
        "      #print(\"mediaEmbeddingSiTokens=\", mediaEmbeddingSiTokens.shape)\n",
        "\n",
        "      # Calcula a média dos embeddings.\n",
        "      # Entrada: <768 ou 1024>  \n",
        "      mediaEmbeddingSi = torch.mean(mediaEmbeddingSiTokens, dim=0)    \n",
        "      # Saída: <valor real>\n",
        "              \n",
        "      # Acumula a media\n",
        "      soma = soma + torch.mean(mediaEmbeddingSi)\n",
        "      conta = conta + 1\n",
        "\n",
        "print(\"Resultado da soma:\", soma)\n",
        "print(\"Sentenças consideradas:\", conta)\n",
        "media = float(soma)/conta\n",
        "print(\"Média:\", media)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Resultado da soma: tensor(-0.0211)\n",
            "Sentenças consideradas: 2\n",
            "Média: -0.010556587018072605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lQiCmrrTsRi"
      },
      "source": [
        "## Max"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSgGC3YR5qWN"
      },
      "source": [
        "### Calcula o coerência global pela média aritmética dos maiores embeddings tokens de todas as palavras sentenças."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKziqsQv5qWO",
        "outputId": "a3e5516f-4bee-4dab-aeaf-9362ca5b0ebf"
      },
      "source": [
        "print(\"Documento  :\", str(documento))\n",
        "print(\"Quantidade de sentenças:\",len(documento))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento)\n",
        "\n",
        "soma = 0\n",
        "conta = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento[i]\n",
        "    \n",
        "    # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer)\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "   \n",
        "    # Calcula os maiores embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSiTokens = torch.max(embeddingSi, dim=1)    \n",
        "    # Saída: <2><valores x índices>\n",
        "    # Indice 0 retorna os maiores valores de embeddings de cada token, 1 retorna os índices dos maiores\n",
        "    #print(\"maiorEmbeddingSiTokens=\", maiorEmbeddingSiTokens[0].shape)\n",
        "    \n",
        "    # Calcula a média dos embeddings.\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    #mediaEmbeddingSi = torch.mean(maiorEmbeddingSiTokens, dim=0)    \n",
        "    mediaEmbeddingSi = torch.mean(maiorEmbeddingSiTokens[0])\n",
        "    # Saída: <valor real>\n",
        "                 \n",
        "    # Acumula a medida\n",
        "    soma = soma + mediaEmbeddingSi\n",
        "    conta = conta + 1\n",
        "\n",
        "print(\"Resultado da soma:\", soma)\n",
        "print(\"Sentenças consideradas:\", conta)\n",
        "media = float(soma)/conta\n",
        "print(\"Média:\", media)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Resultado da soma: tensor(34.5081)\n",
            "Sentenças consideradas: 4\n",
            "Média: 8.627022743225098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94h6g5UkT4uA"
      },
      "source": [
        "### Calcula o coerência global pela média aritmética dos maiores embeddings dos tokens de substantivos das sentenças.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycbCwqinT4uA",
        "outputId": "cb79abfc-f098-4b5a-af05-f768fdce0f3f"
      },
      "source": [
        "print(\"Documento  :\", str(documento))\n",
        "print(\"Quantidade de sentenças:\",len(documento))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento)\n",
        "\n",
        "soma = 0\n",
        "conta = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento[i]\n",
        "    \n",
        "    # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer, filtro=\"NOUN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "    #print(\"embeddingSi=\", embeddingSi.shape)\n",
        "   \n",
        "    # Calcula os maiores embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "    # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "    maiorEmbeddingSiTokens = torch.max(embeddingSi, dim=1)    \n",
        "    # Saída: <2><valores x índices>\n",
        "    # Indice 0 retorna os maiores valores de embeddings de cada token, 1 retorna os índices dos maiores\n",
        "    #print(\"maiorEmbeddingSiTokens=\", maiorEmbeddingSiTokens[0].shape)\n",
        "    \n",
        "    # Calcula a média dos embeddings.\n",
        "    # Entrada: <768 ou 1024>  \n",
        "    #mediaEmbeddingSi = torch.mean(maiorEmbeddingSiTokens, dim=0)    \n",
        "    mediaEmbeddingSi = torch.mean(maiorEmbeddingSiTokens[0])\n",
        "    # Saída: <valor real>\n",
        "             \n",
        "    # Acumula a media\n",
        "    soma = soma + torch.mean(mediaEmbeddingSi)\n",
        "    conta = conta + 1\n",
        "\n",
        "print(\"Resultado da soma:\", soma)\n",
        "print(\"Sentenças consideradas:\", conta)\n",
        "media = float(soma)/conta\n",
        "print(\"Média:\", media)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Resultado da soma: tensor(37.8337)\n",
            "Sentenças consideradas: 4\n",
            "Média: 9.458419799804688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9v7WD19b4Yw"
      },
      "source": [
        "### Calcula o coerência global pela média aritmética dos maiores embeddings dos tokens das palavras das sentenças sem stopwords.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8e2sfWzb4Yw",
        "outputId": "aec0f7ae-fbb4-455c-d285-c74fcc7647e2"
      },
      "source": [
        "print(\"Documento  :\", str(documento))\n",
        "print(\"Quantidade de sentenças:\",len(documento))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento)\n",
        "\n",
        "soma = 0\n",
        "conta = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento[i]\n",
        "    \n",
        "    # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer, filtro=\"CLEAN\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "    #print(\"embeddingSi=\", embeddingSi)\n",
        "    if embeddingSi != None:\n",
        "      # Calcula os maiores embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "      # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "      maiorEmbeddingSiTokens = torch.max(embeddingSi, dim=1)    \n",
        "      # Saída: <2><valores x índices>\n",
        "      # Indice 0 retorna os maiores valores de embeddings de cada token, 1 retorna os índices dos maiores\n",
        "      #print(\"maiorEmbeddingSiTokens=\", maiorEmbeddingSiTokens[0].shape)\n",
        "      \n",
        "      # Calcula a média dos embeddings.\n",
        "      # Entrada: <768 ou 1024>  \n",
        "      #mediaEmbeddingSi = torch.mean(maiorEmbeddingSiTokens, dim=0)    \n",
        "      mediaEmbeddingSi = torch.mean(maiorEmbeddingSiTokens[0])\n",
        "      # Saída: <valor real>\n",
        "              \n",
        "      # Acumula a media\n",
        "      soma = soma + torch.mean(mediaEmbeddingSi)\n",
        "      conta = conta + 1\n",
        "\n",
        "print(\"Resultado da soma:\", soma)\n",
        "print(\"Sentenças consideradas:\", conta)\n",
        "media = float(soma)/conta\n",
        "print(\"Média:\", media)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Resultado da soma: tensor(34.7088)\n",
            "Sentenças consideradas: 4\n",
            "Média: 8.677196502685547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsgK7vikQiiA"
      },
      "source": [
        "### Calcula o coerência global pela média aritmética dos maiores embeddings dos tokens de verbos das sentenças.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpekyRmIQiiB",
        "outputId": "3abf64da-b134-4aca-da35-e5b7e9799b45"
      },
      "source": [
        "print(\"Documento  :\", str(documento))\n",
        "print(\"Quantidade de sentenças:\",len(documento))\n",
        "\n",
        "# Quantidade de sentenças no documento\n",
        "n = len(documento)\n",
        "\n",
        "soma = 0\n",
        "conta = 0\n",
        "\n",
        "# Percorre as sentenças do documento\n",
        "for i in range(n):\n",
        "    # Seleciona as sentenças do documento  \n",
        "    Si = documento[i]\n",
        "    \n",
        "    # Recupera os embeddings das sentenças no embeddings do documento    \n",
        "    # Entrada: <qtde_tokens_documento> x <768 ou 1024>, documentoConcatenado,  Si (Sentença i), tokenizador\n",
        "    embeddingSi = getEmbeddingSentencaEmbeddingDocumento(embeddingDocumentoOriginal, documentoOriginalConcatenado, Si, tokenizer, filtro=\"VERB\")\n",
        "    # Saída: <qtde_tokens_Si> x <768 ou 1024> 768 ou 1024 devido ao tamanho do BERT. Ou 3184(4x768) ou 4096(4x1024) devido a concatenação das 4 últimas camadas ocultas\n",
        "    #print(\"embeddingSi=\", embeddingSi)\n",
        "    if embeddingSi != None:\n",
        "      # Calcula os maiores embeddings para os tokens de Si, removendo a primeira dimensão.\n",
        "      # Entrada: <qtde_tokens> x <768 ou 1024>  \n",
        "      maiorEmbeddingSiTokens = torch.max(embeddingSi, dim=1)    \n",
        "      # Saída: <2><valores x índices>\n",
        "      # Indice 0 retorna os maiores valores de embeddings de cada token, 1 retorna os índices dos maiores\n",
        "      #print(\"maiorEmbeddingSiTokens=\", maiorEmbeddingSiTokens[0].shape)\n",
        "      \n",
        "      # Calcula a média dos embeddings.\n",
        "      # Entrada: <768 ou 1024>  \n",
        "      #mediaEmbeddingSi = torch.mean(maiorEmbeddingSiTokens, dim=0)    \n",
        "      mediaEmbeddingSi = torch.mean(maiorEmbeddingSiTokens[0])\n",
        "      # Saída: <valor real>\n",
        "              \n",
        "      # Acumula a media\n",
        "      soma = soma + torch.mean(mediaEmbeddingSi)\n",
        "      conta = conta + 1\n",
        "\n",
        "print(\"Resultado da soma:\", soma)\n",
        "print(\"Sentenças consideradas:\", conta)\n",
        "media = float(soma)/conta\n",
        "print(\"Média:\", media)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento  : ['Bom Dia, professor.', 'Qual o conteúdo da prova?', 'Vai cair tudo na prova?', 'Aguardo uma resposta, João.']\n",
            "Quantidade de sentenças: 4\n",
            "Resultado da soma: tensor(17.0928)\n",
            "Sentenças consideradas: 2\n",
            "Média: 8.546401977539062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzF3oQ_uJhb0"
      },
      "source": [
        "## Usando função para calcular o coerência global"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dM2x6k-TDJe"
      },
      "source": [
        "## Documento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo1KTuybTEuI",
        "outputId": "fa1f93ae-7b5b-40db-c805-6193c7d7d223"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento = [\"Bom Dia, professor.\",\n",
        "             \"Qual o conteúdo da prova?\",              \n",
        "             \"Vai cair tudo na prova?\",\n",
        "             \"Aguardo uma resposta, João.\"]\n",
        "\n",
        "condocumentoGlobal = getCondocumentoGlobalMean(documento, \"MEAN\", \"ALL\")\n",
        "\n",
        "print(\"Condocumento Global:\", condocumentoGlobal)             "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Condocumento Global: -0.0104870880022645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U28ArwHTKAl"
      },
      "source": [
        "## Sentenças"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-y5X3tISjnf"
      },
      "source": [
        "Coerente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7JB2OEZNCY1",
        "outputId": "c2d71f63-2afe-453d-b8e4-99062bc1b878"
      },
      "source": [
        "# Submetendo uma única sentença\n",
        "sentenca = \"O que é uma pilha e como empilhar seu elemento?\"\n",
        "\n",
        "condocumentoGlobalc = getCondocumentoGlobalMean(sentenca, \"MEAN\", \"ALL\")\n",
        "\n",
        "print(\"Condocumento Global:\", condocumentoGlobalc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Condocumento Global: -0.011407746933400631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aBHe7b8SlLE"
      },
      "source": [
        "Incoerente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haJiq1LkJmtq",
        "outputId": "5d48b5b9-cbae-475f-8cfe-d685325a8180"
      },
      "source": [
        "# Submetendo uma única sentença\n",
        "sentenca = \"O que é uma pilha e como enfileirar seu elemento?\"\n",
        "\n",
        "condocumentoGlobali = getCondocumentoGlobalMean(sentenca, \"MEAN\", \"ALL\")\n",
        "\n",
        "print(\"Condocumento Global:\", condocumentoGlobali)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Condocumento Global: -0.011488448828458786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hELzFKCZRTPh",
        "outputId": "e0f499e1-16f5-4a68-a7f9-272b49a9cf14"
      },
      "source": [
        "dif = abs(condocumentoGlobalc) - abs(condocumentoGlobali)\n",
        "\n",
        "print(\"diferença coerente e incoerente: {:.20f}\".format(dif))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diferença coerente e incoerente: -0.00008070189505815506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osdtZSUmShK6"
      },
      "source": [
        "## Alternativas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Alo6gh1RLnCL",
        "outputId": "1f5c90d1-3373-4ded-a980-d8685b69b3b1"
      },
      "source": [
        "sentenca = \"O que é uma pilha e como identificar seu elemento?\"\n",
        "\n",
        "condocumentoGlobal1 = getCondocumentoGlobalMean(sentenca, \"MEAN\", \"ALL\")\n",
        "\n",
        "print(\"Condocumento Global:\", condocumentoGlobal1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Condocumento Global: -0.0113617442548275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5JoM5NZSpYj",
        "outputId": "ff34a14d-9693-4502-cc2a-e6a6d84f0b68"
      },
      "source": [
        "difc = abs(abs(condocumentoGlobalc) - abs(condocumentoGlobal1))\n",
        "difi = abs(abs(condocumentoGlobali) - abs(condocumentoGlobal1))\n",
        "\n",
        "print(\"diferença com coerente: {:.20f}\".format(difc))\n",
        "print(\"diferença com incoerente: {:.20f}\".format(difi))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diferença com coerente: 0.00004600267857313156\n",
            "diferença com incoerente: 0.00012670457363128662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wQSICEgL5Jl",
        "outputId": "42152e04-3a3b-40f1-b7bc-2cb4a92c1f40"
      },
      "source": [
        "sentenca = \"O que é uma pilha e como funciona seu elemento?\"\n",
        "\n",
        "condocumentoGlobal2 = getCondocumentoGlobalMean(sentenca, \"MEAN\", \"ALL\")\n",
        "\n",
        "print(\"Condocumento Global:\", condocumentoGlobal2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Condocumento Global: -0.010914635844528675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxOj0K4ZS4Zg",
        "outputId": "e607f4ea-054e-4213-f151-54e5bd56cd89"
      },
      "source": [
        "difc = abs(abs(condocumentoGlobalc) - abs(condocumentoGlobal2))\n",
        "difi = abs(abs(condocumentoGlobali) - abs(condocumentoGlobal2))\n",
        "\n",
        "print(\"diferença com coerente: {:.20f}\".format(difc))\n",
        "print(\"diferença com incoerente: {:.20f}\".format(difi))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diferença com coerente: 0.00049311108887195587\n",
            "diferença com incoerente: 0.00057381298393011093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8f9u6F0L5bJ",
        "outputId": "c70397ef-cf18-4b38-d225-3c88107daeaa"
      },
      "source": [
        "sentenca = \"O que é uma pilha e como localizar seu elemento?\"\n",
        "\n",
        "condocumentoGlobal3 = getCondocumentoGlobalMean(sentenca, \"MEAN\", \"ALL\")\n",
        "\n",
        "print(\"Condocumento Global:\", condocumentoGlobal3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Condocumento Global: -0.011291160248219967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEuwM9M0S7J1",
        "outputId": "9a073518-379b-4feb-9fb1-784eddf328e1"
      },
      "source": [
        "difc = abs(abs(condocumentoGlobalc) - abs(condocumentoGlobal3))\n",
        "difi = abs(abs(condocumentoGlobali) - abs(condocumentoGlobal3))\n",
        "\n",
        "print(\"diferença com coerente: {:.20f}\".format(difc))\n",
        "print(\"diferença com incoerente: {:.20f}\".format(difi))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diferença com coerente: 0.00011658668518066406\n",
            "diferença com incoerente: 0.00019728858023881912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLul5SIeL5lk",
        "outputId": "c38683dc-019b-4698-f608-f6fa13f1b707"
      },
      "source": [
        "sentenca = \"O que é uma pilha e como é seu elemento?\"\n",
        "\n",
        "condocumentoGlobal4 = getCondocumentoGlobalMean(sentenca, \"MEAN\", \"ALL\")\n",
        "\n",
        "print(\"Condocumento Global:\", condocumentoGlobal4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Condocumento Global: -0.01119023934006691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_kuXdhMS9Ys",
        "outputId": "d0361f56-54ca-47bc-e78c-42b845d3e997"
      },
      "source": [
        "difc = abs(abs(condocumentoGlobalc) - abs(condocumentoGlobal4))\n",
        "difi = abs(abs(condocumentoGlobali) - abs(condocumentoGlobal4))\n",
        "\n",
        "print(\"diferença com coerente: {:.20f}\".format(difc))\n",
        "print(\"diferença com incoerente: {:.20f}\".format(difi))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diferença com coerente: 0.00021750759333372116\n",
            "diferença com incoerente: 0.00029820948839187622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTfFC6WEL5vc",
        "outputId": "41bfb327-2ae7-4680-f90e-ee33843e4976"
      },
      "source": [
        "sentenca = \"O que é uma pilha e como encontrar seu elemento?\"\n",
        "\n",
        "condocumentoGlobal5 = getCondocumentoGlobalMean(sentenca, \"MEAN\", \"ALL\")\n",
        "\n",
        "print(\"Condocumento Global:\", condocumentoGlobal5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Condocumento Global: -0.011427938006818295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQLVZjADS_l_",
        "outputId": "9d4bd5a1-3f9f-4bcc-c6a4-ddef3002894c"
      },
      "source": [
        "difc = abs(abs(condocumentoGlobalc) - abs(condocumentoGlobal5))\n",
        "difi = abs(abs(condocumentoGlobali) - abs(condocumentoGlobal5))\n",
        "\n",
        "print(\"diferença com coerente: {:.20f}\".format(difc))\n",
        "print(\"diferença com incoerente: {:.20f}\".format(difi))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diferença com coerente: 0.00002019107341766357\n",
            "diferença com incoerente: 0.00006051082164049149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB4almt_nCgE"
      },
      "source": [
        "# 7 - Exemplo contexto utilizando embedding da concatenação das 4 últimas camadas do BERT e a similaridade do coseno(COS) entre os tokens das sentença de um documento usando estratégia a MEAN e MAX.\n",
        "\n",
        "Como estamos utilizando os embeddings concatenado das 4 últimas camadas onde ocorre 768 entenda-se 3072 que é o resultado de 768 por 4 que é a dimensão do MCL BERT de tamanho base. E onde ocorre 1024 entenda-se 4096 que é o resultado de 1024 por 4 que é a dimensão do MCL BERT de tamanho large."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2TniSdHY4M0",
        "outputId": "2c2514ca-1b91-4a2d-b1d2-7de109e6e3ef"
      },
      "source": [
        "# Define um documento com 4 sentenças e lista para as palavras\n",
        "# Sentenças e suas palavras(usar o spacy para sentenciar e tokenizar.)\n",
        "documento = [[\"Bom\",\"Dia\",\",\",\"professor\",\".\"],\n",
        "             [\"Qual\",\"o\",\"conteúdo\",\"da\",\"prova\",\"?\"],              \n",
        "             [\"Vai\",\"cair\",\"tudo\",\"na\",\"prova\",\"?\"],\n",
        "             [\"Aguardo\",\"uma\",\"resposta\",\",\",\"João\",\".\"]]\n",
        "\n",
        "condocumentoGlobal = getCondocumentoGlobalSim(documento, \"COS\", \"MEAN\", \"ALL\")\n",
        "print(\"Condocumento Global COS MEAN:\", condocumentoGlobal)     \n",
        "\n",
        "condocumentoGlobal = getCondocumentoGlobalSim(documento, \"COS\", \"MAX\", \"ALL\")\n",
        "print(\"Condocumento Global COS MAX:\", condocumentoGlobal)     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Condocumento Global COS MEAN: 0.3815824300050735\n",
            "Condocumento Global COS MAX: 9.325146675109864e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f17vUS6BvidS"
      },
      "source": [
        "# 8 - Exemplo contexto utilizando embedding da concatenação das 4 últimas camadas do BERT e medidas de distância(euclidiana-EUC e manhattan-MAN) entre os tokens das sentenças de um documento usando estratégia a MEAN e MAX.\n",
        "\n",
        "Como estamos utilizando os embeddings concatenado das 4 últimas camadas onde ocorre 768 entenda-se 3072 que é o resultado de 768 por 4 que é a dimensão do MCL BERT de tamanho base. E onde ocorre 1024 entenda-se 4096 que é o resultado de 1024 por 4 que é a dimensão do MCL BERT de tamanho large."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd2wxqp6vus8",
        "outputId": "0d7c3bf9-694b-4eb3-b5d1-6b8cabdfec31"
      },
      "source": [
        "# Define um documento com 4 sentenças e lista para as palavras\n",
        "# Sentenças e suas palavras(usar o spacy para sentenciar e tokenizar.)\n",
        "documento = [[\"Bom\",\"Dia\",\",\",\"professor\",\".\"],\n",
        "             [\"Qual\",\"o\",\"conteúdo\",\"da\",\"prova\",\"?\"],              \n",
        "             [\"Vai\",\"cair\",\"tudo\",\"na\",\"prova\",\"?\"],\n",
        "             [\"Aguardo\",\"uma\",\"resposta\",\",\",\"João\",\".\"]]\n",
        "\n",
        "condocumentoGlobal = getCondocumentoGlobalDis(documento, \"EUC\", \"MEAN\", \"ALL\")\n",
        "print(\"Condocumento Global EUC MEAN:\", condocumentoGlobal)     \n",
        "condocumentoGlobal = getCondocumentoGlobalDis(documento, \"EUC\", \"MAX\", \"ALL\")\n",
        "print(\"Condocumento Global EUC MAX:\", condocumentoGlobal)     \n",
        "\n",
        "condocumentoGlobal = getCondocumentoGlobalDis(documento, \"MAN\", \"MEAN\", \"ALL\")\n",
        "print(\"Condocumento Global MAN MEAN:\", condocumentoGlobal)     \n",
        "condocumentoGlobal = getCondocumentoGlobalDis(documento, \"MAN\", \"MAX\", \"ALL\")\n",
        "print(\"Condocumento Global MAN MAX:\", condocumentoGlobal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Condocumento Global EUC MEAN: 35.35420560836792\n",
            "Condocumento Global EUC MAX: 0.6187058165669441\n",
            "Condocumento Global MAN MEAN: 1695.040348815918\n",
            "Condocumento Global MAN MAX: 0.6236791610717773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 - Exemplo documento calculado a média e soma dos embeddings da concatenação das 4 últimas camadas do BERT em janelas"
      ],
      "metadata": {
        "id": "C_PHQ0jmj7zk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2UTjW65kovv"
      },
      "source": [
        "## Documento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a66749b1-3f25-41aa-dfa1-5637a58c4324",
        "id": "qAn0yd-1kovv"
      },
      "source": [
        "# Define um documento com 4 sentenças\n",
        "documento = \"Bom Dia, professor. Qual o conteúdo da prova? Vai cair tudo na prova? Aguardo uma resposta, João.\"\n",
        "\n",
        "# Concatena as sentenças do documento em uma string\n",
        "documentoOriginalConcatenado = documento\n",
        "\n",
        "# Adiciona os tokens especiais\n",
        "documento_marcado_original = \"[CLS] \" + documentoOriginalConcatenado + \" [SEP]\"\n",
        "\n",
        "# Divide a sentença em tokens\n",
        "documento_tokenizado_original = tokenizer.tokenize(documento_marcado_original)\n",
        "\n",
        "# Mapeia os tokens em seus índices do vocabulário\n",
        "documento_tokens_indexados_original = tokenizer.convert_tokens_to_ids(documento_tokenizado_original)\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(documento_tokenizado_original, documento_tokens_indexados_original):\n",
        "    print(\"{:>3} {:<12} {:>6,}\".format(i, tup[0], tup[1]))\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 [CLS]           101\n",
            "  1 Bom           8,399\n",
            "  2 Dia           3,616\n",
            "  3 ,               117\n",
            "  4 professor     2,917\n",
            "  5 .               119\n",
            "  6 Qual         13,082\n",
            "  7 o               146\n",
            "  8 conteúdo      5,015\n",
            "  9 da              180\n",
            " 10 prova         2,310\n",
            " 11 ?               136\n",
            " 12 Vai          20,805\n",
            " 13 cair          9,322\n",
            " 14 tudo          2,745\n",
            " 15 na              229\n",
            " 16 prova         2,310\n",
            " 17 ?               136\n",
            " 18 Agu           8,125\n",
            " 19 ##ardo        2,222\n",
            " 20 uma             230\n",
            " 21 resposta      4,299\n",
            " 22 ,               117\n",
            " 23 João          1,453\n",
            " 24 .               119\n",
            " 25 [SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PuXJ5nmkovw"
      },
      "source": [
        "Máscara de atenção das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b963472e-3c16-41b5-a445-f92e4443c5c2",
        "id": "NwObtitfkovw"
      },
      "source": [
        "# Marca cada um dos tokens como pertencentes à sentença \"1\".\n",
        "mascara_atencao_original = [1] * len(documento_tokenizado_original)\n",
        "\n",
        "print (mascara_atencao_original)\n",
        "print (len(mascara_atencao_original))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U59KbUD6kovx"
      },
      "source": [
        "Convertendo as listas em tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Swz2xWibkovx"
      },
      "source": [
        "# Importa a biblioteca\n",
        "import torch\n",
        "\n",
        "# Converte as entradas de listas para tensores do torch\n",
        "tokens_tensores_original = torch.as_tensor([documento_tokens_indexados_original])\n",
        "mascara_atencao_tensores_original = torch.as_tensor([mascara_atencao_original])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWO_cDLakovx"
      },
      "source": [
        "Gera os embeddings para o documento. Guarda todas as camadas da rede em `outputs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDusZx9Mkovx"
      },
      "source": [
        "# Prediz os atributos dos estados ocultos para cada camada\n",
        "with torch.no_grad():\n",
        "    # output[0] contém last_hidden_states\n",
        "    outputs = model(tokens_tensores_original, mascara_atencao_tensores_original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KSZ_UOxkovx"
      },
      "source": [
        "Recupera a saída e concatena as 4 últimas camada do BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c52257-cb39-4f51-e4a1-0b1991362b14",
        "id": "Gu7UYj1Xkovx"
      },
      "source": [
        "# Cria uma lista com os tensores a serem concatenados\n",
        "# Entrada: List das camadas(13 ou 25) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "# Lista com os tensores a serem concatenados\n",
        "listaConcat = []\n",
        "# Percorre os 4 últimos\n",
        "for i in [-1,-2,-3,-4]:\n",
        "    # Concatena da lista\n",
        "    listaConcat.append(outputs[2][i])\n",
        "    # Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "     #print(\"listaConcat=\",len(listaConcat))\n",
        "\n",
        "# Realiza a concatenação dos embeddings de todos as camadas\n",
        "# Saída: Entrada: List das camadas(4) (<1(lote)> x <qtde_tokens> <768 ou 1024>)  \n",
        "concat4_hidden_states = torch.cat(listaConcat, dim=-1)\n",
        "# Saída: Entrada: (<1(lote)> x <qtde_tokens> <3072 ou 4096>)  \n",
        "\n",
        "print (\"O vetor da  concatenação das 4 últimas camadas oculta tem o formato:\", concat4_hidden_states.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor da  concatenação das 4 últimas camadas oculta tem o formato: torch.Size([1, 26, 4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjQ3QXfjkovy"
      },
      "source": [
        "Vamos nos livrar da dimensão lotes \"batches\", pois não precisamos dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f90574c-1e97-4276-bbb7-62b2b49cc19f",
        "id": "PJ0HnqJHkovy"
      },
      "source": [
        "# Remove a dimensão 1, o lote \"batches\".\n",
        "#O método squeeze remove a primeira dimensão(0) pois possui tamanho 1\n",
        "embeddingDocumentoOriginal = torch.squeeze(concat4_hidden_states, dim=0)\n",
        "\n",
        "print (\"O vetor de tokens de embedding do documento tem o formato:\", embeddingDocumentoOriginal.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O vetor de tokens de embedding do documento tem o formato: torch.Size([26, 4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW3AnGaGkovy"
      },
      "source": [
        "Confirmando vetores dependentes do documento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daea424f-0650-4705-e065-54ed071d2e80",
        "id": "y62V3-bSkovy"
      },
      "source": [
        "for i, token_str in enumerate(documento_tokenizado_original):\n",
        "  print (i, token_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [CLS]\n",
            "1 Bom\n",
            "2 Dia\n",
            "3 ,\n",
            "4 professor\n",
            "5 .\n",
            "6 Qual\n",
            "7 o\n",
            "8 conteúdo\n",
            "9 da\n",
            "10 prova\n",
            "11 ?\n",
            "12 Vai\n",
            "13 cair\n",
            "14 tudo\n",
            "15 na\n",
            "16 prova\n",
            "17 ?\n",
            "18 Agu\n",
            "19 ##ardo\n",
            "20 uma\n",
            "21 resposta\n",
            "22 ,\n",
            "23 João\n",
            "24 .\n",
            "25 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSSDjW36kovy"
      },
      "source": [
        "Examinando os embenddings das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a07cdf-29a5-47a5-cd42-bb6108b1e66f",
        "id": "U4uww_8Hkovz"
      },
      "source": [
        "# Índice dos documentos a serem comparadas\n",
        "sentencaOriginal = documento\n",
        "\n",
        "print(\"\\nDocumento       :\", sentencaOriginal)\n",
        "\n",
        "print(\"\\nOs primeiros 10 embeddings.\")\n",
        "print(\"    Embeddings      : \", str(embeddingDocumentoOriginal[:10]))\n",
        "\n",
        "print(\"\\nOs 10 primeiros embeddings.\")\n",
        "print(\"    Formato         : \", embeddingDocumentoOriginal.size())\n",
        "print(\"    Soma embeddings : %.10f\" % torch.sum(embeddingDocumentoOriginal[:10]))\n",
        "print(\"    Média embeddings: %.10f\" % torch.mean(embeddingDocumentoOriginal[:10]))\n",
        "\n",
        "print(\"\\nTodos os embeddings.\")\n",
        "print(\"    Formato         : \", embeddingDocumentoOriginal.size())\n",
        "print(\"    Soma embeddings : %.10f\" % torch.sum(embeddingDocumentoOriginal))\n",
        "print(\"    Média embeddings: %.10f\" % torch.mean(embeddingDocumentoOriginal))\n",
        "\n",
        "print(\"\\nMédia dimensão tokens(dim=0).\")\n",
        "embeddingsD0 = torch.mean(embeddingDocumentoOriginal, dim=0)\n",
        "print(\"    Formato         :\", embeddingsD0.size())\n",
        "print(\"    Soma embeddings : %.10f\" % torch.sum(embeddingsD0))\n",
        "print(\"    Média embeddings: %.10f\" % torch.mean(embeddingsD0))\n",
        "\n",
        "print(\"\\nMédia dimensão embeddings(dim=1).\")\n",
        "embeddingsD1 = torch.mean(embeddingDocumentoOriginal, dim=1)\n",
        "print(\"    Formato         :\", embeddingsD1.size())\n",
        "print(\"    Soma embeddings : %.10f\" % torch.sum(embeddingsD1))\n",
        "print(\"    Média embeddings: %.10f\" % torch.mean(embeddingsD1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Documento       : Bom Dia, professor. Qual o conteúdo da prova? Vai cair tudo na prova? Aguardo uma resposta, João.\n",
            "\n",
            "Os primeiros 10 embeddings.\n",
            "    Embeddings      :  tensor([[ 0.2685, -0.1836,  0.2072,  ...,  0.3430, -0.0420,  0.1674],\n",
            "        [-0.5098, -0.2877,  0.0873,  ...,  0.9288, -0.0796,  0.3684],\n",
            "        [ 0.6078, -0.8869,  0.3736,  ...,  0.0124,  0.0094, -0.1719],\n",
            "        ...,\n",
            "        [ 0.2067, -0.2705,  0.7145,  ...,  0.2113,  0.2899,  0.5135],\n",
            "        [ 0.2355,  0.2686,  0.5669,  ...,  0.7205,  0.2510,  0.0924],\n",
            "        [ 0.5264, -0.4600,  0.4810,  ...,  0.1881,  0.0931,  0.1731]])\n",
            "\n",
            "Os 10 primeiros embeddings.\n",
            "    Formato         :  torch.Size([26, 4096])\n",
            "    Soma embeddings : -431.0525512695\n",
            "    Média embeddings: -0.0105237439\n",
            "\n",
            "Todos os embeddings.\n",
            "    Formato         :  torch.Size([26, 4096])\n",
            "    Soma embeddings : -1095.7685546875\n",
            "    Média embeddings: -0.0102892928\n",
            "\n",
            "Média dimensão tokens(dim=0).\n",
            "    Formato         : torch.Size([4096])\n",
            "    Soma embeddings : -42.1449470520\n",
            "    Média embeddings: -0.0102892937\n",
            "\n",
            "Média dimensão embeddings(dim=1).\n",
            "    Formato         : torch.Size([26])\n",
            "    Soma embeddings : -0.2675216198\n",
            "    Média embeddings: -0.0102892928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vetor em "
      ],
      "metadata": {
        "id": "ZNH8QyXFCkOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista = [0,1,2,3,4]\n",
        "alvo = 2\n",
        "print(lista[alvo:alvo])\n",
        "print(lista[alvo:alvo+1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA3XNIwQ7dJX",
        "outputId": "9968545e-a331-460b-e76d-fb5cf896918a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_UZH5eekov0"
      },
      "source": [
        "## Calcula a  média aritmética da média dos embeddings tokens das palavras da janela.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9880d0cf-70db-43ad-b283-388d27109e38",
        "id": "ClxCVLOpkov0"
      },
      "source": [
        "print(\"Documento           :\", str(documento))\n",
        "\n",
        "# Quantidade de tokens no documento\n",
        "n = len(documento_tokenizado_original)\n",
        "\n",
        "print(\"Quantidade de tokens: \", n)\n",
        "print(\"Formato             : \", embeddingDocumentoOriginal.size())\n",
        "\n",
        "indicePalavraAlvo = 10 #prova\n",
        "print(\"Índice palavra alvo : \", indicePalavraAlvo)\n",
        "print(\"Palavra alvo        : \", documento_tokenizado_original[indicePalavraAlvo])\n",
        "\n",
        "i = indicePalavraAlvo\n",
        "j = indicePalavraAlvo\n",
        "inicio = 1 #Desconsidera o token especial [CLS]\n",
        "fim = n-1 #Desconsidera o token especial [SEP]\n",
        "janela = documento_tokenizado_original[indicePalavraAlvo]\n",
        "\n",
        "iJanela = 0\n",
        "\n",
        "# Enquanto no for o ínicio do documento ou o fim\n",
        "while (i != inicio) or (j != fim):\n",
        "    # print(\"i:\",documento_tokenizado_original[i])\n",
        "    # print(\"j:\",documento_tokenizado_original[j])\n",
        "    \n",
        "    if iJanela != 0:\n",
        "      janelaEmbeddingAnterior = janelaEmbedding      \n",
        "    else:\n",
        "      similaridadeAnterior = 0\n",
        "      \n",
        "    print(\"\\nJanela {} - ({},{}): {}\".format(iJanela, i, j, janela))    \n",
        "    janelaEmbedding = embeddingDocumentoOriginal[i:j+1]    \n",
        "    #print(\"    Formato atual        :\", janelaEmbedding.size())    \n",
        "    \n",
        "    #print(\"    Formato         :\", janelaEmbedding.size())\n",
        "    embeddingsD0 = torch.mean(janelaEmbedding, dim=0)\n",
        "    \n",
        "    janelaDocumentoAnterio  = janela\n",
        "    print(\"    Soma embeddings            : %.10f\" % torch.sum(embeddingsD0))\n",
        "    print(\"    Média embeddings           : %.10f\" % torch.mean(embeddingsD0))\n",
        "    \n",
        "    if iJanela != 0:\n",
        "      #print(\"Formato  anterior       : \", janelaEmbeddingAnterior.size())\n",
        "      embeddingD0JanelaAnterior = torch.mean(janelaEmbeddingAnterior, dim=0)\n",
        "      similaridadeAtual = similaridadeCoseno(embeddingD0JanelaAnterior,embeddingsD0)\n",
        "      print(\"    Similaridade(cos) com a janela anterior :\", similaridadeAtual)\n",
        "      if iJanela > 1:\n",
        "          print(\"    Diferença cos com a janela anterior     :\", similaridadeAnterior - similaridadeAtual)\n",
        "      similaridadeAnterior = similaridadeAtual\n",
        "\n",
        "    \n",
        "    # Decrementa i se não chegou no início do documento\n",
        "    antes = \"\"\n",
        "    if i != inicio:\n",
        "      i = i - 1 \n",
        "      antes = documento_tokenizado_original[i]\n",
        "      # print(\"antes:\", antes)\n",
        "    \n",
        "    # Incrementa j se não chegou no fim do documento\n",
        "    depois = \"\"\n",
        "    if j != fim:\n",
        "      j = j + 1     \n",
        "      depois =  documento_tokenizado_original[j]\n",
        "      # print(\"depois:\", depois)\n",
        "    \n",
        "    janela = (antes + \" \" + janela + \" \" + depois).strip()\n",
        "\n",
        "    iJanela = iJanela + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento           : Bom Dia, professor. Qual o conteúdo da prova? Vai cair tudo na prova? Aguardo uma resposta, João.\n",
            "Quantidade de tokens:  26\n",
            "Formato             :  torch.Size([26, 4096])\n",
            "Índice palavra alvo :  10\n",
            "Palavra alvo        :  prova\n",
            "\n",
            "Janela 0 - (10,10): prova\n",
            "    Soma embeddings            : -40.8396873474\n",
            "    Média embeddings           : -0.0099706268\n",
            "\n",
            "Janela 1 - (9,11): da prova ?\n",
            "    Soma embeddings            : -43.1494903564\n",
            "    Média embeddings           : -0.0105345435\n",
            "    Similaridade(cos) com a janela anterior : 0.14918243885040283\n",
            "\n",
            "Janela 2 - (8,12): conteúdo da prova ? Vai\n",
            "    Soma embeddings            : -43.0518531799\n",
            "    Média embeddings           : -0.0105107063\n",
            "    Similaridade(cos) com a janela anterior : 0.04047346115112305\n",
            "    Diferença cos com a janela anterior     : 0.10870897769927979\n",
            "\n",
            "Janela 3 - (7,13): o conteúdo da prova ? Vai cair\n",
            "    Soma embeddings            : -42.8061866760\n",
            "    Média embeddings           : -0.0104507292\n",
            "    Similaridade(cos) com a janela anterior : 0.014799416065216064\n",
            "    Diferença cos com a janela anterior     : 0.025674045085906982\n",
            "\n",
            "Janela 4 - (6,14): Qual o conteúdo da prova ? Vai cair tudo\n",
            "    Soma embeddings            : -42.6554260254\n",
            "    Média embeddings           : -0.0104139224\n",
            "    Similaridade(cos) com a janela anterior : 0.008895277976989746\n",
            "    Diferença cos com a janela anterior     : 0.005904138088226318\n",
            "\n",
            "Janela 5 - (5,15): . Qual o conteúdo da prova ? Vai cair tudo na\n",
            "    Soma embeddings            : -43.4903602600\n",
            "    Média embeddings           : -0.0106177637\n",
            "    Similaridade(cos) com a janela anterior : 0.005801737308502197\n",
            "    Diferença cos com a janela anterior     : 0.003093540668487549\n",
            "\n",
            "Janela 6 - (4,16): professor . Qual o conteúdo da prova ? Vai cair tudo na prova\n",
            "    Soma embeddings            : -43.3844680786\n",
            "    Média embeddings           : -0.0105919112\n",
            "    Similaridade(cos) com a janela anterior : 0.004425227642059326\n",
            "    Diferença cos com a janela anterior     : 0.001376509666442871\n",
            "\n",
            "Janela 7 - (3,17): , professor . Qual o conteúdo da prova ? Vai cair tudo na prova ?\n",
            "    Soma embeddings            : -43.7006988525\n",
            "    Média embeddings           : -0.0106691159\n",
            "    Similaridade(cos) com a janela anterior : 0.004144072532653809\n",
            "    Diferença cos com a janela anterior     : 0.0002811551094055176\n",
            "\n",
            "Janela 8 - (2,18): Dia , professor . Qual o conteúdo da prova ? Vai cair tudo na prova ? Agu\n",
            "    Soma embeddings            : -43.7244873047\n",
            "    Média embeddings           : -0.0106749237\n",
            "    Similaridade(cos) com a janela anterior : 0.005152702331542969\n",
            "    Diferença cos com a janela anterior     : -0.0010086297988891602\n",
            "\n",
            "Janela 9 - (1,19): Bom Dia , professor . Qual o conteúdo da prova ? Vai cair tudo na prova ? Agu ##ardo\n",
            "    Soma embeddings            : -43.4025115967\n",
            "    Média embeddings           : -0.0105963163\n",
            "    Similaridade(cos) com a janela anterior : 0.003640413284301758\n",
            "    Diferença cos com a janela anterior     : 0.001512289047241211\n",
            "\n",
            "Janela 10 - (1,20): Bom Dia , professor . Qual o conteúdo da prova ? Vai cair tudo na prova ? Agu ##ardo uma\n",
            "    Soma embeddings            : -43.3612022400\n",
            "    Média embeddings           : -0.0105862310\n",
            "    Similaridade(cos) com a janela anterior : 0.0012288689613342285\n",
            "    Diferença cos com a janela anterior     : 0.0024115443229675293\n",
            "\n",
            "Janela 11 - (1,21): Bom Dia , professor . Qual o conteúdo da prova ? Vai cair tudo na prova ? Agu ##ardo uma resposta\n",
            "    Soma embeddings            : -43.2495040894\n",
            "    Média embeddings           : -0.0105589610\n",
            "    Similaridade(cos) com a janela anterior : 0.0010606050491333008\n",
            "    Diferença cos com a janela anterior     : 0.00016826391220092773\n",
            "\n",
            "Janela 12 - (1,22): Bom Dia , professor . Qual o conteúdo da prova ? Vai cair tudo na prova ? Agu ##ardo uma resposta ,\n",
            "    Soma embeddings            : -43.4022521973\n",
            "    Média embeddings           : -0.0105962530\n",
            "    Similaridade(cos) com a janela anterior : 0.0008623003959655762\n",
            "    Diferença cos com a janela anterior     : 0.0001983046531677246\n",
            "\n",
            "Janela 13 - (1,23): Bom Dia , professor . Qual o conteúdo da prova ? Vai cair tudo na prova ? Agu ##ardo uma resposta , João\n",
            "    Soma embeddings            : -43.3970680237\n",
            "    Média embeddings           : -0.0105949873\n",
            "    Similaridade(cos) com a janela anterior : 0.0011466145515441895\n",
            "    Diferença cos com a janela anterior     : -0.0002843141555786133\n",
            "\n",
            "Janela 14 - (1,24): Bom Dia , professor . Qual o conteúdo da prova ? Vai cair tudo na prova ? Agu ##ardo uma resposta , João .\n",
            "    Soma embeddings            : -42.7931556702\n",
            "    Média embeddings           : -0.0104475478\n",
            "    Similaridade(cos) com a janela anterior : 0.0006338953971862793\n",
            "    Diferença cos com a janela anterior     : 0.0005127191543579102\n"
          ]
        }
      ]
    }
  ]
}